<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Jonathan Maynard" />


<title>ESG MLR Modeling</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Ecological Site Mapping</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Overview</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Code
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="code.html">Modeling Steps</a>
    </li>
    <li>
      <a href="ESG_Covariate_Processing.html">1. Covariate Processing</a>
    </li>
    <li>
      <a href="ESG_HyperTemp_Image_Processing.html">2. Hyper-temporal RS-Image Processing</a>
    </li>
    <li>
      <a href="ESG_MLR_Modeling.html">3. Modeling, Prediction and Validation</a>
    </li>
    <li>
      <a href="ESG_SSURGO_mapping.html">4. Mapping SSURGO ESGs</a>
    </li>
  </ul>
</li>
<li>
  <a href="data.html">Data</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/jjmaynard/ESG.Mapping">
    <span class="fa fa-github"></span>
     
    Git Repository
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">ESG MLR Modeling</h1>
<h4 class="author">Jonathan Maynard</h4>
<h4 class="date">Feb 10, 2017</h4>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span> workflowr <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> </a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2019-06-17
</p>
<p>
<strong>Checks:</strong> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 6 <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> 0
</p>
<p>
<strong>Knit directory:</strong> <code>ESG.Mapping/</code> <span class="glyphicon glyphicon-question-sign" aria-hidden="true" title="This is the local directory in which the code in this file was executed."> </span>
</p>
<p>
This reproducible <a href="http://rmarkdown.rstudio.com">R Markdown</a> analysis was created with <a
  href="https://github.com/jdblischak/workflowr">workflowr</a> (version 1.3.0). The <em>Checks</em> tab describes the reproducibility checks that were applied when the results were created. The <em>Past versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguptodate"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>R Markdown file:</strong> up-to-date </a>
</p>
</div>
<div id="strongRMarkdownfilestronguptodate" class="panel-collapse collapse">
<div class="panel-body">
<p>Great! Since the R Markdown file has been committed to the Git repository, you know the exact version of the code that produced these results.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20190528code"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Seed:</strong> <code>set.seed(20190528)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20190528code" class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20190528)</code> was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Session information:</strong> recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomjjmaynardESGMappingtree0e084ee921fd3e32c5ac50384bbdd2f72b39c2f1targetblank0e084eea"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Repository version:</strong> <a href="https://github.com/jjmaynard/ESG.Mapping/tree/0e084ee921fd3e32c5ac50384bbdd2f72b39c2f1" target="_blank">0e084ee</a> </a>
</p>
</div>
<div id="strongRepositoryversionstrongahrefhttpsgithubcomjjmaynardESGMappingtree0e084ee921fd3e32c5ac50384bbdd2f72b39c2f1targetblank0e084eea" class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility. The version displayed above was the version of the Git repository at the time these results were generated. <br><br> Note that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use <code>wflow_publish</code> or <code>wflow_git_commit</code>). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .Rhistory
    Ignored:    .Rproj.user/
    Ignored:    data/raw_data/
    Ignored:    manuscript/figures/
    Ignored:    proj_setup/

Untracked files:
    Untracked:  data/derived_data/vector/

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the R Markdown and HTML files. If you’ve configured a remote Git repository (see <code>?wflow_git_remote</code>), click on the hyperlinks in the table below to view them.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/jjmaynard/ESG.Mapping/blob/0e084ee921fd3e32c5ac50384bbdd2f72b39c2f1/analysis/ESG_MLR_Modeling.Rmd" target="_blank">0e084ee</a>
</td>
<td>
jjmaynard
</td>
<td>
2019-06-17
</td>
<td>
Site update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/jjmaynard/ESG.Mapping/a46f4597d8e1e25d1bbdbe7446eb9b81dee43ca2/docs/ESG_MLR_Modeling.html" target="_blank">a46f459</a>
</td>
<td>
jjmaynard
</td>
<td>
2019-05-31
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/jjmaynard/ESG.Mapping/23be7142612be273d8fff94e4538b884ede3548f/docs/ESG_MLR_Modeling.html" target="_blank">23be714</a>
</td>
<td>
jjmaynard
</td>
<td>
2019-05-31
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/jjmaynard/ESG.Mapping/blob/6713ad76cc551e0658b6f034c189e6c02aa6ac7b/analysis/ESG_MLR_Modeling.Rmd" target="_blank">6713ad7</a>
</td>
<td>
jjmaynard
</td>
<td>
2019-05-31
</td>
<td>
ESG_MLR_Modeling
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<pre class="r"><code>required.packages &lt;- c(&quot;here&quot;, &quot;parallelMap&quot;, &quot;mlr&quot;, &quot;mlrHyperopt&quot;, &quot;e1071&quot;, 
    &quot;Cairo&quot;, &quot;raster&quot;, &quot;lattice&quot;, &quot;ggplot2&quot;, &quot;scales&quot;, &quot;rasterVis&quot;, &quot;caret&quot;, 
    &quot;kernlab&quot;, &quot;pROC&quot;, &quot;parallel&quot;, &quot;foreach&quot;, &quot;snowfall&quot;, &quot;ranger&quot;, &quot;randomForest&quot;, 
    &quot;rgdal&quot;, &quot;MODIS&quot;, &quot;iml&quot;, &quot;dplyr&quot;, &quot;gridExtra&quot;, &quot;Rsenal&quot;, &quot;FSelector&quot;)
new.packages &lt;- required.packages[!(required.packages %in% installed.packages()[, 
    &quot;Package&quot;])]
if (length(new.packages)) install.packages(new.packages)
lapply(required.packages, require, character.only = T)
rm(required.packages, new.packages)

no_cores &lt;- detectCores() - 1
cl &lt;- makeCluster(no_cores, type = &quot;SOCK&quot;, outfile = &quot;&quot;)
registerDoParallel(cl)
getDoParWorkers()</code></pre>
<pre class="r"><code>############################################################################################################################### Load data Load covariate names
load(&quot;/data/data2/data/esgMapping/R/co_covariate_names.Rdata&quot;)
load(&quot;/data/data2/data/esgMapping/R/nm_covariate_names.Rdata&quot;)
load(&quot;/data/data2/data/esgMapping/R/CO_ESG_modeling_data.Rdata&quot;)
load(&quot;/data/data2/data/esgMapping/R/NM_ESG_modeling_data.Rdata&quot;)
co.points &lt;- readOGR(dsn = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/vector_data&quot;, 
    layer = &quot;CO_points_final&quot;)
nm.points &lt;- readOGR(dsn = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/vector_data&quot;, 
    layer = &quot;NM_points_final&quot;)
############################################################################################################################### Study areas
CO &lt;- readOGR(dsn = &quot;/data/data1/R_DATA/Imagery_Vector/Ecosite_Group_Mapping/CO_Plat_study_area&quot;, 
    layer = &quot;epaL4_mlra35sel_bndc&quot;)
# read in second shapefile of Mongolia and use projection of first to
# reproject new shapefile
NM &lt;- readOGR(dsn = &quot;/data/data1/R_DATA/Imagery_Vector/Ecosite_Group_Mapping/Chihuahuan_study_area&quot;, 
    layer = &quot;Chihauhuan_Study_Area_Boundary&quot;)

CO.shapepath &lt;- &quot;/data/data1/R_DATA/Imagery_Vector/Ecosite_Group_Mapping/CO_Plat_study_area/epaL4_mlra35sel_bndc.shp&quot;
NM.shapepath &lt;- &quot;/data/data1/R_DATA/Imagery_Vector/Ecosite_Group_Mapping/Chihuahuan_study_area/Chihauhuan_Study_Area_Boundary.shp&quot;

# CO rasterize Set up a raster &#39;template&#39; to use in rasterize()
ext.co &lt;- extent(CO)
xy.co &lt;- abs(apply(as.matrix(bbox(CO)), 1, diff))
r.co &lt;- raster(ext.co, ncol = xy.co[1]/250, nrow = xy.co[2]/250)

# Rasterize the shapefile
CO_raster &lt;- rasterize(CO, r.co)
res(CO_raster) &lt;- 250
plot(CO_raster)

# NM rasterize Set up a raster &#39;template&#39; to use in rasterize()
ext.nm &lt;- extent(NM)
xy.nm &lt;- abs(apply(as.matrix(bbox(NM)), 1, diff))
r.nm &lt;- raster(ext.nm, ncol = xy.nm[1]/250, nrow = xy.nm[2]/250)

# Rasterize the shapefile
NM_raster &lt;- rasterize(NM, r.nm)
res(NM_raster) &lt;- 250
plot(NM_raster)</code></pre>
<p>##Plotting of Covariates</p>
<pre class="r"><code>cbPalette &lt;- Set3(87)
NM_TAXOUSDA_plot &lt;- gplot(nm.cov.brick[[269]], maxpixels=800000) +
  geom_raster(aes(fill = factor(value))) +
  coord_equal()+ #labs(x = &quot;Easting (m)&quot;, y = &quot;Northing (m)&quot;, fill = &quot;Eco-Site&quot;) +
  ggtitle(&quot;NM TAXOUSDA&quot;) +scale_fill_manual(values=cbPalette)#, labels = c(as.character(c(&quot;Clayey&quot;, &quot;Loamy&quot;, &quot;Shallow Sandy&quot;, &quot;Sandy&quot;,&quot;Deep Sandy&quot;))))
NM_TAXOUSDA_plot
ggsave(NM_TAXOUSDA_plot, file = &quot;NM_TAXOUSDA_plot.pdf&quot;, width = 8, height = 8)
ggsave(NM_TAXOUSDA_plot, file = &quot;NM_TAXOUSDA_plot.png&quot;, width = 8, height = 8, type = &quot;cairo-png&quot;)</code></pre>
<p>#MODELING</p>
<p>##Modelling steps 1. Create preprocess wrapper, wrap in the filter wrappers and tune across models to select filter threshold 2. Perform hyperparameter tuning with preprocess and filter selection done within CV folds 3. Assign learning optimal hyperparameters and then run final model using repeated 10-fold cross validation. 4. Perform stacked ensemble modeling using selected models. 5. Evaluate models with external validation dataset.</p>
<pre class="r"><code>#-------------------------------------------------------------------------------------------
# --------register parallel backend
parallelStartSocket(cpus = 24, show.info = TRUE)
# parallelStop() --------create svm parameter set
svm.sigest.par = makeParamSet(makeNumericParam(&quot;cost&quot;, lower = -5, upper = 10, 
    trafo = function(x) 2^x), makeNumericParam(id = &quot;gamma&quot;, lower = -10, 
    upper = 10, trafo = function(x) 2^x))
svm.sigest.par.config = makeParConfig(par.set = svm.sigest.par, learner.name = &quot;classif.svm&quot;)
getParConfigParSet(svm.sigest.par.config)

# --------create random forest parameter set
rf.par.config = generateParConfig(learner = &quot;classif.randomForest&quot;)
getParConfigParSet(rf.par.config)

# --------create xgboost parameter set
xgboost.par.config = generateParConfig(learner = &quot;classif.xgboost&quot;)
getParConfigParSet(xgboost.par.config)

xgboost.par = makeParamSet(makeIntegerParam(&quot;max_depth&quot;, lower = 1, upper = 25), 
    makeNumericParam(&quot;eta&quot;, lower = 0.002, upper = 0.6), makeDiscreteParam(&quot;nrounds&quot;, 
        values = c(10, 30, 50, 80, 100)), makeNumericParam(&quot;lambda&quot;, lower = -1, 
        upper = 0, trafo = function(x) 10^x))

xgboost.par.config = makeParConfig(par.set = xgboost.par, learner.name = &quot;classif.xgboost&quot;)
getParConfigParSet(xgboost.par.config)

#-------------------Set hypercontrol parameters
hyper.control = makeHyperControl(mlr.control = makeTuneControlRandom(maxit = 50L), 
    resampling = cv3, measures = list(acc, mmce, timetrain))
hyper.control.xgboost = makeHyperControl(mlr.control = makeTuneControlRandom(maxit = 50L), 
    resampling = cv3, measures = list(acc, mmce, timetrain))
#-------------------------------------------------------------------------------------------

# lookup key
unique(co.points@data[10:11])
unique(nm.points@data[14:15])</code></pre>
<p>##Define modeling functions</p>
<pre class="r"><code>#-----------------------------------------------------------------------------------------------
# multi-learner preprocessing function
#-----------------------------------------------------------------------------------------------

multiLearner.preprocess &lt;- function(task) {
    
    #---------------Create preprocessing wrappers for each learner
    svm.proc.lrn = makePreprocWrapperCaret(&quot;classif.svm&quot;, ppc.nzv = TRUE, 
        ppc.center = TRUE, ppc.scale = TRUE)
    rf.proc.lrn = makePreprocWrapperCaret(&quot;classif.randomForest&quot;, ppc.nzv = TRUE, 
        ppc.center = TRUE, ppc.scale = TRUE)
    xgboost.proc.lrn = makePreprocWrapperCaret(&quot;classif.xgboost&quot;, ppc.nzv = TRUE, 
        ppc.center = TRUE, ppc.scale = TRUE)
    
    
    #----------------Feature selection with filter tuning
    # define different learners
    svm.lrn = makeFilterWrapper(learner = svm.proc.lrn, fw.method = &quot;information.gain&quot;)
    rf.lrn = makeFilterWrapper(learner = rf.proc.lrn, fw.method = &quot;information.gain&quot;)
    xgboost.lrn = makeFilterWrapper(learner = xgboost.proc.lrn, fw.method = &quot;information.gain&quot;)
    
    # set threshold range for feature selection tuning
    ps.thresh = makeParamSet(makeDiscreteParam(&quot;fw.threshold&quot;, values = seq(0, 
        0.6, 0.05)))
    # set resampling scheme for feature tuning
    rdesc.3fcv = makeResampleDesc(&quot;CV&quot;, iters = 3)
    
    set.seed(120, &quot;L&#39;Ecuyer-CMRG&quot;)
    Task.filtune.svm = tuneParams(svm.lrn, task = task, resampling = rdesc.3fcv, 
        par.set = ps.thresh, control = makeTuneControlGrid(), measures = list(acc, 
            mmce, timetrain))
    
    Task.filtune.svm.lrn = makeFilterWrapper(learner = svm.proc.lrn, fw.method = &quot;information.gain&quot;, 
        fw.threshold = Task.filtune.svm$x$fw.threshold)
    
    set.seed(120, &quot;L&#39;Ecuyer-CMRG&quot;)
    Task.filtune.rf = tuneParams(rf.lrn, task = task, resampling = rdesc.3fcv, 
        par.set = ps.thresh, control = makeTuneControlGrid(), measures = list(acc, 
            mmce, timetrain))
    
    Task.filtune.rf.lrn = makeFilterWrapper(learner = rf.proc.lrn, fw.method = &quot;information.gain&quot;, 
        fw.threshold = Task.filtune.rf$x$fw.threshold)
    
    set.seed(120, &quot;L&#39;Ecuyer-CMRG&quot;)
    Task.filtune.xgboost = tuneParams(xgboost.lrn, task = task, resampling = rdesc.3fcv, 
        par.set = ps.thresh, control = makeTuneControlGrid(), measures = list(acc, 
            mmce, timetrain))
    
    Task.filtune.xgboost.lrn = makeFilterWrapper(learner = xgboost.proc.lrn, 
        fw.method = &quot;information.gain&quot;, fw.threshold = Task.filtune.xgboost$x$fw.threshold)
    
    # #----------------Filter features using maximum optimized thresholds
    # from the three models threshold &lt;-
    # min(c(Task.filtune.svm$x$fw.threshold,
    # Task.filtune.rf$x$fw.threshold, Task.filtune.xgboost$x$fw.threshold))
    # #------------------Set learners to minimum threshold--retain all
    # important covariates Task.filtune.svm.lrn &lt;-
    # setHyperPars(Task.filtune.svm.lrn, fw.threshold = threshold)
    # Task.filtune.rf.lrn &lt;- setHyperPars(Task.filtune.rf.lrn, fw.threshold
    # = threshold) Task.filtune.xgboost.lrn &lt;-
    # setHyperPars(Task.filtune.xgboost.lrn, fw.threshold = threshold)
    
    #----------------Tune hyperparameters with Hyperopt approach
    # Hyperparameter tuning where feature selection is perfored within each
    # CV fold during parameter tuning
    
    set.seed(120, &quot;L&#39;Ecuyer-CMRG&quot;)
    Task_hyper.svm = hyperopt(task = task, learner = Task.filtune.svm.lrn, 
        par.config = svm.sigest.par.config, hyper.control = hyper.control)
    Task.filtune.partune.svm.lrn = setHyperPars(Task.filtune.svm.lrn, par.vals = Task_hyper.svm$x)
    Task.filtune.partune.svm.lrn = setLearnerId(Task.filtune.partune.svm.lrn, 
        id = &quot;SVM&quot;)
    Task.filtune.partune.svm.lrn$short.name = &quot;SVM&quot;
    
    set.seed(120, &quot;L&#39;Ecuyer-CMRG&quot;)
    Task_hyper.rf = hyperopt(task = task, learner = Task.filtune.rf.lrn, 
        par.config = rf.par.config, hyper.control = hyper.control)
    Task.filtune.partune.rf.lrn = setHyperPars(Task.filtune.rf.lrn, par.vals = c(Task_hyper.rf$x, 
        importance = TRUE))
    Task.filtune.partune.rf.lrn = setLearnerId(Task.filtune.partune.rf.lrn, 
        id = &quot;RF&quot;)
    Task.filtune.partune.rf.lrn$short.name = &quot;RF&quot;
    
    Task.filtune.xgboost.lrn &lt;- setHyperPars(Task.filtune.xgboost.lrn, 
        nthread = 2)
    
    set.seed(120, &quot;L&#39;Ecuyer-CMRG&quot;)
    Task_hyper.xgboost = hyperopt(task = task, learner = Task.filtune.xgboost.lrn, 
        par.config = xgboost.par.config, hyper.control = hyper.control.xgboost)
    Task.filtune.partune.xgboost.lrn = setHyperPars(Task.filtune.xgboost.lrn, 
        par.vals = c(Task_hyper.xgboost$x, nthread = 2))
    Task.filtune.partune.xgboost.lrn = setLearnerId(Task.filtune.partune.xgboost.lrn, 
        id = &quot;XGB&quot;)
    Task.filtune.partune.xgboost.lrn$short.name = &quot;XGB&quot;
    
    Task.stacked.lrns = list(Task.filtune.partune.svm.lrn, Task.filtune.partune.rf.lrn, 
        Task.filtune.partune.xgboost.lrn)
    Task.stacked.lrns = lapply(Task.stacked.lrns, setPredictType, &quot;prob&quot;)
    # averaging of base learner probabiilties
    Task.stacked.lrns.model = makeStackedLearner(base.learners = Task.stacked.lrns, 
        predict.type = &quot;prob&quot;, method = &quot;average&quot;)
    Task.stacked.lrns.model = setLearnerId(Task.stacked.lrns.model, id = &quot;Ensemble&quot;)
    Task.stacked.lrns.model$short.name = &quot;Ensemble&quot;
    
    Task.lrns = list(Task.filtune.partune.svm.lrn, Task.filtune.partune.rf.lrn, 
        Task.filtune.partune.xgboost.lrn, Task.stacked.lrns.model)
    
    return(Task.lrns)
    
}

#-----------------------------------------------------------------------------------------------
# Resampling function
#-----------------------------------------------------------------------------------------------
resample.multi.learner &lt;- function(Task.lrns, m.task, resamp, m) {
    set.seed(120, &quot;L&#39;Ecuyer-CMRG&quot;)
    Model_var.resamp.ksvm = mlr::resample(learner = Task.lrns[[1]], task = m.task, 
        resampling = resamp, measures = m)
    
    set.seed(120, &quot;L&#39;Ecuyer-CMRG&quot;)
    Model_var.resamp.rf = mlr::resample(learner = Task.lrns[[2]], task = m.task, 
        resampling = resamp, measures = m)
    
    set.seed(120, &quot;L&#39;Ecuyer-CMRG&quot;)
    Model_var.resamp.xgb = mlr::resample(learner = Task.lrns[[3]], task = m.task, 
        resampling = resamp, measures = m)
    
    set.seed(120, &quot;L&#39;Ecuyer-CMRG&quot;)
    Model_var.resamp.ens = mlr::resample(learner = Task.lrns[[4]], task = m.task, 
        resampling = resamp, measures = m)
    
    Mod.resamp = list(Model_var.resamp.ksvm, Model_var.resamp.rf, Model_var.resamp.xgb, 
        Model_var.resamp.ens)
    return(Mod.resamp)
}

#-----------------------------------------------------------------------------------------------
# raster parallel predict function for MLR models
#-----------------------------------------------------------------------------------------------

# x=covariate brick, model=mlr model, block_n = sets min number of
# blocks
mlr.raster.predict.run &lt;- function(x, filename, model, block_n, mask, layers) {
    out &lt;- brick(x, nl = layers, values = FALSE)
    out &lt;- writeStart(out, filename = filename, format = &quot;GTiff&quot;, overwrite = TRUE)
    bs &lt;- blockSize(x, minblocks = block_n)
    pb &lt;- pbCreate(bs$n)
    pb &lt;- txtProgressBar(min = 1, max = bs$n, style = 3)
    todisk &lt;- TRUE
    # foreach(i=1:bs$n, .packages = c(&#39;mlr&#39;, &#39;raster&#39;, &#39;caret&#39;),
    # .export=c(&#39;out&#39;, &#39;bs&#39;, &#39;pb&#39;)) %dopar% {
    for (i in 1:bs$n) {
        v &lt;- getValuesBlock(x, row = bs$row[i], nrows = bs$nrows[i])
        v &lt;- as.data.frame(v)
        v[is.na(v)] &lt;- 0
        vv &lt;- array(, dim = c(nrow(v), layers))
        class_out &lt;- predict(model, newdata = v)
        class_out$data$response &lt;- as.numeric(sub(&quot;ES&quot;, &quot;&quot;, class_out$data$response))
        vv &lt;- class_out$data
        vvv &lt;- do.call(&quot;cbind&quot;, lapply(vv, array))
        out &lt;- writeValues(out, vvv, bs$row[i])
        setTxtProgressBar(pb, i)
    }
    out &lt;- writeStop(out)
    out.mask &lt;- mask(out, mask, filename = filename, overwrite = T)
    return(out.mask)
    close(pb)
}

#-----------------------------------------------------------------------------------------------
# raster parallel predict function for confusion index
#-----------------------------------------------------------------------------------------------
CI.calc.run &lt;- function(x, filename) {
    out &lt;- brick(x, nl = 1, values = FALSE)
    out &lt;- writeStart(out, filename = filename, format = &quot;GTiff&quot;, overwrite = TRUE)
    bs &lt;- blockSize(x, minblocks = block_n)
    pb &lt;- pbCreate(bs$n)
    pb &lt;- txtProgressBar(min = 1, max = bs$n, style = 3)
    todisk &lt;- TRUE
    # foreach(i=1:bs$n, .packages = c(&#39;mlr&#39;, &#39;raster&#39;, &#39;caret&#39;),
    # .export=c(&#39;out&#39;, &#39;bs&#39;, &#39;pb&#39;)) %dopar% {
    for (i in 1:bs$n) {
        v &lt;- getValuesBlock(x, row = bs$row[i], nrows = bs$nrows[i])
        v &lt;- as.data.frame(v)
        vv &lt;- array(, dim = c(nrow(v), 1))
        vv &lt;- (1 - (first(v[, 1:(length(v) - 1)]) - second(v[, 1:(length(v) - 
            1)])))
        # vv[,2] &lt;- apply(v[,1:(ncol(v)-1)], 1, which.max)
        vvv &lt;- t(do.call(&quot;cbind&quot;, lapply(vv, array)))
        out &lt;- writeValues(out, vvv, bs$row[i])
        setTxtProgressBar(pb, i)
    }
    out &lt;- writeStop(out)
    return(out)
    close(pb)
}

## Calculate confusion index and most probable class confusion index
first &lt;- function(x) {
    # order by row number then by value
    y &lt;- t(x)
    array(y[order(col(y), y)], dim(y))[nrow(y), ]
}
second &lt;- function(x) {
    # order by row number then by value
    y &lt;- t(x)
    array(y[order(col(y), y)], dim(y))[nrow(y) - 1, ]
}


#-----------------------------------------------------------------------------------------------
# Scaled Shannon Entropy Index
#-----------------------------------------------------------------------------------------------
# input probability raster
SSEI.calc.run &lt;- function(x, filename) {
    out &lt;- brick(x, nl = 1, values = FALSE)
    out &lt;- writeStart(out, filename = filename, format = &quot;GTiff&quot;, overwrite = TRUE)
    bs &lt;- blockSize(x, minblocks = block_n)
    pb &lt;- pbCreate(bs$n)
    pb &lt;- txtProgressBar(min = 1, max = bs$n, style = 3)
    todisk &lt;- TRUE
    # foreach(i=1:bs$n, .packages = c(&#39;mlr&#39;, &#39;raster&#39;, &#39;caret&#39;),
    # .export=c(&#39;out&#39;, &#39;bs&#39;, &#39;pb&#39;)) %dopar% {
    for (i in 1:bs$n) {
        v &lt;- getValuesBlock(x, row = bs$row[i], nrows = bs$nrows[i])
        v &lt;- as.data.frame(v)
        vv &lt;- array(, dim = c(nrow(v), 1))
        prob.ent &lt;- unlist(alply(v, 1, .fun = function(x) {
            entropy.empirical(unlist(x))
        }))
        vv &lt;- round(prob.ent/entropy.empirical(rep(1/ncol(v), ncol(v))) * 
            100)
        vvv &lt;- t(do.call(&quot;cbind&quot;, lapply(vv, array)))
        out &lt;- writeValues(out, vvv, bs$row[i])
        setTxtProgressBar(pb, i)
    }
    out &lt;- writeStop(out)
    return(out)
    close(pb)
}

#---------------------------------------------------------------------------------------
# Function requires mlr resampling object with probabilities
classwise_accuracy_stats &lt;- function(model) {
    n.l &lt;- plyr::count(model$pred$data$truth)
    n.l &lt;- data.frame(matrix(n.l$freq, nrow = 1, dimnames = list(1, paste(n.l$x))))
    probs &lt;- model$pred$data[, 3:10]
    names &lt;- colnames(n.l)
    names(probs) = names
    obs &lt;- data.frame(lapply(names, FUN = function(i) {
        ifelse(model$pred$data$truth == i, 1, 0)
    }))
    names(obs) = names
    obs.pred &lt;- list(as.matrix(obs[, names]), probs[, names])
    error &lt;- Reduce(&quot;-&quot;, obs.pred)
    error.l &lt;- as.data.frame(t(signif(colSums(error, na.rm = T), 3)))
    ## copy ID of the point
    error &lt;- as.data.frame(error)
    error$id &lt;- paste(model$pred$data$id)
    out &lt;- list(n.l, obs.pred, error, error.l)
    names(out) &lt;- c(&quot;n.l&quot;, &quot;obs.pred&quot;, &quot;error&quot;, &quot;error.l&quot;)
    ## calculate totals per soil type
    N.tot &lt;- plyr::rbind.fill(out[[&quot;n.l&quot;]])
    N.tot &lt;- colSums(N.tot)
    
    ## mean error per soil type:
    mean.error &lt;- plyr::rbind.fill(out[[&quot;error.l&quot;]])
    mean.error &lt;- colSums(mean.error)/N.tot
    obs2 &lt;- plyr::rbind.fill(as.data.frame(out[[&quot;obs.pred&quot;]][[1]]))
    pred &lt;- plyr::rbind.fill(as.data.frame(out[[&quot;obs.pred&quot;]][[2]]))
    ## Get the most probable class:
    ranks.pred &lt;- apply(pred, MARGIN = 1, which.max)
    ranks.obs &lt;- apply(obs2, MARGIN = 1, which.max)
    ## derive confusion matrix:
    cf &lt;- mda::confusion(names(obs2)[ranks.obs], names(pred)[ranks.pred])
    c.kappa &lt;- psych::cohen.kappa(cbind(names(obs2)[ranks.obs], names(pred)[ranks.pred]))
    purity &lt;- sum(diag(cf))/sum(cf) * 100
    ## Accuracy for Binomial var
    ## [http://www.r-bloggers.com/evaluating-logistic-regression-models/]:
    TPR &lt;- sapply(1:ncol(obs2), function(c) {
        mean(performance(prediction(pred[, c], obs2[, c]), measure = &quot;tpr&quot;)@y.values[[1]])
    })
    AUC &lt;- sapply(1:ncol(obs2), function(c) {
        performance(prediction(pred[, c], obs2[, c]), measure = &quot;auc&quot;)@y.values[[1]]
    })
    cv.r &lt;- list(obs2, pred, error, data.frame(ME = mean.error, TPR = TPR, 
        AUC = AUC, N = N.tot), cf, c.kappa, purity)
    names(cv.r) &lt;- c(&quot;Observed&quot;, &quot;Predicted&quot;, &quot;CV_residuals&quot;, &quot;Classes&quot;, 
        &quot;Confusion.matrix&quot;, &quot;Cohen.Kappa&quot;, &quot;Purity&quot;)
    return(cv.r)
}</code></pre>
<p>##CO Models</p>
<pre class="r"><code># register parallel backend
parallelStartSocket(cpus = 24, show.info = TRUE)

# Create covariate datasets
#---------------CO All covariate dataframe and task
co.esg.all.cov &lt;- co.points.covariates[, -1]

co.esg.all.cov &lt;- na.omit(co.esg.all.cov)
co.esg.all.cov.rm &lt;- removeConstantFeatures(co.esg.all.cov)
CO_all_var = makeClassifTask(id = &quot;CO_ESG_all&quot;, data = co.esg.all.cov.rm, 
    target = &quot;esite&quot;)

#---------------CO Hyper-temp covariate dataframe and task
co.esg.hyper.cov &lt;- co.points.covariates[, c(&quot;esite&quot;, hype.ndvi.names, 
    hype.swir2.names, hype.lst_day.names, hype.lst_night.names)]

co.esg.hyper.cov &lt;- na.omit(co.esg.hyper.cov)
co.esg.hyper.cov.rm &lt;- removeConstantFeatures(co.esg.hyper.cov)
CO_hyper_var = makeClassifTask(id = &quot;CO_ESG_hyper&quot;, data = co.esg.hyper.cov.rm, 
    target = &quot;esite&quot;)

#---------------CO NDVI covariate dataframe and task
co.esg.ndvi.cov &lt;- co.points.covariates[, c(&quot;esite&quot;, hype.ndvi.names)]

co.esg.ndvi.cov &lt;- na.omit(co.esg.ndvi.cov)
co.esg.ndvi.cov.rm &lt;- removeConstantFeatures(co.esg.ndvi.cov)
CO_ndvi_var = makeClassifTask(id = &quot;CO_ESG_ndvi&quot;, data = co.esg.ndvi.cov.rm, 
    target = &quot;esite&quot;)

#---------------CO Abiotic covariate dataframe and task
co.esg.abiotic.cov &lt;- co.points.covariates[, c(&quot;esite&quot;, terrain.co.names, 
    lithology.co.names, prec.co.names, temp.co.names, bio.co.names, namrad.co.names, 
    soil.thick.co.names, soilGrids.co.names)]

co.esg.abiotic.cov &lt;- na.omit(co.esg.abiotic.cov)
co.esg.abiotic.cov.rm &lt;- removeConstantFeatures(co.esg.abiotic.cov)
CO_abiotic_var = makeClassifTask(id = &quot;CO_ESG_abiotic&quot;, data = co.esg.abiotic.cov.rm, 
    target = &quot;esite&quot;)

#---------------CO SoilsGrid covariate dataframe and task
co.esg.sg.cov &lt;- co.points.covariates[, c(&quot;esite&quot;, soilGrids.co.names)]

co.esg.sg.cov &lt;- na.omit(co.esg.sg.cov)
co.esg.sg.cov.rm &lt;- removeConstantFeatures(co.esg.sg.cov)
CO_sg_var = makeClassifTask(id = &quot;CO_ESG_sg&quot;, data = co.esg.sg.cov.rm, 
    target = &quot;esite&quot;)

#---------------CO DSM covariate dataframe and task
co.esg.dsm.cov &lt;- co.points.covariates[, c(&quot;esite&quot;, terrain.co.names, lithology.co.names, 
    prec.co.names, temp.co.names, bio.co.names, namrad.co.names, soil.thick.co.names, 
    soilGrids.co.names, ndvi.co.names, mir.co.names, lst.day.co.names, 
    lst.night.co.names)]

co.esg.dsm.cov &lt;- na.omit(co.esg.dsm.cov)
co.esg.dsm.cov.rm &lt;- removeConstantFeatures(co.esg.dsm.cov)
CO_dsm_var = makeClassifTask(id = &quot;CO_ESG_dsm&quot;, data = co.esg.dsm.cov.rm, 
    target = &quot;esite&quot;)


#--------------All covariate models-------------------------------------------------------------------------------
stime1 &lt;- system.time({
    CO_all_lrns &lt;- multiLearner.preprocess(task = CO_all_var)
})[3]
stime1

## Choose the resampling strategy
rdesc.10fcv = makeResampleDesc(method = &quot;CV&quot;, iters = 10, stratify = TRUE)
m = list(acc, kappa, mmce, timetrain)

# Apply resampling function
stime2 &lt;- system.time({
    CO_all_resample &lt;- resample.multi.learner(CO_all_lrns, CO_all_var, 
        rdesc.10fcv, m)
})[3]
stime2
save(CO_all_resample, CO_all_lrns, file = &quot;/data/data2/data/esgMapping/R/CO_all_resample.RData&quot;)

#---------------------------------------------------------------------------------------

CO_all.svm.pred = getRRPredictions(CO_all_resample[[1]])
CO_all.svm.cm &lt;- calculateConfusionMatrix(CO_all.svm.pred, relative = TRUE)
CO_all.rf.pred = getRRPredictions(CO_all_resample[[2]])
CO_all.rf.cm &lt;- calculateConfusionMatrix(CO_all.rf.pred, relative = TRUE)
CO_all.xgb.pred = getRRPredictions(CO_all_resample[[3]])
CO_all.xgb.cm &lt;- calculateConfusionMatrix(CO_all.xgb.pred, relative = TRUE)
CO_all.ens.pred = getRRPredictions(CO_all_resample[[4]])
CO_all.ens.cm &lt;- calculateConfusionMatrix(CO_all.ens.pred, relative = TRUE)

save(CO_all_resample, CO_all_lrns, file = &quot;/data/data2/data/esgMapping/R/CO_all_resample.RData&quot;)
rm(CO_all_resample)

#--------------Hyper covariate models-------------------------------------------------------------------------------
stime1 &lt;- system.time({
    CO_hyper_lrns &lt;- multiLearner.preprocess(task = CO_hyper_var)
})[3]
stime1

## Choose the resampling strategy
rdesc.10fcv = makeResampleDesc(method = &quot;CV&quot;, iters = 10, stratify = TRUE)
m = list(acc, kappa, mmce, timetrain)

# Apply resampling function
stime2 &lt;- system.time({
    CO_hyper_resample &lt;- resample.multi.learner(CO_hyper_lrns, CO_hyper_var, 
        rdesc.10fcv, m)
})[3]
stime2
save(CO_hyper_resample, CO_hyper_lrns, file = &quot;/data/data2/data/esgMapping/R/CO_hyper_resample.RData&quot;)

#---------------------------------------------------------------------------------------

CO_hyper.svm.pred = getRRPredictions(CO_hyper_resample[[1]])
CO_hyper.svm.cm &lt;- calculateConfusionMatrix(CO_hyper.svm.pred, relative = TRUE)
CO_hyper.rf.pred = getRRPredictions(CO_hyper_resample[[2]])
CO_hyper.rf.cm &lt;- calculateConfusionMatrix(CO_hyper.rf.pred, relative = TRUE)
CO_hyper.xgb.pred = getRRPredictions(CO_hyper_resample[[3]])
CO_hyper.xgb.cm &lt;- calculateConfusionMatrix(CO_hyper.xgb.pred, relative = TRUE)
CO_hyper.ens.pred = getRRPredictions(CO_hyper_resample[[4]])
CO_hyper.ens.cm &lt;- calculateConfusionMatrix(CO_hyper.ens.pred, relative = TRUE)

save(CO_hyper_resample, CO_hyper_lrns, file = &quot;/data/data2/data/esgMapping/R/CO_hyper_resample.RData&quot;)
rm(CO_hyper_resample)

#----------------------------------------------------------------------------------------
#--------------NDVI covariate models----------------------------------------------------
stime1 &lt;- system.time({
    CO_ndvi_lrns &lt;- multiLearner.preprocess(task = CO_ndvi_var)
})[3]
stime1

## Choose the resampling strategy
rdesc.10fcv = makeResampleDesc(method = &quot;CV&quot;, iters = 10, stratify = TRUE)
m = list(acc, kappa, mmce, timetrain)

# # Apply resampling function
stime2 &lt;- system.time({
    CO_ndvi_resample &lt;- resample.multi.learner(CO_ndvi_lrns, CO_ndvi_var, 
        rdesc.10fcv, m)
})[3]
stime2
save(CO_ndvi_resample, &quot;/data/data2/data/esgMapping/R/CO_ndvi_resample.RData&quot;)

#---------------------------------------------------------------------------------------

CO_ndvi.svm.pred = getRRPredictions(CO_ndvi_resample[[1]])
CO_ndvi.svm.cm &lt;- calculateConfusionMatrix(CO_ndvi.svm.pred, relative = TRUE)
CO_ndvi.rf.pred = getRRPredictions(CO_ndvi_resample[[2]])
CO_ndvi.rf.cm &lt;- calculateConfusionMatrix(CO_ndvi.rf.pred, relative = TRUE)
CO_ndvi.xgb.pred = getRRPredictions(CO_ndvi_resample[[3]])
CO_ndvi.xgb.cm &lt;- calculateConfusionMatrix(CO_ndvi.xgb.pred, relative = TRUE)
CO_ndvi.ens.pred = getRRPredictions(CO_ndvi_resample[[4]])
CO_ndvi.ens.cm &lt;- calculateConfusionMatrix(CO_ndvi.ens.pred, relative = TRUE)

save(CO_ndvi_resample, CO_ndvi_lrns, file = &quot;/data/data2/data/esgMapping/R/CO_ndvi_resample.RData&quot;)
rm(CO_ndvi_resample)

#----------------------------------------------------------------------------------------
#--------------Abiotic models-----------------------------------------------------------
stime1 &lt;- system.time({
    CO_abiotic_lrns &lt;- multiLearner.preprocess(task = CO_abiotic_var)
})[3]
stime1

## Choose the resampling strategy
rdesc.10fcv = makeResampleDesc(method = &quot;CV&quot;, iters = 10, stratify = TRUE)
m = list(acc, kappa, mmce, timetrain)

# Apply resampling function
stime2 &lt;- system.time({
    CO_abiotic_resample &lt;- resample.multi.learner(CO_abiotic_lrns, CO_abiotic_var, 
        rdesc.10fcv, m)
})[3]
stime2
save(CO_abiotic_resample, &quot;/data/data2/data/esgMapping/R/CO_abiotic_resample.RData&quot;)

#----------------------------------------------------------------------------------------

CO_abiotic.svm.pred = getRRPredictions(CO_abiotic_resample[[1]])
CO_abiotic.svm.cm &lt;- calculateConfusionMatrix(CO_abiotic.svm.pred, relative = TRUE)
CO_abiotic.rf.pred = getRRPredictions(CO_abiotic_resample[[2]])
CO_abiotic.rf.cm &lt;- calculateConfusionMatrix(CO_abiotic.rf.pred, relative = TRUE)
CO_abiotic.xgb.pred = getRRPredictions(CO_abiotic_resample[[3]])
CO_abiotic.xgb.cm &lt;- calculateConfusionMatrix(CO_abiotic.xgb.pred, relative = TRUE)
CO_abiotic.ens.pred = getRRPredictions(CO_abiotic_resample[[4]])
CO_abiotic.ens.cm &lt;- calculateConfusionMatrix(CO_abiotic.ens.pred, relative = TRUE)

save(CO_abiotic_resample, CO_abiotic_lrns, file = &quot;/data/data2/data/esgMapping/R/CO_abiotic_resample.RData&quot;)
rm(CO_abiotic_resample)

#----------------------------------------------------------------------------------------
#--------------SG covariate models------------------------------------------------------
stime1 &lt;- system.time({
    CO_sg_lrns &lt;- multiLearner.preprocess(task = CO_sg_var)
})[3]
stime1

## Choose the resampling strategy
rdesc.10fcv = makeResampleDesc(method = &quot;CV&quot;, iters = 10, stratify = TRUE)
m = list(acc, kappa, mmce, timetrain)

# Apply resampling function
stime2 &lt;- system.time({
    CO_sg_resample &lt;- resample.multi.learner(CO_sg_lrns, CO_sg_var, rdesc.10fcv, 
        m)
})[3]
stime2
save(CO_sg_resample, &quot;/data/data2/data/esgMapping/R/CO_sg_resample.RData&quot;)

#----------------------------------------------------------------------------------------

CO_sg.svm.pred = getRRPredictions(CO_sg_resample[[1]])
CO_sg.svm.cm &lt;- calculateConfusionMatrix(CO_sg.svm.pred, relative = TRUE)
CO_sg.rf.pred = getRRPredictions(CO_sg_resample[[2]])
CO_sg.rf.cm &lt;- calculateConfusionMatrix(CO_sg.rf.pred, relative = TRUE)
CO_sg.xgb.pred = getRRPredictions(CO_sg_resample[[3]])
CO_sg.xgb.cm &lt;- calculateConfusionMatrix(CO_sg.xgb.pred, relative = TRUE)
CO_sg.ens.pred = getRRPredictions(CO_sg_resample[[4]])
CO_sg.ens.cm &lt;- calculateConfusionMatrix(CO_sg.ens.pred, relative = TRUE)

save(CO_sg_resample, CO_sg_lrns, file = &quot;/data/data2/data/esgMapping/R/CO_sg_resample.RData&quot;)
rm(CO_sg_resample)

#----------------------------------------------------------------------------------------
#--------------DSM covariate models-----------------------------------------------------
stime1 &lt;- system.time({
    CO_dsm_lrns &lt;- multiLearner.preprocess(task = CO_dsm_var)
})[3]
stime1

## Choose the resampling strategy
rdesc.10fcv = makeResampleDesc(method = &quot;CV&quot;, iters = 10, stratify = TRUE)
m = list(acc, kappa, mmce, timetrain)

# Apply resampling function
stime2 &lt;- system.time({
    CO_dsm_resample &lt;- resample.multi.learner(CO_dsm_lrns, CO_dsm_var, 
        rdesc.10fcv, m)
})[3]
stime2
save(CO_dsm_resample, &quot;/data/data2/data/esgMapping/R/CO_dsm_resample.RData&quot;)

stime2 &lt;- system.time({
    CO_dsm_resample_prob &lt;- resample.multi.learner(CO_dsm_lrns.prob, CO_dsm_var, 
        rdesc.10fcv, m)
})[3]
stime2

CO_dsm_svm_acc &lt;- classwise_accuracy_stats(CO_dsm_resample_prob[[1]])
CO_dsm_rf_acc &lt;- classwise_accuracy_stats(CO_dsm_resample_prob[[2]])
CO_dsm_xgb_acc &lt;- classwise_accuracy_stats(CO_dsm_resample_prob[[3]])
CO_dsm_ens_acc &lt;- classwise_accuracy_stats(CO_dsm_resample_prob[[4]])
#----------------------------------------------------------------------------------------

CO_dsm.svm.pred = getRRPredictions(CO_dsm_resample[[1]])
CO_dsm.svm.cm &lt;- calculateConfusionMatrix(CO_dsm.svm.pred, relative = TRUE)
CO_dsm.rf.pred = getRRPredictions(CO_dsm_resample[[2]])
CO_dsm.rf.cm &lt;- calculateConfusionMatrix(CO_dsm.rf.pred, relative = TRUE)
CO_dsm.xgb.pred = getRRPredictions(CO_dsm_resample[[3]])
CO_dsm.xgb.cm &lt;- calculateConfusionMatrix(CO_dsm.xgb.pred, relative = TRUE)
CO_dsm.ens.pred = getRRPredictions(CO_dsm_resample[[4]])
CO_dsm.ens.cm &lt;- calculateConfusionMatrix(CO_dsm.ens.pred, relative = TRUE)

save(CO_dsm_resample, CO_dsm_lrns, file = &quot;/data/data2/data/esgMapping/R/CO_dsm_resample.RData&quot;)
rm(CO_dsm_resample)

# Calculate qunatity and allocation dissagreement
CO_dsm_ens.kstat &lt;- kstat(CO_dsm.ens.pred$data$response, CO_dsm.ens.pred$data$truth)

#-----------------------------------------------------------------------------------------------
# Calculate multiclass Precision-Recall AUC with boostrapped 95% CI
model = CO_dsm_resample_prob[[4]]

n.l &lt;- plyr::count(model$pred$data$truth)
n.l &lt;- data.frame(matrix(n.l$freq, nrow = 1, dimnames = list(1, paste(n.l$x))))
probs &lt;- model$pred$data[, 3:10]
names &lt;- colnames(n.l)
names(probs) = names
obs &lt;- data.frame(lapply(names, FUN = function(i) {
    ifelse(model$pred$data$truth == i, 1, 0)
}))
names(obs) = names
obs.pred &lt;- bind_cols(obs, probs)
colnames(obs.pred) &lt;- c(&quot;G1_true&quot;, &quot;G2_true&quot;, &quot;G3_true&quot;, &quot;G4_true&quot;, &quot;G5_true&quot;, 
    &quot;G6_true&quot;, &quot;G7_true&quot;, &quot;G8_true&quot;, &quot;G1_pred_m1&quot;, &quot;G2_pred_m1&quot;, &quot;G3_pred_m1&quot;, 
    &quot;G4_pred_m1&quot;, &quot;G5_pred_m1&quot;, &quot;G6_pred_m1&quot;, &quot;G7_pred_m1&quot;, &quot;G8_pred_m1&quot;)

# Generate ROC and PR curve data
roc_res &lt;- multi_roc(obs.pred, force_diag = T)
pr_res &lt;- multi_pr(obs.pred, force_diag = T)

# Plot PR curves
plot_pr_df &lt;- plot_pr_data(pr_res)
ggplot(plot_pr_df, aes(x = Recall, y = Precision)) + geom_path(aes(color = Group, 
    linetype = Method), size = 1.5) + theme_bw() + theme(plot.title = element_text(hjust = 0.5), 
    legend.justification = c(1, 0), legend.position = c(0.95, 0.05), legend.title = element_blank(), 
    legend.background = element_rect(fill = NULL, size = 0.5, linetype = &quot;solid&quot;, 
        colour = &quot;black&quot;))

# Calculate PR data with confidence intervals
CO_dsm_pr_auc_ci &lt;- pr_auc_with_ci(obs.pred, conf = 0.95, type = &quot;basic&quot;, 
    R = 100)

#-----------------------------------------------------------------------------------------------
multiclass.AUNU.class = function(probabilities, truth) {
    if (length(unique(truth)) != nlevels(truth)) {
        warning(&quot;Measure is undefined if there isn&#39;t at least one sample per class.&quot;)
        return(NA_real_)
    }
    BBmisc::vnapply(1:nlevels(truth), function(i) caTools::colAUC(probabilities[, 
        i], truth == levels(truth)[i]))
}
#-----------------------------------------------------------------------------------------------
TPR.multiclass = function(truth, response) {
    if (length(unique(truth)) != nlevels(truth)) {
        warning(&quot;Measure is undefined if there isn&#39;t at least one sample per class.&quot;)
        return(NA_real_)
    }
    signif(BBmisc::vnapply(1:nlevels(truth), function(i) sum(truth == response &amp; 
        response == levels(truth)[i])/sum(truth == levels(truth)[i])), 
        2)
}
#-----------------------------------------------------------------------------------------------
PPV.multiclass = function(truth, response) {
    signif(BBmisc::vnapply(1:nlevels(truth), function(i) measures::TP(truth, 
        response, levels(truth)[i])/sum(response == levels(truth)[i])), 
        2)
}

#-----------------------------------------------------------------------------------------------

CO_dsm.ens.pred.prob &lt;- CO_dsm.ens.pred$data[, 3:10] %&gt;% set_names(&quot;ES1&quot;, 
    &quot;ES2&quot;, &quot;ES3&quot;, &quot;ES4&quot;, &quot;ES5&quot;, &quot;ES6&quot;, &quot;ES7&quot;, &quot;ES99&quot;)

MMCE(CO_dsm.ens.pred$data$truth, CO_dsm.ens.pred$data$response)
ACC(CO_dsm.ens.pred$data$truth, CO_dsm.ens.pred$data$response)
BER(CO_dsm.ens.pred$data$truth, CO_dsm.ens.pred$data$response)
KAPPA(CO_dsm.ens.pred$data$truth, CO_dsm.ens.pred$data$response)
WKAPPA(CO_dsm.ens.pred$data$truth, CO_dsm.ens.pred$data$response)
TPR(CO_dsm.ens.pred$data$truth, CO_dsm.ens.pred$data$response)

CO_dsm.ens.tpr &lt;- TPR.multiclass(CO_dsm.ens.pred$data$truth, CO_dsm.ens.pred$data$response)
CO_dsm.ens.ppv &lt;- PPV.multiclass(CO_dsm.ens.pred$data$truth, CO_dsm.ens.pred$data$response)
CO_dsm.ens.aunu &lt;- multiclass.AUNU.class(CO_dsm.ens.pred$data[, 3:10], 
    CO_dsm.ens.pred$data$truth)
CO_dsm.ens.logloss &lt;- Logloss(CO_dsm.ens.pred.prob, CO_dsm.ens.pred$data$truth)
#----------------------------------------------------------------------------------------

#--------------------------------------------------------------------------------------------
load(&quot;/data/data2/data/esgMapping/R/CO_all_resample.RData&quot;)
co.all.resamp.metrics &lt;- cbind(rep(&quot;All&quot;, 4), c(&quot;SVM&quot;, &quot;RF&quot;, &quot;XGB&quot;, &quot;ENS&quot;), 
    data.frame(rbind(CO_all_resample[[1]]$aggr, CO_all_resample[[2]]$aggr, 
        CO_all_resample[[3]]$aggr, CO_all_resample[[4]]$aggr)))
colnames(co.all.resamp.metrics)[1] &lt;- &quot;Data&quot;
colnames(co.all.resamp.metrics)[2] &lt;- &quot;Model&quot;
rm(CO_all_resample)
load(&quot;/data/data2/data/esgMapping/R/CO_hyper_resample.RData&quot;)
co.hyper.resamp.metrics &lt;- cbind(rep(&quot;Hyper&quot;, 4), c(&quot;SVM&quot;, &quot;RF&quot;, &quot;XGB&quot;, 
    &quot;ENS&quot;), data.frame(rbind(CO_hyper_resample[[1]]$aggr, CO_hyper_resample[[2]]$aggr, 
    CO_hyper_resample[[3]]$aggr, CO_hyper_resample[[4]]$aggr)))
colnames(co.hyper.resamp.metrics)[1] &lt;- &quot;Data&quot;
colnames(co.hyper.resamp.metrics)[2] &lt;- &quot;Model&quot;
rm(CO_hyper_resample)
load(&quot;/data/data2/data/esgMapping/R/CO_ndvi_resample.RData&quot;)
co.ndvi.resamp.metrics &lt;- cbind(rep(&quot;NDVI&quot;, 4), c(&quot;SVM&quot;, &quot;RF&quot;, &quot;XGB&quot;, &quot;ENS&quot;), 
    data.frame(rbind(CO_ndvi_resample[[1]]$aggr, CO_ndvi_resample[[2]]$aggr, 
        CO_ndvi_resample[[3]]$aggr, CO_ndvi_resample[[4]]$aggr)))
colnames(co.ndvi.resamp.metrics)[1] &lt;- &quot;Data&quot;
colnames(co.ndvi.resamp.metrics)[2] &lt;- &quot;Model&quot;
rm(CO_ndvi_resample)
load(&quot;/data/data2/data/esgMapping/R/CO_abiotic_resample.RData&quot;)
co.Abiotic.resamp.metrics &lt;- cbind(rep(&quot;Abiotic&quot;, 4), c(&quot;SVM&quot;, &quot;RF&quot;, &quot;XGB&quot;, 
    &quot;ENS&quot;), data.frame(rbind(CO_abiotic_resample[[1]]$aggr, CO_abiotic_resample[[2]]$aggr, 
    CO_abiotic_resample[[3]]$aggr, CO_abiotic_resample[[4]]$aggr)))
colnames(co.Abiotic.resamp.metrics)[1] &lt;- &quot;Data&quot;
colnames(co.Abiotic.resamp.metrics)[2] &lt;- &quot;Model&quot;
rm(CO_abiotic_resample)
load(&quot;/data/data2/data/esgMapping/R/CO_sg_resample.RData&quot;)
co.sg.resamp.metrics &lt;- cbind(rep(&quot;SG&quot;, 4), c(&quot;SVM&quot;, &quot;RF&quot;, &quot;XGB&quot;, &quot;ENS&quot;), 
    data.frame(rbind(CO_sg_resample[[1]]$aggr, CO_sg_resample[[2]]$aggr, 
        CO_sg_resample[[3]]$aggr, CO_sg_resample[[4]]$aggr)))
colnames(co.sg.resamp.metrics)[1] &lt;- &quot;Data&quot;
colnames(co.sg.resamp.metrics)[2] &lt;- &quot;Model&quot;
rm(CO_sg_resample)
load(&quot;/data/data2/data/esgMapping/R/CO_dsm_resample.RData&quot;)
co.dsm.resamp.metrics &lt;- cbind(rep(&quot;DSM&quot;, 4), c(&quot;SVM&quot;, &quot;RF&quot;, &quot;XGB&quot;, &quot;ENS&quot;), 
    data.frame(rbind(CO_dsm_resample[[1]]$aggr, CO_dsm_resample[[2]]$aggr, 
        CO_dsm_resample[[3]]$aggr, CO_dsm_resample[[4]]$aggr)))
colnames(co.dsm.resamp.metrics)[1] &lt;- &quot;Data&quot;
colnames(co.dsm.resamp.metrics)[2] &lt;- &quot;Model&quot;
rm(CO_dsm_resample)

# Combine resample results
co.resamp.metrics &lt;- rbind(co.all.resamp.metrics, co.hyper.resamp.metrics, 
    co.ndvi.resamp.metrics, co.Abiotic.resamp.metrics, co.sg.resamp.metrics, 
    co.dsm.resamp.metrics)


theme_set(theme_classic())

# Plot
ggplot(co.resamp.metrics, aes(x = Data, y = acc.test.mean)) + geom_jitter(aes(colour = factor(Model)), 
    size = 4, width = 0.2, height = 0) + # geom_point(col=&#39;tomato2&#39;, size=3) + # Draw points
geom_segment(aes(x = Data, xend = Data, y = 0.63, yend = 0.8), linetype = &quot;dashed&quot;, 
    size = 0.1) + # coord_flip() + # Draw dashed lines
ylab(&quot;Accuracy&quot;) + xlab(&quot;Dataset&quot;) + # coord_flip() + # Draw dashed lines
labs(title = &quot;MLRA 35 ESG Model Accuracy&quot;, color = &quot;Model&quot;)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_model_comparison5.pdf&quot;, 
    width = 4.3, height = 2.6)


# Calculate individual class accuracies
CO_classwise_model &lt;- bind_rows(CO_all.svm.cm$relative.row[, 9], CO_all.rf.cm$relative.row[, 
    9], CO_all.xgb.cm$relative.row[, 9], CO_all.ens.cm$relative.row[, 9], 
    CO_hyper.svm.cm$relative.row[, 9], CO_hyper.rf.cm$relative.row[, 9], 
    CO_hyper.xgb.cm$relative.row[, 9], CO_hyper.ens.cm$relative.row[, 9], 
    CO_ndvi.svm.cm$relative.row[, 9], CO_ndvi.rf.cm$relative.row[, 9], 
    CO_ndvi.xgb.cm$relative.row[, 9], CO_ndvi.ens.cm$relative.row[, 9], 
    CO_abiotic.svm.cm$relative.row[, 9], CO_abiotic.rf.cm$relative.row[, 
        9], CO_abiotic.xgb.cm$relative.row[, 9], CO_abiotic.ens.cm$relative.row[, 
        9], CO_sg.svm.cm$relative.row[, 9], CO_sg.rf.cm$relative.row[, 
        9], CO_sg.xgb.cm$relative.row[, 9], CO_sg.ens.cm$relative.row[, 
        9], CO_dsm.svm.cm$relative.row[, 9], CO_dsm.rf.cm$relative.row[, 
        9], CO_dsm.xgb.cm$relative.row[, 9], CO_dsm.ens.cm$relative.row[, 
        9])
CO_classwise_model - 1
CO.class.accuracy &lt;- data.frame(c(rep(&quot;All&quot;, 4), rep(&quot;Hyper&quot;, 4), rep(&quot;NDVI&quot;, 
    4), rep(&quot;Abiotic&quot;, 4), rep(&quot;SG&quot;, 4), rep(&quot;DSM&quot;, 4)), c(rep(c(&quot;SVM&quot;, 
    &quot;RF&quot;, &quot;XGB&quot;, &quot;ENS&quot;), 6))) %&gt;% set_names(&quot;Data&quot;, &quot;Model&quot;) %&gt;% bind_cols(., 
    (1 - CO_classwise_model))

CO.class.accuracy$Data &lt;- factor(CO.class.accuracy$Data, levels = c(&quot;All&quot;, 
    &quot;Hyper&quot;, &quot;NDVI&quot;, &quot;Abiotic&quot;, &quot;SG&quot;, &quot;DSM&quot;))

ESG.class.plot &lt;- function(data, class, title, filename, scale) {
    ggplot(data, aes(x = data[, 1], y = data[, class])) + geom_jitter(aes(colour = factor(Model)), 
        size = 4, width = 0.2, height = 0) + ylab(&quot;Accuracy&quot;) + xlab(&quot;Dataset&quot;) + 
        # coord_flip() + # Draw dashed lines
    labs(title = title, color = &quot;Model&quot;)
    ggsave(paste0(&quot;/data/data2/data/esgMapping/analysis/figures/&quot;, filename, 
        &quot;_class_comparison.pdf&quot;), scale = scale)
}

ESG.class.plot(data = CO.class.accuracy, class = 3, title = &quot;MLRA 35 ESG Class 1&quot;, 
    filename = &quot;CO_ESG1&quot;, scale = 1)
ESG.class.plot(data = CO.class.accuracy, class = 3, title = &quot;MLRA 35 ESG Class 1&quot;, 
    filename = &quot;CO_ESG1&quot;, scale = 0.6)
ESG.class.plot(data = CO.class.accuracy, class = 4, title = &quot;MLRA 35 ESG Class 2&quot;, 
    filename = &quot;CO_ESG2&quot;, scale = 0.6)
ESG.class.plot(data = CO.class.accuracy, class = 5, title = &quot;MLRA 35 ESG Class 3&quot;, 
    filename = &quot;CO_ESG3&quot;, scale = 0.6)
ESG.class.plot(data = CO.class.accuracy, class = 6, title = &quot;MLRA 35 ESG Class 4&quot;, 
    filename = &quot;CO_ESG4&quot;, scale = 0.6)
ESG.class.plot(data = CO.class.accuracy, class = 7, title = &quot;MLRA 35 ESG Class 5&quot;, 
    filename = &quot;CO_ESG5&quot;, scale = 0.6)
ESG.class.plot(data = CO.class.accuracy, class = 8, title = &quot;MLRA 35 ESG Class 6&quot;, 
    filename = &quot;CO_ESG6&quot;, scale = 0.6)
ESG.class.plot(data = CO.class.accuracy, class = 9, title = &quot;MLRA 35 ESG Class 7&quot;, 
    filename = &quot;CO_ESG7&quot;, scale = 0.6)
ESG.class.plot(data = CO.class.accuracy, class = 10, title = &quot;MLRA 35 ESG Class 8&quot;, 
    filename = &quot;CO_ESG8&quot;, scale = 0.6)

# Evaluate ESD misclassification contribution to ESG class accuracy
CO_dsm_pred_ens &lt;- CO_dsm_resample_prob[[4]]$pred$data
co.esname &lt;- co.points@data[co.points.covariates$Index, c(1, 8, 12)] %&gt;% 
    set_names(c(&quot;id&quot;, &quot;esdname&quot;, &quot;ESG_ID&quot;)) %&gt;% mutate(id = seq(1, length(id), 
    1))
co.esname$id &lt;- as.integer(co.esname$id)


CO_dsm_pred_ens.all &lt;- CO_dsm_pred_ens %&gt;% select(c(id, truth, response)) %&gt;% 
    left_join(., co.esname, by = &quot;id&quot;)
CO_dsm_pred_ens.misclass &lt;- CO_dsm_pred_ens %&gt;% filter(truth != response) %&gt;% 
    select(c(id, truth, response))

# ES1
CO_dsm_pred_ens.misclass.es1 &lt;- CO_dsm_pred_ens.misclass %&gt;% filter(truth == 
    &quot;ES1&quot;) %&gt;% group_by(esdname) %&gt;% summarise(no_rows = length(esdname))
View(CO_dsm_pred_ens.misclass.es1)

# ES2
#----------------------------------------------------------------------------------------  
# Evaluate ES2 misclassification ESD class
CO_dsm_pred_ens.all.es2 &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES2&quot;) %&gt;% 
    group_by(response) %&gt;% summarise(no_rows = length(response))
# View(CO_dsm_pred_ens.all.es2)

CO_dsm_pred_ens.misclass.es2 &lt;- CO_dsm_pred_ens.misclass %&gt;% filter(truth == 
    &quot;ES2&quot;) %&gt;% group_by(esdname) %&gt;% summarise(no_rows = length(esdname))
# View(CO_dsm_pred_ens.misclass.es2)

# combine number of all ESD classes and misclasses
CO_dsm_pred_ens.mis.es2 &lt;- CO_dsm_pred_ens.all.es2 %&gt;% left_join(CO_dsm_pred_ens.misclass.es2, 
    by = &quot;esdname&quot;)
CO_dsm_pred_ens.mis.es2$misclassRate &lt;- CO_dsm_pred_ens.mis.es2$no_rows.y/CO_dsm_pred_ens.mis.es2$no_rows.x

ES_misClass &lt;- data.frame(paste0(&quot;ES&quot;, seq(1:8))) %&gt;% set_names(&quot;ESD&quot;)
for (i in 1L:length(CO_dsm_pred_ens.mis.es2$esdname)) {
    esd_dist &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES2&quot;) %&gt;% filter(str_detect(esdname, 
        gsub(&quot;([.|()\\^{}+$*?]|\\[|\\])&quot;, &quot;\\\\\\1&quot;, as.character(CO_dsm_pred_ens.mis.es2$esdname[i])))) %&gt;% 
        group_by(response) %&gt;% summarise(no_rows = length(response))
    esd_dist &lt;- esd_dist %&gt;% set_names(&quot;ESD&quot;, &quot;num&quot;)
    ES_misClass &lt;- left_join(ES_misClass, esd_dist, by = &quot;ESD&quot;)
}
ES2_misClass &lt;- ES_misClass %&gt;% set_names(c(&quot;ESD&quot;, as.character(CO_dsm_pred_ens.mis.es2$esdname)))
ES2_misClass &lt;- ES2_misClass %&gt;% gather(key = &quot;ESD&quot;)
ES2_misClass &lt;- bind_cols(ES2_misClass, data.frame(rep(paste0(&quot;ES&quot;, c(1, 
    0, 3, 4, 5, 6, 7, 8)), length(unique(CO_dsm_pred_ens.mis.es2$esdname)))) %&gt;% 
    set_names(&quot;ESG&quot;))
ES2_misClass &lt;- ES2_misClass %&gt;% group_by(ESD) %&gt;% arrange(ESG, .by_group = TRUE)

ES2.DF &lt;- ES2_misClass %&gt;% group_by(ESD) %&gt;% mutate(ValuePer = (value/sum(value, 
    na.rm = TRUE))) %&gt;% arrange(ESG, .by_group = TRUE) %&gt;% ungroup()
# cbPalette &lt;- c( &#39;#ffa77f&#39;, &#39;#00a884&#39;, &#39;#beffe7&#39;, &#39;#ffffbf&#39;,
# &#39;#737301&#39;, &#39;#fe0000&#39;, &#39;#0071fe&#39;, &#39;#732500&#39;)
ggplot(ES2.DF, aes(ESD, ValuePer, fill = ESG)) + geom_bar(stat = &quot;identity&quot;, 
    position = position_fill(reverse = TRUE)) + geom_text(aes(label = percent(ValuePer)), 
    position = position_stack(reverse = TRUE)) + scale_y_continuous(labels = percent_format())
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ES2_ESD_misclass.pdf&quot;)


#----------------------------------------------------------------------------------------
# ES3 ESG class
CO_dsm_pred_ens.all.r.es3 &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES3&quot;) %&gt;% 
    group_by(response) %&gt;% summarise(no_rows = length(response))
View(CO_dsm_pred_ens.all.r.es3)

CO_dsm_pred_ens.misclass.r.es3 &lt;- CO_dsm_pred_ens.misclass %&gt;% filter(truth == 
    &quot;ES3&quot;) %&gt;% group_by(response) %&gt;% summarise(no_rows = length(response))
View(CO_dsm_pred_ens.misclass.r.es3)

#----------------------------------------------------------------------------------------  
# Evaluate ES3 misclassification ESD class
CO_dsm_pred_ens.all.es3 &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES3&quot;) %&gt;% 
    group_by(esdname) %&gt;% summarise(no_rows = length(esdname))
View(CO_dsm_pred_ens.all.es3)

CO_dsm_pred_ens.misclass.es3 &lt;- CO_dsm_pred_ens.misclass %&gt;% filter(truth == 
    &quot;ES3&quot;) %&gt;% group_by(esdname) %&gt;% summarise(no_rows = length(esdname))
View(CO_dsm_pred_ens.misclass.es3)

# combine number of all ESD classes and misclasses
CO_dsm_pred_ens.mis.es3 &lt;- CO_dsm_pred_ens.all.es3 %&gt;% left_join(CO_dsm_pred_ens.misclass.es3, 
    by = &quot;esdname&quot;)
CO_dsm_pred_ens.mis.es3$misclassRate &lt;- CO_dsm_pred_ens.mis.es3$no_rows.y/CO_dsm_pred_ens.mis.es3$no_rows.x
CO_dsm_pred_ens.mis.es3$esdname[1]


ES_misClass &lt;- data.frame(paste0(&quot;ES&quot;, seq(1:8))) %&gt;% set_names(&quot;ESD&quot;)
for (i in 1L:length(CO_dsm_pred_ens.mis.es3$esdname)) {
    esd_dist &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES3&quot;) %&gt;% filter(str_detect(esdname, 
        gsub(&quot;([.|()\\^{}+$*?]|\\[|\\])&quot;, &quot;\\\\\\1&quot;, as.character(CO_dsm_pred_ens.mis.es3$esdname[i])))) %&gt;% 
        group_by(response) %&gt;% summarise(no_rows = length(response))
    esd_dist &lt;- esd_dist %&gt;% set_names(&quot;ESD&quot;, &quot;num&quot;)
    ES_misClass &lt;- left_join(ES_misClass, esd_dist, by = &quot;ESD&quot;)
}
ES3_misClass &lt;- ES_misClass %&gt;% set_names(c(&quot;ESD&quot;, as.character(CO_dsm_pred_ens.mis.es3$esdname)))
ES3_misClass &lt;- ES3_misClass %&gt;% gather(key = &quot;ESD&quot;)
ES3_misClass &lt;- bind_cols(ES3_misClass, data.frame(rep(paste0(&quot;ES&quot;, c(1, 
    2, 0, 4, 5, 6, 7, 8)), length(unique(CO_dsm_pred_ens.mis.es3$esdname)))) %&gt;% 
    set_names(&quot;ESG&quot;))
ES3_misClass &lt;- ES3_misClass %&gt;% group_by(ESD) %&gt;% arrange(ESG, .by_group = TRUE)

ES3.DF &lt;- ES3_misClass %&gt;% group_by(ESD) %&gt;% mutate(ValuePer = (value/sum(value, 
    na.rm = TRUE))) %&gt;% arrange(ESG, .by_group = TRUE) %&gt;% ungroup()
# cbPalette &lt;- c( &#39;#ffa77f&#39;, &#39;#00a884&#39;, &#39;#beffe7&#39;, &#39;#ffffbf&#39;,
# &#39;#737301&#39;, &#39;#fe0000&#39;, &#39;#0071fe&#39;, &#39;#732500&#39;)
ggplot(ES3.DF, aes(ESD, ValuePer, fill = ESG)) + geom_bar(stat = &quot;identity&quot;, 
    position = position_fill(reverse = TRUE)) + geom_text(aes(label = percent(ValuePer)), 
    position = position_stack(reverse = TRUE)) + scale_y_continuous(labels = percent_format())
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ES3_ESD_misclass.pdf&quot;)

#----------------------------------------------------------------------------------------
# Look into individual ESD Classes and which ESG they are being
# misclassified as For ESG3, there are two ESDs that contribute to
# misclassification: 1) Semidesert Shallow Sandy Loam (Utah Juniper,
# Blackbrush), 2) Desert Shallow Sandy Loam (Blackbrush)

# The SSSL as a lower rate of misclassification but has much higher
# number of observations and misclassifications
CO_dsm_pred_ens.SSSL.es3 &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES3&quot;) %&gt;% 
    filter(str_detect(esdname, &quot;Semidesert Shallow Sandy Loam&quot;)) %&gt;% group_by(response) %&gt;% 
    summarise(no_rows = length(response))
View(CO_dsm_pred_ens.SSSL.es3)

# The DSSL has a higher misclassification rate but fewer total number
# of misclassications
CO_dsm_pred_ens.DSSL.es3 &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES3&quot;) %&gt;% 
    filter(str_detect(esdname, &quot;Desert Shallow Sandy Loam&quot;)) %&gt;% group_by(response) %&gt;% 
    summarise(no_rows = length(response))
View(CO_dsm_pred_ens.DSSL.es3)
#----------------------------------------------------------------------------------------
# ES4 ES4 ESG class
CO_dsm_pred_ens.all.r.es4 &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES4&quot;) %&gt;% 
    group_by(response) %&gt;% summarise(no_rows = length(response))
View(CO_dsm_pred_ens.all.r.es4)

CO_dsm_pred_ens.misclass.r.es4 &lt;- CO_dsm_pred_ens.misclass %&gt;% filter(truth == 
    &quot;ES4&quot;) %&gt;% group_by(response) %&gt;% summarise(no_rows = length(response))
View(CO_dsm_pred_ens.misclass.r.es4)

#---------------------------------------------------------------------------------------- 
# Evaluate ES4 misclassification ESD class
CO_dsm_pred_ens.all.es4 &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES4&quot;) %&gt;% 
    group_by(esdname) %&gt;% summarise(no_rows = length(esdname))
View(CO_dsm_pred_ens.all.es4)

CO_dsm_pred_ens.misclass.es4 &lt;- CO_dsm_pred_ens.misclass %&gt;% filter(truth == 
    &quot;ES4&quot;) %&gt;% group_by(esdname) %&gt;% summarise(no_rows = length(esdname))
View(CO_dsm_pred_ens.misclass.es4)

# combine number of all ESD classes and misclasses
CO_dsm_pred_ens.mis.es4 &lt;- CO_dsm_pred_ens.all.es4 %&gt;% left_join(CO_dsm_pred_ens.misclass.es4, 
    by = &quot;esdname&quot;)
CO_dsm_pred_ens.mis.es4$misclassRate &lt;- CO_dsm_pred_ens.mis.es4$no_rows.y/CO_dsm_pred_ens.mis.es4$no_rows.x
CO_dsm_pred_ens.mis.es4$esdname[1]


ES_misClass &lt;- data.frame(paste0(&quot;ES&quot;, seq(1:8))) %&gt;% set_names(&quot;ESD&quot;)
for (i in 1L:length(CO_dsm_pred_ens.mis.es4$esdname)) {
    esd_dist &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES4&quot;) %&gt;% filter(str_detect(esdname, 
        gsub(&quot;([.|()\\^{}+$*?]|\\[|\\])&quot;, &quot;\\\\\\1&quot;, as.character(CO_dsm_pred_ens.mis.es4$esdname[i])))) %&gt;% 
        group_by(response) %&gt;% summarise(no_rows = length(response))
    esd_dist &lt;- esd_dist %&gt;% set_names(&quot;ESD&quot;, &quot;num&quot;)
    ES_misClass &lt;- left_join(ES_misClass, esd_dist, by = &quot;ESD&quot;)
}
ES4_misClass &lt;- ES_misClass %&gt;% set_names(c(&quot;ESD&quot;, as.character(CO_dsm_pred_ens.mis.es4$esdname)))
ES4_misClass &lt;- ES4_misClass %&gt;% gather(key = &quot;ESD&quot;)
ES4_misClass &lt;- bind_cols(ES4_misClass, data.frame(rep(paste0(&quot;ES&quot;, c(1, 
    2, 3, 0, 5, 6, 7, 8)), length(unique(CO_dsm_pred_ens.mis.es4$esdname)))) %&gt;% 
    set_names(&quot;ESG&quot;))
ES4_misClass &lt;- ES4_misClass %&gt;% group_by(ESD) %&gt;% arrange(ESG, .by_group = TRUE)

ES4.DF &lt;- ES4_misClass %&gt;% group_by(ESD) %&gt;% mutate(ValuePer = (value/sum(value, 
    na.rm = TRUE))) %&gt;% arrange(ESG, .by_group = TRUE) %&gt;% ungroup()
# cbPalette &lt;- c( &#39;#ffa77f&#39;, &#39;#00a884&#39;, &#39;#beffe7&#39;, &#39;#ffffbf&#39;,
# &#39;#737301&#39;, &#39;#fe0000&#39;, &#39;#0071fe&#39;, &#39;#732500&#39;)
ggplot(ES4.DF, aes(ESD, ValuePer, fill = ESG)) + geom_bar(stat = &quot;identity&quot;, 
    position = position_fill(reverse = TRUE)) + geom_text(aes(label = percent(ValuePer)), 
    position = position_stack(reverse = TRUE)) + scale_y_continuous(labels = percent_format())
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ES4_ESD_misclass.pdf&quot;)

#----------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------
# ES5
#----------------------------------------------------------------------------------------
# ES6 ESG class
CO_dsm_pred_ens.all.r.es6 &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES6&quot;) %&gt;% 
    group_by(esdname) %&gt;% summarise(no_rows = length(esdname))
View(CO_dsm_pred_ens.all.r.es6)

CO_dsm_pred_ens.misclass.r.es6 &lt;- CO_dsm_pred_ens.misclass %&gt;% filter(truth == 
    &quot;ES6&quot;) %&gt;% group_by(esdname) %&gt;% summarise(no_rows = length(esdname))
View(CO_dsm_pred_ens.misclass.r.es6)

# combine number of all ESD classes and misclasses
CO_dsm_pred_ens.mis.es6 &lt;- CO_dsm_pred_ens.all.es6 %&gt;% left_join(CO_dsm_pred_ens.misclass.es6, 
    by = &quot;esdname&quot;)
View(CO_dsm_pred_ens.mis.es6)
CO_dsm_pred_ens.mis.es6$no_rows.y/CO_dsm_pred_ens.mis.es6$no_rows.x

# Individual classes
CO_dsm_pred_ens.SSRP.es6 &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES6&quot;) %&gt;% 
    filter(str_detect(esdname, &quot;Shallow Sand Rock Pocket&quot;)) %&gt;% group_by(response) %&gt;% 
    summarise(no_rows = length(response))
View(CO_dsm_pred_ens.SSRP.es6)

CO_dsm_pred_ens.SSSL.es6 &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES6&quot;) %&gt;% 
    filter(str_detect(esdname, &quot;Semidesert Steep Shallow Loam&quot;)) %&gt;% group_by(response) %&gt;% 
    summarise(no_rows = length(response))
View(CO_dsm_pred_ens.SSSL.es6)

CO_dsm_pred_ens.SVSS.es6 &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES6&quot;) %&gt;% 
    filter(str_detect(esdname, &quot;Semidesert Very Steep Stony Loam&quot;)) %&gt;% 
    group_by(response) %&gt;% summarise(no_rows = length(response))
View(CO_dsm_pred_ens.SVSS.es6)



# ESD class
CO_dsm_pred_ens.all.es6.resp &lt;- CO_dsm_pred_ens.all %&gt;% filter(truth == 
    &quot;ES6&quot;) %&gt;% group_by(response) %&gt;% summarise(no_rows = length(response))
View(CO_dsm_pred_ens.all.es6.resp)

CO_dsm_pred_ens.misclass.es3.resp &lt;- CO_dsm_pred_ens.misclass %&gt;% filter(truth == 
    &quot;ES6&quot;) %&gt;% group_by(response) %&gt;% summarise(no_rows = length(response))
View(CO_dsm_pred_ens.misclass.es3.resp)</code></pre>
<p>##NM Models</p>
<pre class="r"><code># register parallel backend
parallelStartSocket(cpus = 24, show.info = TRUE)
# Create covariate datasets

#---------------NM All covariate dataframe and task
nm.esg.all.cov &lt;- nm.points.covariates[, -1]

nm.esg.all.cov &lt;- na.omit(nm.esg.all.cov)
nm.esg.all.cov.rm &lt;- removeConstantFeatures(nm.esg.all.cov)
NM_all_var = makeClassifTask(id = &quot;NM_ESG_all&quot;, data = nm.esg.all.cov.rm, 
    target = &quot;esite&quot;)

#---------------NM Hyper-temp covariate dataframe and task
nm.esg.hyper.cov &lt;- nm.points.covariates[, c(&quot;esite&quot;, hype.ndvi.names, 
    hype.swir2.names, hype.lst_day.names, hype.lst_night.names)]

nm.esg.hyper.cov &lt;- na.omit(nm.esg.hyper.cov)
nm.esg.hyper.cov.rm &lt;- removeConstantFeatures(nm.esg.hyper.cov)
NM_hyper_var = makeClassifTask(id = &quot;NM_ESG_hyper&quot;, data = nm.esg.hyper.cov.rm, 
    target = &quot;esite&quot;)

#---------------NM NDVI covariate dataframe and task
nm.esg.ndvi.cov &lt;- nm.points.covariates[, c(&quot;esite&quot;, hype.ndvi.names)]

nm.esg.ndvi.cov &lt;- na.omit(nm.esg.ndvi.cov)
nm.esg.ndvi.cov.rm &lt;- removeConstantFeatures(nm.esg.ndvi.cov)
NM_ndvi_var = makeClassifTask(id = &quot;NM_ESG_ndvi&quot;, data = nm.esg.ndvi.cov.rm, 
    target = &quot;esite&quot;)

#---------------NM Abiotic covariate dataframe and task
nm.esg.abiotic.cov &lt;- nm.points.covariates[, c(&quot;esite&quot;, terrain.nm.names, 
    lithology.nm.names, prec.nm.names, temp.nm.names, bio.nm.names, soil.thick.nm.names, 
    soilGrids.nm.names)]

nm.esg.abiotic.cov &lt;- na.omit(nm.esg.abiotic.cov)
nm.esg.abiotic.cov.rm &lt;- removeConstantFeatures(nm.esg.abiotic.cov)
NM_abiotic_var = makeClassifTask(id = &quot;NM_ESG_abiotic&quot;, data = nm.esg.abiotic.cov.rm, 
    target = &quot;esite&quot;)

#---------------NM SoilsGrid covariate dataframe and task
nm.esg.sg.cov &lt;- nm.points.covariates[, c(&quot;esite&quot;, soilGrids.nm.names)]

nm.esg.sg.cov &lt;- na.omit(nm.esg.sg.cov)
nm.esg.sg.cov.rm &lt;- removeConstantFeatures(nm.esg.sg.cov)
NM_sg_var = makeClassifTask(id = &quot;NM_ESG_sg&quot;, data = nm.esg.sg.cov.rm, 
    target = &quot;esite&quot;)

#---------------NM DSM covariate dataframe and task
nm.esg.dsm.cov &lt;- nm.points.covariates[, c(&quot;esite&quot;, terrain.nm.names, lithology.nm.names, 
    prec.nm.names, temp.nm.names, bio.nm.names, soil.thick.nm.names, soilGrids.nm.names, 
    ndvi.nm.names, mir.nm.names, lst.day.nm.names, lst.night.nm.names)]

nm.esg.dsm.cov &lt;- na.omit(nm.esg.dsm.cov)
nm.esg.dsm.cov.rm &lt;- removeConstantFeatures(nm.esg.dsm.cov)
NM_dsm_var = makeClassifTask(id = &quot;NM_ESG_dsm&quot;, data = nm.esg.dsm.cov.rm, 
    target = &quot;esite&quot;)


#--------------All covariate models-------------------------------------------------------------------------------
stime1 &lt;- system.time({
    NM_all_lrns &lt;- multiLearner.preprocess(task = NM_all_var)
})[3]
stime1

## Choose the resampling strategy
rdesc.10fcv = makeResampleDesc(method = &quot;CV&quot;, iters = 10, stratify = TRUE)
m = list(acc, kappa, mmce, timetrain)

# Apply resampling function
stime2 &lt;- system.time({
    NM_all_resample &lt;- resample.multi.learner(NM_all_lrns, NM_all_var, 
        rdesc.10fcv, m)
})[3]
stime2


NM_all.svm.pred = getRRPredictions(NM_all_resample[[1]])
NM_all.svm.cm &lt;- calculateConfusionMatrix(NM_all.svm.pred, relative = TRUE)
NM_all.rf.pred = getRRPredictions(NM_all_resample[[2]])
NM_all.rf.cm &lt;- calculateConfusionMatrix(NM_all.rf.pred, relative = TRUE)
NM_all.xgb.pred = getRRPredictions(NM_all_resample[[3]])
NM_all.xgb.cm &lt;- calculateConfusionMatrix(NM_all.xgb.pred, relative = TRUE)
NM_all.ens.pred = getRRPredictions(NM_all_resample[[4]])
NM_all.ens.cm &lt;- calculateConfusionMatrix(NM_all.ens.pred, relative = TRUE)

save(NM_all_resample, NM_all_lrns, file = &quot;/data/data2/data/esgMapping/R/NM_all_resample.RData&quot;)
rm(NM_all_resample)

#--------------Hyper covariate models-------------------------------------------------------------------------------
stime1 &lt;- system.time({
    NM_hyper_lrns &lt;- multiLearner.preprocess(task = NM_hyper_var)
})[3]
stime1

## Choose the resampling strategy
rdesc.10fcv = makeResampleDesc(method = &quot;CV&quot;, iters = 10, stratify = TRUE)
m = list(acc, kappa, mmce, timetrain)

# Apply resampling function
stime2 &lt;- system.time({
    NM_hyper_resample &lt;- resample.multi.learner(NM_hyper_lrns, NM_hyper_var, 
        rdesc.10fcv, m)
})[3]
stime2

NM_hyper.svm.pred = getRRPredictions(NM_hyper_resample[[1]])
NM_hyper.svm.cm &lt;- calculateConfusionMatrix(NM_hyper.svm.pred, relative = TRUE)
NM_hyper.rf.pred = getRRPredictions(NM_hyper_resample[[2]])
NM_hyper.rf.cm &lt;- calculateConfusionMatrix(NM_hyper.rf.pred, relative = TRUE)
NM_hyper.xgb.pred = getRRPredictions(NM_hyper_resample[[3]])
NM_hyper.xgb.cm &lt;- calculateConfusionMatrix(NM_hyper.xgb.pred, relative = TRUE)
NM_hyper.ens.pred = getRRPredictions(NM_hyper_resample[[4]])
NM_hyper.ens.cm &lt;- calculateConfusionMatrix(NM_hyper.ens.pred, relative = TRUE)

save(NM_hyper_resample, NM_hyper_lrns, file = &quot;/data/data2/data/esgMapping/R/NM_hyper_resample.RData&quot;)
rm(NM_hyper_resample)

#--------------NDVI covariate models-------------------------------------------------------------------------------
stime1 &lt;- system.time({
    NM_ndvi_lrns &lt;- multiLearner.preprocess(task = NM_ndvi_var)
})[3]
stime1

## Choose the resampling strategy
rdesc.10fcv = makeResampleDesc(method = &quot;CV&quot;, iters = 10, stratify = TRUE)
m = list(acc, kappa, mmce, timetrain)

# Apply resampling function
stime2 &lt;- system.time({
    NM_ndvi_resample &lt;- resample.multi.learner(NM_ndvi_lrns, NM_ndvi_var, 
        rdesc.10fcv, m)
})[3]
stime2

NM_ndvi.svm.pred = getRRPredictions(NM_ndvi_resample[[1]])
NM_ndvi.svm.cm &lt;- calculateConfusionMatrix(NM_ndvi.svm.pred, relative = TRUE)
NM_ndvi.rf.pred = getRRPredictions(NM_ndvi_resample[[2]])
NM_ndvi.rf.cm &lt;- calculateConfusionMatrix(NM_ndvi.rf.pred, relative = TRUE)
NM_ndvi.xgb.pred = getRRPredictions(NM_ndvi_resample[[3]])
NM_ndvi.xgb.cm &lt;- calculateConfusionMatrix(NM_ndvi.xgb.pred, relative = TRUE)
NM_ndvi.ens.pred = getRRPredictions(NM_ndvi_resample[[4]])
NM_ndvi.ens.cm &lt;- calculateConfusionMatrix(NM_ndvi.ens.pred, relative = TRUE)

save(NM_ndvi_resample, NM_ndvi_lrns, file = &quot;/data/data2/data/esgMapping/R/NM_ndvi_resample.RData&quot;)
rm(NM_ndvi_resample)

#--------------SG covariate models-------------------------------------------------------------------------------
stime1 &lt;- system.time({
    NM_sg_lrns &lt;- multiLearner.preprocess(task = NM_sg_var)
})[3]
stime1

## Choose the resampling strategy
rdesc.10fcv = makeResampleDesc(method = &quot;CV&quot;, iters = 10, stratify = TRUE)
m = list(acc, kappa, mmce, timetrain)

# Apply resampling function
stime2 &lt;- system.time({
    NM_sg_resample &lt;- resample.multi.learner(NM_sg_lrns, NM_sg_var, rdesc.10fcv, 
        m)
})[3]
stime2

NM_sg.svm.pred = getRRPredictions(NM_sg_resample[[1]])
NM_sg.svm.cm &lt;- calculateConfusionMatrix(NM_sg.svm.pred, relative = TRUE)
NM_sg.rf.pred = getRRPredictions(NM_sg_resample[[2]])
NM_sg.rf.cm &lt;- calculateConfusionMatrix(NM_sg.rf.pred, relative = TRUE)
NM_sg.xgb.pred = getRRPredictions(NM_sg_resample[[3]])
NM_sg.xgb.cm &lt;- calculateConfusionMatrix(NM_sg.xgb.pred, relative = TRUE)
NM_sg.ens.pred = getRRPredictions(NM_sg_resample[[4]])
NM_sg.ens.cm &lt;- calculateConfusionMatrix(NM_sg.ens.pred, relative = TRUE)

save(NM_sg_resample, NM_sg_lrns, file = &quot;/data/data2/data/esgMapping/R/NM_sg_resample.RData&quot;)
rm(NM_sg_resample)

#--------------Abiotic models-------------------------------------------------------------------------------
stime1 &lt;- system.time({
    NM_abiotic_lrns &lt;- multiLearner.preprocess(task = NM_abiotic_var)
})[3]
stime1

## Choose the resampling strategy
rdesc.10fcv = makeResampleDesc(method = &quot;CV&quot;, iters = 10, stratify = TRUE)
m = list(acc, kappa, mmce, timetrain)

# Apply resampling function
stime2 &lt;- system.time({
    NM_abiotic_resample &lt;- resample.multi.learner(NM_abiotic_lrns, NM_abiotic_var, 
        rdesc.10fcv, m)
})[3]
stime2

NM_abiotic.svm.pred = getRRPredictions(NM_abiotic_resample[[1]])
NM_abiotic.svm.cm &lt;- calculateConfusionMatrix(NM_abiotic.svm.pred, relative = TRUE)
NM_abiotic.rf.pred = getRRPredictions(NM_abiotic_resample[[2]])
NM_abiotic.rf.cm &lt;- calculateConfusionMatrix(NM_abiotic.rf.pred, relative = TRUE)
NM_abiotic.xgb.pred = getRRPredictions(NM_abiotic_resample[[3]])
NM_abiotic.xgb.cm &lt;- calculateConfusionMatrix(NM_abiotic.xgb.pred, relative = TRUE)
NM_abiotic.ens.pred = getRRPredictions(NM_abiotic_resample[[4]])
NM_abiotic.ens.cm &lt;- calculateConfusionMatrix(NM_abiotic.ens.pred, relative = TRUE)

save(NM_abiotic_resample, NM_abiotic_lrns, file = &quot;/data/data2/data/esgMapping/R/NM_abiotic_resample.RData&quot;)
rm(NM_abiotic_resample)

#--------------DSM covariate models-------------------------------------------------------------------------------
stime1 &lt;- system.time({
    NM_dsm_lrns &lt;- multiLearner.preprocess(task = NM_dsm_var)
})[3]
stime1

## Choose the resampling strategy
rdesc.10fcv = makeResampleDesc(method = &quot;CV&quot;, iters = 10, stratify = TRUE)
m = list(acc, kappa, mmce, timetrain)

# Apply resampling function
stime2 &lt;- system.time({
    NM_dsm_resample &lt;- resample.multi.learner(NM_dsm_lrns, NM_dsm_var, 
        rdesc.10fcv, m)
})[3]
stime2

stime2 &lt;- system.time({
    NM_dsm_resample_prob &lt;- resample.multi.learner(NM_dsm_lrns.prob, NM_dsm_var, 
        rdesc.10fcv, m)
})[3]
stime2

NM_dsm.svm.pred = getRRPredictions(NM_dsm_resample[[1]])
NM_dsm.svm.cm &lt;- calculateConfusionMatrix(NM_dsm.svm.pred, relative = TRUE)
NM_dsm.rf.pred = getRRPredictions(NM_dsm_resample[[2]])
NM_dsm.rf.cm &lt;- calculateConfusionMatrix(NM_dsm.rf.pred, relative = TRUE)
NM_dsm.xgb.pred = getRRPredictions(NM_dsm_resample[[3]])
NM_dsm.xgb.cm &lt;- calculateConfusionMatrix(NM_dsm.xgb.pred, relative = TRUE)
NM_dsm.ens.pred = getRRPredictions(NM_dsm_resample[[4]])
NM_dsm.ens.cm &lt;- calculateConfusionMatrix(NM_dsm.ens.pred, relative = TRUE)

save(NM_dsm_resample, NM_dsm_lrns, file = &quot;/data/data2/data/esgMapping/R/NM_dsm_resample.RData&quot;)
rm(NM_dsm_resample)

NM_dsm_svm_acc &lt;- classwise_accuracy_stats(NM_dsm_resample_prob[[1]])
NM_dsm_rf_acc &lt;- classwise_accuracy_stats(NM_dsm_resample_prob[[2]])
NM_dsm_xgb_acc &lt;- classwise_accuracy_stats(NM_dsm_resample_prob[[3]])
NM_dsm_ens_acc &lt;- classwise_accuracy_stats(NM_dsm_resample_prob[[4]])


NM_dsm_ens.kstat &lt;- kstat(NM_dsm.ens.pred$data$response, NM_dsm.ens.pred$data$truth)

#-----------------------------------------------------------------------------------------------
# Calculate multiclass Precision-Recall AUC with boostrapped 95% CI
NM_dsm_ens = NM_dsm_resample_prob[[4]]

NM_dsm_ens_n.l &lt;- plyr::count(NM_dsm_ens$pred$data$truth)
NM_dsm_ens_n.l &lt;- data.frame(matrix(NM_dsm_ens_n.l$freq, nrow = 1, dimnames = list(1, 
    paste(NM_dsm_ens_n.l$x))))
NM_dsm_ens_probs &lt;- NM_dsm_ens$pred$data[, 3:9]
NM_dsm_ens_names &lt;- colnames(NM_dsm_ens_n.l)
names(NM_dsm_ens_probs) = NM_dsm_ens_names
NM_dsm_ens_obs &lt;- data.frame(lapply(NM_dsm_ens_names, FUN = function(i) {
    ifelse(NM_dsm_ens$pred$data$truth == i, 1, 0)
}))
names(NM_dsm_ens_obs) = NM_dsm_ens_names
NM_dsm_ens_obs.pred &lt;- bind_cols(NM_dsm_ens_obs, NM_dsm_ens_probs)
colnames(NM_dsm_ens_obs.pred) &lt;- c(&quot;G1_true&quot;, &quot;G2_true&quot;, &quot;G3_true&quot;, &quot;G4_true&quot;, 
    &quot;G5_true&quot;, &quot;G6_true&quot;, &quot;G7_true&quot;, &quot;G1_pred_m1&quot;, &quot;G2_pred_m1&quot;, &quot;G3_pred_m1&quot;, 
    &quot;G4_pred_m1&quot;, &quot;G5_pred_m1&quot;, &quot;G6_pred_m1&quot;, &quot;G7_pred_m1&quot;)

# Generate ROC and PR curve data
NM_dsm_ens_roc_res &lt;- multi_roc(NM_dsm_ens_obs.pred, force_diag = T)
NM_dsm_ens_pr_res &lt;- multi_pr(NM_dsm_ens_obs.pred, force_diag = T)

# Plot PR curves
NM_dsm_ens_plot_pr_df &lt;- plot_pr_data(NM_dsm_ens_pr_res)
ggplot(NM_dsm_ens_plot_pr_df, aes(x = Recall, y = Precision)) + geom_path(aes(color = Group, 
    linetype = Method), size = 1.5) + theme_bw() + theme(plot.title = element_text(hjust = 0.5), 
    legend.justification = c(1, 0), legend.position = c(0.95, 0.05), legend.title = element_blank(), 
    legend.background = element_rect(fill = NULL, size = 0.5, linetype = &quot;solid&quot;, 
        colour = &quot;black&quot;))

# Calculate PR data with confidence intervals
NM_dsm_pr_auc_ci &lt;- pr_auc_with_ci(NM_dsm_ens_obs.pred, conf = 0.95, type = &quot;basic&quot;, 
    R = 100)

#-----------------------------------------------------------------------------------------------

NM_dsm.ens.pred.prob &lt;- NM_dsm.ens.pred$data[, 3:9] %&gt;% set_names(&quot;ES1&quot;, 
    &quot;ES2&quot;, &quot;ES3&quot;, &quot;ES4&quot;, &quot;ES5&quot;, &quot;ES6&quot;, &quot;ES7&quot;)

MMCE(NM_dsm.ens.pred$data$truth, NM_dsm.ens.pred$data$response)
ACC(NM_dsm.ens.pred$data$truth, NM_dsm.ens.pred$data$response)
BER(NM_dsm.ens.pred$data$truth, NM_dsm.ens.pred$data$response)
KAPPA(NM_dsm.ens.pred$data$truth, NM_dsm.ens.pred$data$response)
WKAPPA(NM_dsm.ens.pred$data$truth, NM_dsm.ens.pred$data$response)


NM_dsm.ens.tpr &lt;- TPR.multiclass(NM_dsm.ens.pred$data$truth, NM_dsm.ens.pred$data$response)
NM_dsm.ens.ppv &lt;- PPV.multiclass(NM_dsm.ens.pred$data$truth, NM_dsm.ens.pred$data$response)
NM_dsm.ens.aunu &lt;- multiclass.AUNU.class(NM_dsm.ens.pred.prob, NM_dsm.ens.pred$data$truth)
NM_dsm.ens.logloss &lt;- Logloss(NM_dsm.ens.pred.prob, NM_dsm.ens.pred$data$truth)
#----------------------------------------------------------------------------------------
#------------------------------------------------------------------------------------------
load(&quot;/data/data2/data/esgMapping/R/NM_all_resample.RData&quot;)
nm.all.resamp.metrics &lt;- cbind(rep(&quot;All&quot;, 4), c(&quot;SVM&quot;, &quot;RF&quot;, &quot;XGB&quot;, &quot;ENS&quot;), 
    data.frame(rbind(NM_all_resample[[1]]$aggr, NM_all_resample[[2]]$aggr, 
        NM_all_resample[[3]]$aggr, NM_all_resample[[4]]$aggr)))
colnames(nm.all.resamp.metrics)[1] &lt;- &quot;Data&quot;
colnames(nm.all.resamp.metrics)[2] &lt;- &quot;Model&quot;
rm(NM_all_resample)
load(&quot;/data/data2/data/esgMapping/R/NM_hyper_resample.RData&quot;)
nm.hyper.resamp.metrics &lt;- cbind(rep(&quot;Hyper&quot;, 4), c(&quot;SVM&quot;, &quot;RF&quot;, &quot;XGB&quot;, 
    &quot;ENS&quot;), data.frame(rbind(NM_hyper_resample[[1]]$aggr, NM_hyper_resample[[2]]$aggr, 
    NM_hyper_resample[[3]]$aggr, NM_hyper_resample[[4]]$aggr)))
colnames(nm.hyper.resamp.metrics)[1] &lt;- &quot;Data&quot;
colnames(nm.hyper.resamp.metrics)[2] &lt;- &quot;Model&quot;
rm(NM_hyper_resample)
load(&quot;/data/data2/data/esgMapping/R/NM_ndvi_resample.RData&quot;)
nm.ndvi.resamp.metrics &lt;- cbind(rep(&quot;NDVI&quot;, 4), c(&quot;SVM&quot;, &quot;RF&quot;, &quot;XGB&quot;, &quot;ENS&quot;), 
    data.frame(rbind(NM_ndvi_resample[[1]]$aggr, NM_ndvi_resample[[2]]$aggr, 
        NM_ndvi_resample[[3]]$aggr, NM_ndvi_resample[[4]]$aggr)))
colnames(nm.ndvi.resamp.metrics)[1] &lt;- &quot;Data&quot;
colnames(nm.ndvi.resamp.metrics)[2] &lt;- &quot;Model&quot;
rm(NM_ndvi_resample)
load(&quot;/data/data2/data/esgMapping/R/NM_abiotic_resample.RData&quot;)
nm.Abiotic.resamp.metrics &lt;- cbind(rep(&quot;Abiotic&quot;, 4), c(&quot;SVM&quot;, &quot;RF&quot;, &quot;XGB&quot;, 
    &quot;ENS&quot;), data.frame(rbind(NM_abiotic_resample[[1]]$aggr, NM_abiotic_resample[[2]]$aggr, 
    NM_abiotic_resample[[3]]$aggr, NM_abiotic_resample[[4]]$aggr)))
colnames(nm.Abiotic.resamp.metrics)[1] &lt;- &quot;Data&quot;
colnames(nm.Abiotic.resamp.metrics)[2] &lt;- &quot;Model&quot;
rm(NM_abiotic_resample)
load(&quot;/data/data2/data/esgMapping/R/NM_sg_resample.RData&quot;)
nm.sg.resamp.metrics &lt;- cbind(rep(&quot;SG&quot;, 4), c(&quot;SVM&quot;, &quot;RF&quot;, &quot;XGB&quot;, &quot;ENS&quot;), 
    data.frame(rbind(NM_sg_resample[[1]]$aggr, NM_sg_resample[[2]]$aggr, 
        NM_sg_resample[[3]]$aggr, NM_sg_resample[[4]]$aggr)))
colnames(nm.sg.resamp.metrics)[1] &lt;- &quot;Data&quot;
colnames(nm.sg.resamp.metrics)[2] &lt;- &quot;Model&quot;
rm(NM_sg_resample)
load(&quot;/data/data2/data/esgMapping/R/NM_dsm_resample.RData&quot;)
nm.dsm.resamp.metrics &lt;- cbind(rep(&quot;DSM&quot;, 4), c(&quot;SVM&quot;, &quot;RF&quot;, &quot;XGB&quot;, &quot;ENS&quot;), 
    data.frame(rbind(NM_dsm_resample[[1]]$aggr, NM_dsm_resample[[2]]$aggr, 
        NM_dsm_resample[[3]]$aggr, NM_dsm_resample[[4]]$aggr)))
colnames(nm.dsm.resamp.metrics)[1] &lt;- &quot;Data&quot;
colnames(nm.dsm.resamp.metrics)[2] &lt;- &quot;Model&quot;
rm(NM_dsm_resample)

# Combine resample results
nm.resamp.metrics &lt;- rbind(nm.all.resamp.metrics, nm.hyper.resamp.metrics, 
    nm.ndvi.resamp.metrics, nm.Abiotic.resamp.metrics, nm.sg.resamp.metrics, 
    nm.dsm.resamp.metrics)

theme_set(theme_classic())

# Plot
NM_model_comparison &lt;- ggplot(nm.resamp.metrics, aes(x = Data, y = acc.test.mean)) + 
    geom_point(aes(colour = factor(Model)), size = 4) + # geom_point(col=&#39;tomato2&#39;, size=3) + # Draw points
geom_segment(aes(x = Data, xend = Data, y = min(acc.test.mean), yend = max(acc.test.mean)), 
    linetype = &quot;dashed&quot;, size = 0.1) + # coord_flip() + # Draw dashed lines
labs(title = &quot;Dot Plot&quot;, subtitle = &quot;Data Type Vs ACC&quot;, caption = &quot;source: NM&quot;)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_model_comparison2.pdf&quot;)

ggplot(nm.resamp.metrics, aes(x = Data, y = acc.test.mean)) + geom_jitter(aes(colour = factor(Model)), 
    size = 4, width = 0.2, height = 0) + geom_segment(aes(x = Data, xend = Data, 
    y = 0.63, yend = 0.8), linetype = &quot;dashed&quot;, size = 0.1) + ylab(&quot;Accuracy&quot;) + 
    xlab(&quot;Dataset&quot;) + # coord_flip() + # Draw dashed lines
labs(title = &quot;MLRA 42 ESG Model Accuracy&quot;, color = &quot;Model&quot;)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_model_comparison5.pdf&quot;, 
    width = 4.3, height = 2.6)

# Calculate individual class accuracies
NM_classwise_model &lt;- bind_rows(NM_all.svm.cm$relative.row[, 8], NM_all.rf.cm$relative.row[, 
    8], NM_all.xgb.cm$relative.row[, 8], NM_all.ens.cm$relative.row[, 8], 
    NM_hyper.svm.cm$relative.row[, 8], NM_hyper.rf.cm$relative.row[, 8], 
    NM_hyper.xgb.cm$relative.row[, 8], NM_hyper.ens.cm$relative.row[, 8], 
    NM_ndvi.svm.cm$relative.row[, 8], NM_ndvi.rf.cm$relative.row[, 8], 
    NM_ndvi.xgb.cm$relative.row[, 8], NM_ndvi.ens.cm$relative.row[, 8], 
    NM_abiotic.svm.cm$relative.row[, 8], NM_abiotic.rf.cm$relative.row[, 
        8], NM_abiotic.xgb.cm$relative.row[, 8], NM_abiotic.ens.cm$relative.row[, 
        8], NM_sg.svm.cm$relative.row[, 8], NM_sg.rf.cm$relative.row[, 
        8], NM_sg.xgb.cm$relative.row[, 8], NM_sg.ens.cm$relative.row[, 
        8], NM_dsm.svm.cm$relative.row[, 8], NM_dsm.rf.cm$relative.row[, 
        8], NM_dsm.xgb.cm$relative.row[, 8], NM_dsm.ens.cm$relative.row[, 
        8])
NM_classwise_model - 1
NM.class.accuracy &lt;- data.frame(c(rep(&quot;All&quot;, 4), rep(&quot;Hyper&quot;, 4), rep(&quot;NDVI&quot;, 
    4), rep(&quot;Abiotic&quot;, 4), rep(&quot;SG&quot;, 4), rep(&quot;DSM&quot;, 4)), c(rep(c(&quot;SVM&quot;, 
    &quot;RF&quot;, &quot;XGB&quot;, &quot;ENS&quot;), 6))) %&gt;% set_names(&quot;Data&quot;, &quot;Model&quot;) %&gt;% bind_cols(., 
    (1 - NM_classwise_model))

NM.class.accuracy$Data &lt;- factor(NM.class.accuracy$Data, levels = c(&quot;All&quot;, 
    &quot;Hyper&quot;, &quot;NDVI&quot;, &quot;Abiotic&quot;, &quot;SG&quot;, &quot;DSM&quot;))

ESG.class.plot &lt;- function(data, class, title, filename, scale) {
    ggplot(data, aes(x = data[, 1], y = data[, class])) + geom_point(aes(colour = factor(Model)), 
        size = 4) + ylab(&quot;Accuracy&quot;) + xlab(&quot;Dataset&quot;) + # coord_flip() + # Draw dashed lines
    labs(title = title, color = &quot;Model&quot;)
    ggsave(paste0(&quot;/data/data2/data/esgMapping/analysis/figures/&quot;, filename, 
        &quot;_class_comparison.pdf&quot;), scale = scale)
}

ESG.class.plot(data = NM.class.accuracy, class = 3, title = &quot;MLRA 42 ESG Class 1&quot;, 
    filename = &quot;NM_ESG1&quot;, scale = 1)
ESG.class.plot(data = NM.class.accuracy, class = 3, title = &quot;MLRA 42 ESG Class 1&quot;, 
    filename = &quot;NM_ESG1&quot;, scale = 0.6)
ESG.class.plot(data = NM.class.accuracy, class = 4, title = &quot;MLRA 42 ESG Class 2&quot;, 
    filename = &quot;NM_ESG2&quot;, scale = 0.6)
ESG.class.plot(data = NM.class.accuracy, class = 5, title = &quot;MLRA 42 ESG Class 3&quot;, 
    filename = &quot;NM_ESG3&quot;, scale = 0.6)
ESG.class.plot(data = NM.class.accuracy, class = 6, title = &quot;MLRA 42 ESG Class 4&quot;, 
    filename = &quot;NM_ESG4&quot;, scale = 0.6)
ESG.class.plot(data = NM.class.accuracy, class = 7, title = &quot;MLRA 42 ESG Class 5&quot;, 
    filename = &quot;NM_ESG5&quot;, scale = 0.6)
ESG.class.plot(data = NM.class.accuracy, class = 8, title = &quot;MLRA 42 ESG Class 6&quot;, 
    filename = &quot;NM_ESG6&quot;, scale = 0.6)
ESG.class.plot(data = NM.class.accuracy, class = 9, title = &quot;MLRA 42 ESG Class 7&quot;, 
    filename = &quot;NM_ESG7&quot;, scale = 0.6)
#----------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------
NM_dsm_pred_ens &lt;- NM_dsm_resample_prob[[4]]$pred$data
nm.esname &lt;- nm.points@data[nm.points.covariates$Index, c(1, 8, 9, 16)] %&gt;% 
    set_names(c(&quot;id&quot;, &quot;es_id&quot;, &quot;esdname&quot;, &quot;ESG_ID&quot;)) %&gt;% mutate(id = seq(1, 
    length(id), 1))
nm.esname$id &lt;- as.integer(nm.esname$id)


NM_dsm_pred_ens.all &lt;- NM_dsm_pred_ens %&gt;% select(c(id, truth, response)) %&gt;% 
    left_join(., nm.esname, by = &quot;id&quot;)
NM_dsm_pred_ens.misclass &lt;- NM_dsm_pred_ens %&gt;% filter(truth != response) %&gt;% 
    select(c(id, truth, response)) %&gt;% left_join(., nm.esname, by = &quot;id&quot;)



#----------------------------------------------------------------------------------------
# ES3 ESG class
NM_dsm_pred_ens.all.r.es3 &lt;- NM_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES3&quot;) %&gt;% 
    group_by(response) %&gt;% summarise(no_rows = length(response))
View(NM_dsm_pred_ens.all.r.es3)

NM_dsm_pred_ens.misclass.r.es3 &lt;- NM_dsm_pred_ens.misclass %&gt;% filter(truth == 
    &quot;ES3&quot;) %&gt;% group_by(response) %&gt;% summarise(no_rows = length(response))
View(NM_dsm_pred_ens.misclass.r.es3)

#----------------------------------------------------------------------------------------  
# Evaluate ES3 misclassification ESD class
NM_dsm_pred_ens.all.es3 &lt;- NM_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES3&quot;) %&gt;% 
    group_by(esdname) %&gt;% summarise(no_rows = length(esdname))
View(NM_dsm_pred_ens.all.es3)

NM_dsm_pred_ens.misclass.es3 &lt;- NM_dsm_pred_ens.misclass %&gt;% filter(truth == 
    &quot;ES3&quot;) %&gt;% group_by(esdname) %&gt;% summarise(no_rows = length(esdname))
View(NM_dsm_pred_ens.misclass.es3)

# combine number of all ESD classes and misclasses
NM_dsm_pred_ens.mis.es3 &lt;- NM_dsm_pred_ens.all.es3 %&gt;% left_join(NM_dsm_pred_ens.misclass.es3, 
    by = &quot;esdname&quot;)
NM_dsm_pred_ens.mis.es3$misclassRate &lt;- NM_dsm_pred_ens.mis.es3$no_rows.y/NM_dsm_pred_ens.mis.es3$no_rows.x
NM_dsm_pred_ens.mis.es3$esdname[1]


NM_ES_misClass &lt;- data.frame(paste0(&quot;ES&quot;, seq(1:8))) %&gt;% set_names(&quot;ESD&quot;)
for (i in 1L:length(NM_dsm_pred_ens.mis.es3$esdname)) {
    esd_dist &lt;- NM_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES3&quot;) %&gt;% filter(str_detect(esdname, 
        gsub(&quot;([.|()\\^{}+$*?]|\\[|\\])&quot;, &quot;\\\\\\1&quot;, as.character(NM_dsm_pred_ens.mis.es3$esdname[i])))) %&gt;% 
        group_by(response) %&gt;% summarise(no_rows = length(response))
    esd_dist &lt;- esd_dist %&gt;% set_names(&quot;ESD&quot;, &quot;num&quot;)
    NM_ES_misClass &lt;- left_join(NM_ES_misClass, esd_dist, by = &quot;ESD&quot;)
}
NM_ES3_misClass &lt;- NM_ES_misClass %&gt;% set_names(c(&quot;ESD&quot;, as.character(NM_dsm_pred_ens.mis.es3$esdname)))
NM_ES3_misClass &lt;- NM_ES3_misClass %&gt;% gather(key = &quot;ESD&quot;)
NM_ES3_misClass &lt;- bind_cols(NM_ES3_misClass, data.frame(rep(paste0(&quot;ES&quot;, 
    c(1, 2, 0, 4, 5, 6, 7, 8)), length(unique(NM_dsm_pred_ens.mis.es3$esdname)))) %&gt;% 
    set_names(&quot;ESG&quot;))
NM_ES3_misClass &lt;- NM_ES3_misClass %&gt;% group_by(ESD) %&gt;% arrange(ESG, .by_group = TRUE)

NM_ES3.DF &lt;- NM_ES3_misClass %&gt;% group_by(ESD) %&gt;% mutate(ValuePer = (value/sum(value, 
    na.rm = TRUE))) %&gt;% arrange(ESG, .by_group = TRUE) %&gt;% ungroup()
# cbPalette &lt;- c( &#39;#ffa77f&#39;, &#39;#00a884&#39;, &#39;#beffe7&#39;, &#39;#ffffbf&#39;,
# &#39;#737301&#39;, &#39;#fe0000&#39;, &#39;#0071fe&#39;, &#39;#732500&#39;)
ggplot(NM_ES3.DF, aes(ESD, ValuePer, fill = ESG)) + geom_bar(stat = &quot;identity&quot;, 
    position = position_fill(reverse = TRUE)) + geom_text(aes(label = percent(ValuePer)), 
    position = position_stack(reverse = TRUE)) + scale_y_continuous(labels = percent_format())
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ES3_ESD_misclass.pdf&quot;)

#-------------------------------------------------------------------------------------------

#----------------------------------------------------------------------------------------
# ES7 ESG class
NM_dsm_pred_ens.all.r.es7 &lt;- NM_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES7&quot;) %&gt;% 
    group_by(response) %&gt;% summarise(no_rows = length(response))
# View(NM_dsm_pred_ens.all.r.es7)

NM_dsm_pred_ens.misclass.r.es7 &lt;- NM_dsm_pred_ens.misclass %&gt;% filter(truth == 
    &quot;ES7&quot;) %&gt;% group_by(response) %&gt;% summarise(no_rows = length(response))
# View(NM_dsm_pred_ens.misclass.r.es7)

#----------------------------------------------------------------------------------------  
# Evaluate ES7 misclassification ESD class
NM_dsm_pred_ens.all.es7 &lt;- NM_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES7&quot;) %&gt;% 
    group_by(esdname) %&gt;% summarise(no_rows = length(esdname))
# View(NM_dsm_pred_ens.all.es7)

NM_dsm_pred_ens.misclass.es7 &lt;- NM_dsm_pred_ens.misclass %&gt;% filter(truth == 
    &quot;ES7&quot;) %&gt;% group_by(esdname) %&gt;% summarise(no_rows = length(esdname))
# View(NM_dsm_pred_ens.misclass.es7)

# combine number of all ESD classes and misclasses
NM_dsm_pred_ens.mis.es7 &lt;- NM_dsm_pred_ens.all.es7 %&gt;% left_join(NM_dsm_pred_ens.misclass.es7, 
    by = &quot;esdname&quot;)
NM_dsm_pred_ens.mis.es7$misclassRate &lt;- NM_dsm_pred_ens.mis.es7$no_rows.y/NM_dsm_pred_ens.mis.es7$no_rows.x
NM_dsm_pred_ens.mis.es7$esdname[1]
View(NM_dsm_pred_ens.mis.es7)

NM_ES_misClass &lt;- data.frame(paste0(&quot;ES&quot;, seq(1:8))) %&gt;% set_names(&quot;ESD&quot;)
for (i in 1L:length(NM_dsm_pred_ens.mis.es7$esdname)) {
    esd_dist &lt;- NM_dsm_pred_ens.all %&gt;% filter(truth == &quot;ES7&quot;) %&gt;% filter(str_detect(esdname, 
        gsub(&quot;([.|()\\^{}+$*?]|\\[|\\])&quot;, &quot;\\\\\\1&quot;, as.character(NM_dsm_pred_ens.mis.es7$esdname[i])))) %&gt;% 
        group_by(response) %&gt;% summarise(no_rows = length(response))
    esd_dist &lt;- esd_dist %&gt;% set_names(&quot;ESD&quot;, &quot;num&quot;)
    NM_ES_misClass &lt;- left_join(NM_ES_misClass, esd_dist, by = &quot;ESD&quot;)
}
NM_ES7_misClass &lt;- NM_ES_misClass %&gt;% set_names(c(&quot;ESD&quot;, as.character(NM_dsm_pred_ens.mis.es7$esdname)))
NM_ES7_misClass &lt;- NM_ES7_misClass %&gt;% gather(key = &quot;ESD&quot;)
NM_ES7_misClass &lt;- bind_cols(NM_ES7_misClass, data.frame(rep(paste0(&quot;ES&quot;, 
    c(1, 2, 3, 4, 5, 6, 0, 8)), length(unique(NM_dsm_pred_ens.mis.es7$esdname)))) %&gt;% 
    set_names(&quot;ESG&quot;))
NM_ES7_misClass &lt;- NM_ES7_misClass %&gt;% group_by(ESD) %&gt;% arrange(ESG, .by_group = TRUE)

NM_ES7.DF &lt;- NM_ES7_misClass %&gt;% group_by(ESD) %&gt;% mutate(ValuePer = (value/sum(value, 
    na.rm = TRUE))) %&gt;% arrange(ESG, .by_group = TRUE) %&gt;% ungroup()
# cbPalette &lt;- c( &#39;#ffa77f&#39;, &#39;#00a884&#39;, &#39;#beffe7&#39;, &#39;#ffffbf&#39;,
# &#39;#737301&#39;, &#39;#fe0000&#39;, &#39;#0071fe&#39;, &#39;#732500&#39;)
ggplot(NM_ES7.DF, aes(ESD, ValuePer, fill = ESG)) + geom_bar(stat = &quot;identity&quot;, 
    position = position_fill(reverse = TRUE)) + geom_text(aes(label = percent(ValuePer)), 
    position = position_stack(reverse = TRUE)) + scale_y_continuous(labels = percent_format())
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ES7_ESD_misclass.pdf&quot;)

#-------------------------------------------------------------------------------------------</code></pre>
<pre class="r"><code># Run train models for different datasets

# CO models All
CO_all_train.svm &lt;- mlr::train(CO_all_lrns[[1]], CO_all_var)
CO_all_train.rf &lt;- mlr::train(CO_all_lrns[[2]], CO_all_var)
CO_all_train.xgb &lt;- mlr::train(CO_all_lrns[[3]], CO_all_var)
CO_all_train.ens &lt;- mlr::train(CO_all_lrns[[4]], CO_all_var)

CO_all_lrns.prob = lapply(CO_all_lrns, setPredictType, &quot;prob&quot;)
CO_all_train.svm.prob &lt;- mlr::train(CO_all_lrns.prob[[1]], CO_all_var)
CO_all_train.rf.prob &lt;- mlr::train(CO_all_lrns.prob[[2]], CO_all_var)
CO_all_train.xgb.prob &lt;- mlr::train(CO_all_lrns.prob[[3]], CO_all_var)

# Hyper
CO_hyper_train.svm &lt;- mlr::train(CO_hyper_lrns[[1]], CO_hyper_var)
CO_hyper_train.rf &lt;- mlr::train(CO_hyper_lrns[[2]], CO_hyper_var)
CO_hyper_train.xgb &lt;- mlr::train(CO_hyper_lrns[[3]], CO_hyper_var)
CO_hyper_train.ens &lt;- mlr::train(CO_hyper_lrns[[4]], CO_hyper_var)

CO_hyper_lrns.prob = lapply(CO_hyper_lrns, setPredictType, &quot;prob&quot;)
CO_hyper_train.svm.prob &lt;- mlr::train(CO_hyper_lrns.prob[[1]], CO_hyper_var)
CO_hyper_train.rf.prob &lt;- mlr::train(CO_hyper_lrns.prob[[2]], CO_hyper_var)
CO_hyper_train.xgb.prob &lt;- mlr::train(CO_hyper_lrns.prob[[3]], CO_hyper_var)

# NDVI
CO_ndvi_train.svm &lt;- mlr::train(CO_ndvi_lrns[[1]], CO_ndvi_var)
CO_ndvi_train.rf &lt;- mlr::train(CO_ndvi_lrns[[2]], CO_ndvi_var)
CO_ndvi_train.xgb &lt;- mlr::train(CO_ndvi_lrns[[3]], CO_ndvi_var)
CO_ndvi_train.ens &lt;- mlr::train(CO_ndvi_lrns[[4]], CO_ndvi_var)

CO_ndvi_lrns.prob = lapply(CO_ndvi_lrns, setPredictType, &quot;prob&quot;)
CO_ndvi_train.svm.prob &lt;- mlr::train(CO_ndvi_lrns.prob[[1]], CO_ndvi_var)
CO_ndvi_train.rf.prob &lt;- mlr::train(CO_ndvi_lrns.prob[[2]], CO_ndvi_var)
CO_ndvi_train.xgb.prob &lt;- mlr::train(CO_ndvi_lrns.prob[[3]], CO_sg_var)

# Abiotic
CO_abiotic_train.svm &lt;- mlr::train(CO_abiotic_lrns[[1]], CO_abiotic_var)
CO_abiotic_train.rf &lt;- mlr::train(CO_abiotic_lrns[[2]], CO_abiotic_var)
CO_abiotic_train.xgb &lt;- mlr::train(CO_abiotic_lrns[[3]], CO_abiotic_var)
CO_abiotic_train.ens &lt;- mlr::train(CO_abiotic_lrns[[4]], CO_abiotic_var)

CO_abiotic_lrns.prob = lapply(CO_abiotic_lrns, setPredictType, &quot;prob&quot;)
CO_abiotic_train.svm.prob &lt;- mlr::train(CO_abiotic_lrns.prob[[1]], CO_abiotic_var)
CO_abiotic_train.rf.prob &lt;- mlr::train(CO_abiotic_lrns.prob[[2]], CO_abiotic_var)
CO_abiotic_train.xgb.prob &lt;- mlr::train(CO_abiotic_lrns.prob[[3]], CO_abiotic_var)

# SG
CO_sg_train.svm &lt;- mlr::train(CO_sg_lrns[[1]], CO_sg_var)
CO_sg_train.rf &lt;- mlr::train(CO_sg_lrns[[2]], CO_sg_var)
CO_sg_train.xgb &lt;- mlr::train(CO_sg_lrns[[3]], CO_sg_var)
CO_sg_train.ens &lt;- mlr::train(CO_sg_lrns[[4]], CO_sg_var)

CO_sg_lrns.prob = lapply(CO_sg_lrns, setPredictType, &quot;prob&quot;)
CO_sg_train.svm.prob &lt;- mlr::train(CO_sg_lrns.prob[[1]], CO_sg_var)
CO_sg_train.rf.prob &lt;- mlr::train(CO_sg_lrns.prob[[2]], CO_sg_var)
CO_sg_train.xgb.prob &lt;- mlr::train(CO_sg_lrns.prob[[3]], CO_sg_var)

# DSM
CO_dsm_train.svm &lt;- mlr::train(CO_dsm_lrns[[1]], CO_dsm_var)
CO_dsm_train.rf &lt;- mlr::train(CO_dsm_lrns[[2]], CO_dsm_var)
CO_dsm_train.xgb &lt;- mlr::train(CO_dsm_lrns[[3]], CO_dsm_var)
CO_dsm_train.ens &lt;- mlr::train(CO_dsm_lrns[[4]], CO_dsm_var)

CO_dsm_lrns.prob = lapply(CO_dsm_lrns, setPredictType, &quot;prob&quot;)
CO_dsm_train.svm.prob &lt;- mlr::train(CO_dsm_lrns.prob[[1]], CO_dsm_var)
CO_dsm_train.rf.prob &lt;- mlr::train(CO_dsm_lrns.prob[[2]], CO_dsm_var)
CO_dsm_train.xgb.prob &lt;- mlr::train(CO_dsm_lrns.prob[[3]], CO_dsm_var)

# NM models
#---------------------------------------------------------------------------------------------
# All
NM_all_train.svm &lt;- mlr::train(NM_all_lrns[[1]], NM_all_var)
NM_all_train.rf &lt;- mlr::train(NM_all_lrns[[2]], NM_all_var)
NM_all_train.xgb &lt;- mlr::train(NM_all_lrns[[3]], NM_all_var)
NM_all_train.ens &lt;- mlr::train(NM_all_lrns[[4]], NM_all_var)

NM_all_lrns.prob = lapply(NM_all_lrns, setPredictType, &quot;prob&quot;)
NM_all_train.svm.prob &lt;- mlr::train(NM_all_lrns.prob[[1]], NM_all_var)
NM_all_train.rf.prob &lt;- mlr::train(NM_all_lrns.prob[[2]], NM_all_var)
NM_all_train.xgb.prob &lt;- mlr::train(NM_all_lrns.prob[[3]], NM_all_var)

# Hyper
NM_hyper_train.svm &lt;- mlr::train(NM_hyper_lrns[[1]], NM_hyper_var)
NM_hyper_train.rf &lt;- mlr::train(NM_hyper_lrns[[2]], NM_hyper_var)
NM_hyper_train.xgb &lt;- mlr::train(NM_hyper_lrns[[3]], NM_hyper_var)
NM_hyper_train.ens &lt;- mlr::train(NM_hyper_lrns[[4]], NM_hyper_var)

NM_hyper_lrns.prob = lapply(NM_hyper_lrns, setPredictType, &quot;prob&quot;)
NM_hyper_train.svm.prob &lt;- mlr::train(NM_hyper_lrns.prob[[1]], NM_hyper_var)
NM_hyper_train.rf.prob &lt;- mlr::train(NM_hyper_lrns.prob[[2]], NM_hyper_var)
NM_hyper_train.xgb.prob &lt;- mlr::train(NM_hyper_lrns.prob[[3]], NM_hyper_var)

# NDVI
NM_ndvi_train.svm &lt;- mlr::train(NM_ndvi_lrns[[1]], NM_ndvi_var)
NM_ndvi_train.rf &lt;- mlr::train(NM_ndvi_lrns[[2]], NM_ndvi_var)
NM_ndvi_train.xgb &lt;- mlr::train(NM_ndvi_lrns[[3]], NM_ndvi_var)
NM_ndvi_train.ens &lt;- mlr::train(NM_ndvi_lrns[[4]], NM_ndvi_var)

NM_ndvi_lrns.prob = lapply(NM_ndvi_lrns, setPredictType, &quot;prob&quot;)
NM_ndvi_train.svm.prob &lt;- mlr::train(NM_ndvi_lrns.prob[[1]], NM_ndvi_var)
NM_ndvi_train.rf.prob &lt;- mlr::train(NM_ndvi_lrns.prob[[2]], NM_ndvi_var)
NM_ndvi_train.xgb.prob &lt;- mlr::train(NM_ndvi_lrns.prob[[3]], NM_sg_var)

# Abiotic
NM_abiotic_train.svm &lt;- mlr::train(NM_abiotic_lrns[[1]], NM_abiotic_var)
NM_abiotic_train.rf &lt;- mlr::train(NM_abiotic_lrns[[2]], NM_abiotic_var)
NM_abiotic_train.xgb &lt;- mlr::train(NM_abiotic_lrns[[3]], NM_abiotic_var)
NM_abiotic_train.ens &lt;- mlr::train(NM_abiotic_lrns[[4]], NM_abiotic_var)

NM_abiotic_lrns.prob = lapply(NM_abiotic_lrns, setPredictType, &quot;prob&quot;)
NM_abiotic_train.svm.prob &lt;- mlr::train(NM_abiotic_lrns.prob[[1]], NM_abiotic_var)
NM_abiotic_train.rf.prob &lt;- mlr::train(NM_abiotic_lrns.prob[[2]], NM_abiotic_var)
NM_abiotic_train.xgb.prob &lt;- mlr::train(NM_abiotic_lrns.prob[[3]], NM_abiotic_var)

# SG
NM_sg_train.svm &lt;- mlr::train(NM_sg_lrns[[1]], NM_sg_var)
NM_sg_train.rf &lt;- mlr::train(NM_sg_lrns[[2]], NM_sg_var)
NM_sg_train.xgb &lt;- mlr::train(NM_sg_lrns[[3]], NM_sg_var)
NM_sg_train.ens &lt;- mlr::train(NM_sg_lrns[[4]], NM_sg_var)

NM_sg_lrns.prob = lapply(NM_sg_lrns, setPredictType, &quot;prob&quot;)
NM_sg_train.svm.prob &lt;- mlr::train(NM_sg_lrns.prob[[1]], NM_sg_var)
NM_sg_train.rf.prob &lt;- mlr::train(NM_sg_lrns.prob[[2]], NM_sg_var)
NM_sg_train.xgb.prob &lt;- mlr::train(NM_sg_lrns.prob[[3]], NM_sg_var)

# DSM
NM_dsm_train.svm &lt;- mlr::train(NM_dsm_lrns[[1]], NM_dsm_var)
NM_dsm_train.rf &lt;- mlr::train(NM_dsm_lrns[[2]], NM_dsm_var)
NM_dsm_train.xgb &lt;- mlr::train(NM_dsm_lrns[[3]], NM_dsm_var)
NM_dsm_train.ens &lt;- mlr::train(NM_dsm_lrns[[4]], NM_dsm_var)

NM_dsm_lrns.prob = lapply(NM_dsm_lrns, setPredictType, &quot;prob&quot;)
NM_dsm_train.svm.prob &lt;- mlr::train(NM_dsm_lrns.prob[[1]], NM_dsm_var)
NM_dsm_train.rf.prob &lt;- mlr::train(NM_dsm_lrns.prob[[2]], NM_dsm_var)
NM_dsm_train.xgb.prob &lt;- mlr::train(NM_dsm_lrns.prob[[3]], NM_dsm_var)</code></pre>
<p>##Classifier Calibration</p>
<pre class="r"><code># generate calibration data from resampling
CO_dsm_train.svm.cal = generateCalibrationData(CO_dsm_resample_prob)
plotCalibration(CO_dsm_train.svm.cal, smooth = TRUE)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_dsm_classifier calibration.png&quot;, 
    width = 7, height = 4, dpi = 200, units = &quot;in&quot;, device = &quot;png&quot;)


NM_dsm_train.svm.cal = generateCalibrationData(NM_dsm_resample_prob)
plotCalibration(NM_dsm_train.svm.cal, smooth = TRUE)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_dsm_classifier calibration.png&quot;, 
    width = 7, height = 4, dpi = 200, units = &quot;in&quot;, device = &quot;png&quot;)


CO_dsm_train.ens.cal = generateCalibrationData(CO_dsm_resample_prob[[4]])
plotCalibration(CO_dsm_train.ens.cal, smooth = TRUE)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_dsm_classifier calibration_ens.png&quot;, 
    width = 6, height = 4, dpi = 200, units = &quot;in&quot;, device = &quot;png&quot;)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_dsm_classifier calibration_ens.pdf&quot;, 
    width = 6, height = 4, dpi = 200, units = &quot;in&quot;)

NM_dsm_train.ens.cal = generateCalibrationData(NM_dsm_resample_prob[[4]])
plotCalibration(NM_dsm_train.ens.cal, smooth = TRUE)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_dsm_classifier calibration_ens.png&quot;, 
    width = 6, height = 4, dpi = 200, units = &quot;in&quot;, device = &quot;png&quot;)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_dsm_classifier calibration_ens.pdf&quot;, 
    width = 6, height = 4, dpi = 200, units = &quot;in&quot;)</code></pre>
<p>##Feature Importance</p>
<pre class="r"><code>#---------------------------------------------------------------------------------------  
# Model Agnostic method for feature importance



# define loss function
acc &lt;- function(actual, predicted) {
    cm = as.matrix(table(Actual = actual, Predicted = predicted))
    n = sum(cm)
    diag = diag(cm)
    accuracy = sum(diag)/n
    return(accuracy)
}

# uses the metric &#39;ce&#39;=classification error which is the opposite of
# accuracy all
CO.all.svm = Predictor$new(CO_all_train.svm.prob, data = co.esg.all.cov.rm[-1], 
    y = co.esg.all.cov.rm[1])
CO.all.svm.imp = FeatureImp$new(CO.all.svm, loss = &quot;ce&quot;)
CO.all.rf = Predictor$new(CO_all_train.rf.prob, data = co.esg.all.cov.rm[-1], 
    y = co.esg.all.cov.rm[1])
CO.all.rf.imp = FeatureImp$new(CO.all.rf, loss = &quot;ce&quot;)
CO.all.xgb = Predictor$new(CO_all_train.xgb.prob, data = co.esg.all.cov.rm[-1], 
    y = co.esg.all.cov.rm[1])
CO.all.xgb.imp = FeatureImp$new(CO.all.xgb, loss = &quot;ce&quot;)
CO.all.ens = Predictor$new(CO_all_train.ens, data = co.esg.all.cov.rm[-1], 
    y = co.esg.all.cov.rm[1])
CO.all.ens.imp = FeatureImp$new(CO.all.ens, loss = &quot;ce&quot;)

NM.all.svm = Predictor$new(NM_all_train.svm.prob, data = nm.esg.all.cov.rm[-1], 
    y = nm.esg.all.cov.rm[1])
NM.all.svm.imp = FeatureImp$new(NM.all.svm, loss = &quot;ce&quot;)
NM.all.rf = Predictor$new(NM_all_train.rf.prob, data = nm.esg.all.cov.rm[-1], 
    y = nm.esg.all.cov.rm[1])
NM.all.rf.imp = FeatureImp$new(NM.all.rf, loss = &quot;ce&quot;)
NM.all.xgb = Predictor$new(NM_all_train.xgb.prob, data = nm.esg.all.cov.rm[-1], 
    y = nm.esg.all.cov.rm[1])
NM.all.xgb.imp = FeatureImp$new(NM.all.xgb, loss = &quot;ce&quot;)
NM.all.ens = Predictor$new(NM_all_train.ens, data = NM.esg.all.cov.rm[-1], 
    y = nm.esg.all.cov.rm[1])
NM.all.ens.imp = FeatureImp$new(NM.all.ens, loss = &quot;ce&quot;)

# hyper
CO.hyper.svm = Predictor$new(CO_hyper_train.svm.prob, data = co.esg.hyper.cov.rm[-1], 
    y = co.esg.hyper.cov.rm[1])
CO.hyper.svm.imp = FeatureImp$new(CO.hyper.svm, loss = &quot;ce&quot;)
CO.hyper.rf = Predictor$new(CO_hyper_train.rf.prob, data = co.esg.hyper.cov.rm[-1], 
    y = co.esg.hyper.cov.rm[1])
CO.hyper.rf.imp = FeatureImp$new(CO.hyper.rf, loss = &quot;ce&quot;)
CO.hyper.xgb = Predictor$new(CO_hyper_train.xgb.prob, data = co.esg.hyper.cov.rm[-1], 
    y = co.esg.hyper.cov.rm[1])
CO.hyper.xgb.imp = FeatureImp$new(CO.hyper.xgb, loss = &quot;ce&quot;)
CO.hyper.ens = Predictor$new(CO_hyper_train.ens, data = co.esg.hyper.cov.rm[-1], 
    y = co.esg.hyper.cov.rm[1])
CO.hyper.ens.imp = FeatureImp$new(CO.hyper.ens, loss = &quot;ce&quot;)

NM.hyper.svm = Predictor$new(NM_hyper_train.svm.prob, data = nm.esg.hyper.cov.rm[-1], 
    y = nm.esg.hyper.cov.rm[1])
NM.hyper.svm.imp = FeatureImp$new(NM.hyper.svm, loss = &quot;ce&quot;)
NM.hyper.rf = Predictor$new(NM_hyper_train.rf.prob, data = nm.esg.hyper.cov.rm[-1], 
    y = nm.esg.hyper.cov.rm[1])
NM.hyper.rf.imp = FeatureImp$new(NM.hyper.rf, loss = &quot;ce&quot;)
NM.hyper.xgb = Predictor$new(NM_hyper_train.xgb.prob, data = nm.esg.hyper.cov.rm[-1], 
    y = nm.esg.hyper.cov.rm[1])
NM.hyper.xgb.imp = FeatureImp$new(NM.hyper.xgb, loss = &quot;ce&quot;)
NM.hyper.ens = Predictor$new(NM_hyper_train.ens, data = NM.esg.hyper.cov.rm[-1], 
    y = nm.esg.hyper.cov.rm[1])
NM.hyper.ens.imp = FeatureImp$new(NM.hyper.ens, loss = &quot;ce&quot;)

# ndvi
CO.ndvi.svm = Predictor$new(CO_ndvi_train.svm.prob, data = co.esg.ndvi.cov.rm[-1], 
    y = co.esg.ndvi.cov.rm[1])
CO.ndvi.svm.imp = FeatureImp$new(CO.ndvi.svm, loss = &quot;ce&quot;)
CO.ndvi.rf = Predictor$new(CO_ndvi_train.rf.prob, data = co.esg.ndvi.cov.rm[-1], 
    y = co.esg.ndvi.cov.rm[1])
CO.ndvi.rf.imp = FeatureImp$new(CO.ndvi.rf, loss = &quot;ce&quot;)
CO.ndvi.xgb = Predictor$new(CO_ndvi_train.xgb.prob, data = co.esg.ndvi.cov.rm[-1], 
    y = co.esg.ndvi.cov.rm[1])
CO.ndvi.xgb.imp = FeatureImp$new(CO.ndvi.xgb, loss = &quot;ce&quot;)
CO.ndvi.ens = Predictor$new(CO_ndvi_train.ens, data = co.esg.ndvi.cov.rm[-1], 
    y = co.esg.ndvi.cov.rm[1])
CO.ndvi.ens.imp = FeatureImp$new(CO.ndvi.ens, loss = &quot;ce&quot;)

NM.ndvi.svm = Predictor$new(NM_ndvi_train.svm.prob, data = nm.esg.ndvi.cov.rm[-1], 
    y = nm.esg.ndvi.cov.rm[1])
NM.ndvi.svm.imp = FeatureImp$new(NM.ndvi.svm, loss = &quot;ce&quot;)
NM.ndvi.rf = Predictor$new(NM_ndvi_train.rf.prob, data = nm.esg.ndvi.cov.rm[-1], 
    y = nm.esg.ndvi.cov.rm[1])
NM.ndvi.rf.imp = FeatureImp$new(NM.ndvi.rf, loss = &quot;ce&quot;)
NM.ndvi.xgb = Predictor$new(NM_ndvi_train.xgb.prob, data = nm.esg.ndvi.cov.rm[-1], 
    y = nm.esg.ndvi.cov.rm[1])
NM.ndvi.xgb.imp = FeatureImp$new(NM.ndvi.xgb, loss = &quot;ce&quot;)
NM.ndvi.ens = Predictor$new(NM_ndvi_train.ens, data = NM.esg.ndvi.cov.rm[-1], 
    y = nm.esg.ndvi.cov.rm[1])
NM.ndvi.ens.imp = FeatureImp$new(NM.ndvi.ens, loss = &quot;ce&quot;)

# abiotic
CO.abiotic.svm = Predictor$new(CO_abiotic_train.svm.prob, data = co.esg.abiotic.cov.rm[-1], 
    y = co.esg.abiotic.cov.rm[1])
CO.abiotic.svm.imp = FeatureImp$new(CO.abiotic.svm, loss = &quot;ce&quot;)
CO.abiotic.rf = Predictor$new(CO_abiotic_train.rf.prob, data = co.esg.abiotic.cov.rm[-1], 
    y = co.esg.abiotic.cov.rm[1])
CO.abiotic.rf.imp = FeatureImp$new(CO.abiotic.rf, loss = &quot;ce&quot;)
CO.abiotic.xgb = Predictor$new(CO_abiotic_train.xgb.prob, data = co.esg.abiotic.cov.rm[-1], 
    y = co.esg.abiotic.cov.rm[1])
CO.abiotic.xgb.imp = FeatureImp$new(CO.abiotic.xgb, loss = &quot;ce&quot;)
CO.abiotic.ens = Predictor$new(CO_abiotic_train.ens, data = co.esg.abiotic.cov.rm[-1], 
    y = co.esg.abiotic.cov.rm[1])
CO.abiotic.ens.imp = FeatureImp$new(CO.abiotic.ens, loss = &quot;ce&quot;)

NM.abiotic.svm = Predictor$new(NM_abiotic_train.svm.prob, data = nm.esg.abiotic.cov.rm[-1], 
    y = nm.esg.abiotic.cov.rm[1])
NM.abiotic.svm.imp = FeatureImp$new(NM.abiotic.svm, loss = &quot;ce&quot;)
NM.abiotic.rf = Predictor$new(NM_abiotic_train.rf.prob, data = nm.esg.abiotic.cov.rm[-1], 
    y = nm.esg.abiotic.cov.rm[1])
NM.abiotic.rf.imp = FeatureImp$new(NM.abiotic.rf, loss = &quot;ce&quot;)
NM.abiotic.xgb = Predictor$new(NM_abiotic_train.xgb.prob, data = nm.esg.abiotic.cov.rm[-1], 
    y = nm.esg.abiotic.cov.rm[1])
NM.abiotic.xgb.imp = FeatureImp$new(NM.abiotic.xgb, loss = &quot;ce&quot;)
NM.abiotic.ens = Predictor$new(NM_abiotic_train.ens, data = NM.esg.abiotic.cov.rm[-1], 
    y = nm.esg.abiotic.cov.rm[1])
NM.abiotic.ens.imp = FeatureImp$new(NM.abiotic.ens, loss = &quot;ce&quot;)

# sg
CO.sg.svm = Predictor$new(CO_sg_train.svm.prob, data = co.esg.sg.cov.rm[-1], 
    y = co.esg.sg.cov.rm[1])
CO.sg.svm.imp = FeatureImp$new(CO.sg.svm, loss = &quot;ce&quot;)
CO.sg.rf = Predictor$new(CO_sg_train.rf.prob, data = co.esg.sg.cov.rm[-1], 
    y = co.esg.sg.cov.rm[1])
CO.sg.rf.imp = FeatureImp$new(CO.sg.rf, loss = &quot;ce&quot;)
CO.sg.xgb = Predictor$new(CO_sg_train.xgb.prob, data = co.esg.sg.cov.rm[-1], 
    y = co.esg.sg.cov.rm[1])
CO.sg.xgb.imp = FeatureImp$new(CO.sg.xgb, loss = &quot;ce&quot;)
CO.sg.ens = Predictor$new(CO_sg_train.ens, data = co.esg.sg.cov.rm[-1], 
    y = co.esg.sg.cov.rm[1])
CO.sg.ens.imp = FeatureImp$new(CO.sg.ens, loss = &quot;ce&quot;)

NM.sg.svm = Predictor$new(NM_sg_train.svm.prob, data = nm.esg.sg.cov.rm[-1], 
    y = nm.esg.sg.cov.rm[1])
NM.sg.svm.imp = FeatureImp$new(NM.sg.svm, loss = &quot;ce&quot;)
NM.sg.rf = Predictor$new(NM_sg_train.rf.prob, data = nm.esg.sg.cov.rm[-1], 
    y = nm.esg.sg.cov.rm[1])
NM.sg.rf.imp = FeatureImp$new(NM.sg.rf, loss = &quot;ce&quot;)
NM.sg.xgb = Predictor$new(NM_sg_train.xgb.prob, data = nm.esg.sg.cov.rm[-1], 
    y = nm.esg.sg.cov.rm[1])
NM.sg.xgb.imp = FeatureImp$new(NM.sg.xgb, loss = &quot;ce&quot;)
NM.sg.ens = Predictor$new(NM_sg_train.ens, data = NM.esg.sg.cov.rm[-1], 
    y = nm.esg.sg.cov.rm[1])
NM.sg.ens.imp = FeatureImp$new(NM.sg.ens, loss = &quot;ce&quot;)

# dsm
CO.dsm.svm = Predictor$new(CO_dsm_train.svm.prob, data = co.esg.dsm.cov.rm[-1], 
    y = co.esg.dsm.cov.rm[1])
CO.dsm.svm.imp = FeatureImp$new(CO.dsm.svm, loss = &quot;ce&quot;)
CO.dsm.rf = Predictor$new(CO_dsm_train.rf.prob, data = co.esg.dsm.cov.rm[-1], 
    y = co.esg.dsm.cov.rm[1])
CO.dsm.rf.imp = FeatureImp$new(CO.dsm.rf, loss = &quot;ce&quot;)
CO.dsm.xgb = Predictor$new(CO_dsm_train.xgb.prob, data = co.esg.dsm.cov.rm[-1], 
    y = co.esg.dsm.cov.rm[1])
CO.dsm.xgb.imp = FeatureImp$new(CO.dsm.xgb, loss = &quot;ce&quot;)
CO.dsm.ens = Predictor$new(CO_dsm_train.ens, data = co.esg.dsm.cov.rm[-1], 
    y = co.esg.dsm.cov.rm[1])
CO.dsm.ens.imp = FeatureImp$new(CO.dsm.ens, loss = &quot;ce&quot;)

# need to convert categorical to factor
nm.esg.dsm.cov.rm$TAXOUSDA_250m &lt;- as.numeric(nm.esg.dsm.cov.rm$TAXOUSDA_250m)
NM.dsm.svm = Predictor$new(NM_dsm_train.svm.prob, data = nm.esg.dsm.cov.rm[-1], 
    y = nm.esg.dsm.cov.rm[1])
NM.dsm.svm.imp = FeatureImp$new(NM.dsm.svm, loss = &quot;ce&quot;)
NM.dsm.rf = Predictor$new(NM_dsm_train.rf.prob, data = nm.esg.dsm.cov.rm[-1], 
    y = nm.esg.dsm.cov.rm[1])
NM.dsm.rf.imp = FeatureImp$new(NM.dsm.rf, loss = &quot;ce&quot;)
NM.dsm.xgb = Predictor$new(NM_dsm_train.xgb.prob, data = nm.esg.dsm.cov.rm[-1], 
    y = nm.esg.dsm.cov.rm[1])
NM.dsm.xgb.imp = FeatureImp$new(NM.dsm.xgb, loss = &quot;ce&quot;)
NM.dsm.ens = Predictor$new(NM_dsm_train.ens, data = nm.esg.dsm.cov.rm[-1], 
    y = nm.esg.dsm.cov.rm[1])
NM.dsm.ens.imp = FeatureImp$new(NM.dsm.ens, loss = &quot;ce&quot;)

CO.dsm.ens = Predictor$new(CO_dsm_train.ens, data = co.esg.dsm.cov.rm[-1], 
    y = co.esg.dsm.cov.rm[1])
CO.dsm.ens.imp = FeatureImp$new(CO.dsm.ens, loss = &quot;ce&quot;)

mlr_VI_ggplot &lt;- function(imp, metric_title, sub) {
    mlr_vi &lt;- imp$results
    var_importance &lt;- data.frame(variable = mlr_vi$feature, importance = as.vector(mlr_vi$importance))
    var_importance &lt;- arrange(var_importance, desc(importance))
    var_importance$variable &lt;- factor(var_importance$variable, levels = var_importance$variable)
    if (is.null(sub)) {
        var_importance_sub &lt;- var_importance
    } else {
        var_importance_sub &lt;- var_importance[1:sub, ]
    }
    
    var_importance_sub$variable = with(var_importance_sub, factor(variable, 
        levels = rev(levels(variable))))
    # Plot
    theme_set(theme_bw())
    p &lt;- ggplot(var_importance_sub, aes(x = variable, y = importance)) + 
        geom_point(size = 3) + geom_segment(aes(x = variable, xend = variable, 
        y = 1, yend = importance)) + coord_flip() + labs(title = paste0(&quot;Variable Importance&quot;), 
        subtitle = paste0(&quot;(&quot;, metric_title, &quot;)&quot;)) + xlab(&quot;Covariates&quot;) + 
        ylab(paste0(&quot;Importance&quot;)) + theme(axis.text.x = element_text(angle = 0, 
        vjust = 0.6)) + theme(axis.text.y = element_text(angle = 0, vjust = 0.6))
    return(p)
}

multiplot &lt;- function(..., plotlist = NULL, file, cols = 1, layout = NULL) {
    library(grid)
    
    # Make a list from the ... arguments and plotlist
    plots &lt;- c(list(...), plotlist)
    
    numPlots = length(plots)
    
    # If layout is NULL, then use &#39;cols&#39; to determine layout
    if (is.null(layout)) {
        # Make the panel ncol: Number of columns of plots nrow: Number of rows
        # needed, calculated from # of cols
        layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, 
            nrow = ceiling(numPlots/cols))
    }
    
    if (numPlots == 1) {
        print(plots[[1]])
        
    } else {
        # Set up the page
        grid.newpage()
        pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
        
        # Make each plot, in the correct location
        for (i in 1:numPlots) {
            # Get the i,j matrix positions of the regions that contain this subplot
            matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE))
            
            print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, 
                layout.pos.col = matchidx$col))
        }
    }
}

CO.dsm.svm.imp.plot &lt;- mlr_VI_ggplot(CO.dsm.svm.imp, &quot;Support Vector Machines&quot;, 
    10)
CO.dsm.rf.imp.plot &lt;- mlr_VI_ggplot(CO.dsm.rf.imp, &quot;Random Forest&quot;, 10)
CO.dsm.xgb.imp.plot &lt;- mlr_VI_ggplot(CO.dsm.xgb.imp, &quot;Extreme Gradient Boosting&quot;, 
    10)

# multiplot(CO.dsm.svm.imp.plot, CO.dsm.rf.imp.plot,
# CO.dsm.xgb.imp.plot, cols=3)
CO_dsm_variableImp &lt;- grid.arrange(CO.dsm.svm.imp.plot, CO.dsm.rf.imp.plot, 
    CO.dsm.xgb.imp.plot, ncol = 3)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_dsm_svm_variableImp.pdf&quot;, 
    CO.dsm.svm.imp.plot, scale = 0.6)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_dsm_svm_variableImp.pdf&quot;, 
    CO.dsm.svm.imp.plot, scale = 0.6)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_dsm_rf_variableImp.pdf&quot;, 
    CO.dsm.rf.imp.plot, scale = 0.6)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_dsm_xgb_variableImp.pdf&quot;, 
    CO.dsm.xgb.imp.plot, scale = 0.6)

NM.dsm.svm.imp.plot &lt;- mlr_VI_ggplot(NM.dsm.svm.imp, &quot;Support Vector Machines&quot;, 
    10)
NM.dsm.rf.imp.plot &lt;- mlr_VI_ggplot(NM.dsm.rf.imp, &quot;Random Forest&quot;, 10)
NM.dsm.xgb.imp.plot &lt;- mlr_VI_ggplot(NM.dsm.xgb.imp, &quot;Extreme Gradient Boosting&quot;, 
    10)

# multiplot(NM.dsm.svm.imp.plot, NM.dsm.rf.imp.plot,
# NM.dsm.xgb.imp.plot, cols=3)
NM_dsm_variableImp &lt;- grid.arrange(NM.dsm.svm.imp.plot, NM.dsm.rf.imp.plot, 
    NM.dsm.xgb.imp.plot, ncol = 3)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_dsm_svm_variableImp.pdf&quot;, 
    NM.dsm.svm.imp.plot, scale = 0.6)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_dsm_svm_variableImp.pdf&quot;, 
    NM.dsm.svm.imp.plot, scale = 0.6)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_dsm_rf_variableImp.pdf&quot;, 
    NM.dsm.rf.imp.plot, scale = 0.6)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_dsm_xgb_variableImp.pdf&quot;, 
    NM.dsm.xgb.imp.plot, scale = 0.6)

# #Another model agnostic approach using the vip package--takes too long to calculate  
#   library(vip)
#   mod &lt;- getLearnerModel(NM_dsm_train.svm.prob)
#   svm.hyper &lt;- getHyperPars(NM_dsm_lrns[[1]])
#   svm.model &lt;- svm(esite ~ ., data = nm.esg.dsm.cov.rm, cost = svm.hyper$cost, gamma = svm.hyper$gamma, scale=svm.hyper$ppc.scale, probability=TRUE)
#   mod.vi &lt;- vip(svm.model, method = &quot;pdp&quot;, feature_names=mod$features)</code></pre>
<p>##Partial dependence plots and individual conditional expectation curves</p>
<pre class="r"><code># Create list of observations that were correctly classified
NM_dsm.rf.obsError &lt;- data.frame(as.factor(NM_dsm.rf.pred$data$id)) %&gt;% 
    set_names(&quot;.id&quot;)
for (i in 1:length(NM_dsm.rf.pred$data$response)) {
    if (NM_dsm.rf.pred$data$response[i] == NM_dsm.rf.pred$data$truth[i]) {
        NM_dsm.rf.obsError$predict[i] &lt;- &quot;Truth&quot;
    } else {
        NM_dsm.rf.obsError$predict[i] &lt;- &quot;False&quot;
    }
}

CO_dsm.rf.obsError &lt;- data.frame(as.factor(CO_dsm.rf.pred$data$id)) %&gt;% 
    set_names(&quot;.id&quot;)
for (i in 1:length(CO_dsm.rf.pred$data$response)) {
    if (CO_dsm.rf.pred$data$response[i] == CO_dsm.rf.pred$data$truth[i]) {
        CO_dsm.rf.obsError$predict[i] &lt;- &quot;Truth&quot;
    } else {
        CO_dsm.rf.obsError$predict[i] &lt;- &quot;False&quot;
    }
}

#-----------------Create PDP and ICE plots 

#CO RF NDVI
  # CO.dsm.rf.c1 = Predictor$new(CO_dsm_train.rf.prob, data=co.esg.dsm.cov.rm[-1], y=co.esg.dsm.cov.rm[1], class=1)
  # CO.dsm.rf.ndvi.pd.c1 = Partial$new(CO.dsm.rf.c1, feature = &quot;NDVI_Dec_Mean&quot;, ice=TRUE)
  # plot(CO.dsm.rf.ndvi.pd.c1)

CO.dsm.rf.ndvi.pd = Partial$new(CO.dsm.rf, feature = &quot;NDVI_Dec_Mean&quot;, ice = TRUE)  #, center.at=min(co.esg.dsm.cov.rm$NDVI_Dec_Mean))
plot(CO.dsm.rf.ndvi.pd)
ggplot_build(CO.dsm.rf.ndvi.pd)
CO.dsm.rf.ndvi.results &lt;- CO.dsm.rf.ndvi.pd$results
CO.dsm.rf.ndvi.results$.id &lt;- as.factor(CO.dsm.rf.ndvi.results$.id)
site_es &lt;- data.frame(as.factor(seq(1, length(co.esg.dsm.cov.rm$esite), 
    1)), co.esg.dsm.cov.rm$esite) %&gt;% set_names(c(&quot;.id&quot;, &quot;esite&quot;))
CO.dsm.rf.ndvi.results &lt;- left_join(CO.dsm.rf.ndvi.results, site_es, by = &quot;.id&quot;)
CO.dsm.rf.ndvi.results &lt;- left_join(CO.dsm.rf.ndvi.results, CO_dsm.rf.obsError, 
    by = &quot;.id&quot;)

CO.dsm.rf.ndvi.pd.es1 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES1&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = NDVI_Dec_Mean, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es2 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES2&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = NDVI_Dec_Mean, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es3 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES3&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = NDVI_Dec_Mean, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es4 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES4&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = NDVI_Dec_Mean, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es5 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES5&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = NDVI_Dec_Mean, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es6 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES6&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = NDVI_Dec_Mean, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es7 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES7&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = NDVI_Dec_Mean, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es99 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES99&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = NDVI_Dec_Mean, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)

CO_dsm_rf_ice &lt;- grid.arrange(CO.dsm.rf.ndvi.pd.es1, CO.dsm.rf.ndvi.pd.es2, 
    CO.dsm.rf.ndvi.pd.es3, CO.dsm.rf.ndvi.pd.es4, CO.dsm.rf.ndvi.pd.es5, 
    CO.dsm.rf.ndvi.pd.es6, CO.dsm.rf.ndvi.pd.es7, CO.dsm.rf.ndvi.pd.es99, 
    ncol = 3)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_dsm_rf_ice.pdf&quot;, 
    CO_dsm_rf_ice, width = 10, height = 10, dpi = 300, units = &quot;in&quot;)

# Subset so that it only plots observations belonging to each ESG and
# then displays correct and missclassified observations
CO.dsm.rf.ndvi.pd.es1 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES1&quot; &amp; esite == &quot;ES1&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = NDVI_Dec_Mean, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es2 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES2&quot; &amp; esite == &quot;ES2&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = NDVI_Dec_Mean, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es3 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES3&quot; &amp; esite == &quot;ES3&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = NDVI_Dec_Mean, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es4 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES4&quot; &amp; esite == &quot;ES4&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = NDVI_Dec_Mean, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es5 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES5&quot; &amp; esite == &quot;ES5&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = NDVI_Dec_Mean, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es6 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES6&quot; &amp; esite == &quot;ES6&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = NDVI_Dec_Mean, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es7 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES7&quot; &amp; esite == &quot;ES7&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = NDVI_Dec_Mean, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
CO.dsm.rf.ndvi.pd.es99 &lt;- CO.dsm.rf.ndvi.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES99&quot; &amp; esite == &quot;ES99&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = NDVI_Dec_Mean, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)

CO_dsm_rf_ice.sub &lt;- grid.arrange(CO.dsm.rf.ndvi.pd.es1, CO.dsm.rf.ndvi.pd.es2, 
    CO.dsm.rf.ndvi.pd.es3, CO.dsm.rf.ndvi.pd.es4, CO.dsm.rf.ndvi.pd.es5, 
    CO.dsm.rf.ndvi.pd.es6, CO.dsm.rf.ndvi.pd.es7, CO.dsm.rf.ndvi.pd.es99, 
    ncol = 3)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_dsm_rf_ice.sub.pdf&quot;, 
    CO_dsm_rf_ice.sub, width = 10, height = 10, dpi = 300, units = &quot;in&quot;)

#--------------------------------------------------------------------------------------------
#NM RF TAXOUSDA
  # NM.dsm.rf.c1 = Predictor$new(NM_dsm_train.rf.prob, data=NM.esg.dsm.NMv.rm[-1], y=NM.esg.dsm.NMv.rm[1], class=1)
  # NM.dsm.rf.tax.pd.c1 = Partial$new(NM.dsm.rf.c1, feature = &quot;NDVI_Dec_Mean&quot;, ice=TRUE)
  # plot(NM.dsm.rf.tax.pd.c1)
taxousda_legend &lt;- read.csv(&quot;/data/data2/data/esgMapping/analysis/data/raw_data/TAXOUSDA_250m_Legend.csv&quot;)
nm.taxo &lt;- as.numeric(unique(nm.esg.dsm.cov.rm$TAXOUSDA_250m))
nm.tax.key &lt;- taxousda_legend %&gt;% filter(taxousda_legend$Number %in% nm.taxo) %&gt;% 
    select(Number, Group)
NM.dsm.rf.tax.pd = Partial$new(NM.dsm.rf, feature = &quot;TAXOUSDA_250m&quot;, ice = TRUE)  #, center.at=min(nm.esg.dsm.cov.rm$TAXOUSDA_250m))
plot(NM.dsm.rf.tax.pd)
NM.dsm.rf.tax.results &lt;- NM.dsm.rf.tax.pd$results
NM.dsm.rf.tax.results$.id &lt;- as.factor(NM.dsm.rf.tax.results$.id)
site_es &lt;- data.frame(as.factor(seq(1, length(nm.esg.dsm.cov.rm$esite), 
    1)), nm.esg.dsm.cov.rm$esite) %&gt;% set_names(c(&quot;.id&quot;, &quot;esite&quot;))
NM.dsm.rf.tax.results &lt;- left_join(NM.dsm.rf.tax.results, site_es, by = &quot;.id&quot;)
NM.dsm.rf.tax.results &lt;- left_join(NM.dsm.rf.tax.results, NM_dsm.rf.obsError, 
    by = &quot;.id&quot;)

NM.dsm.rf.tax.pd.es1 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES1&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = TAXOUSDA_250m, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es2 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES2&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = TAXOUSDA_250m, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es3 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES3&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = TAXOUSDA_250m, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es4 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES4&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = TAXOUSDA_250m, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es5 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES5&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = TAXOUSDA_250m, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es6 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES6&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = TAXOUSDA_250m, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es7 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES7&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = TAXOUSDA_250m, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es99 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES99&quot;) %&gt;% group_by(.id) %&gt;% group_by(esite) %&gt;% ggplot(aes(x = TAXOUSDA_250m, 
    y = .y.hat, group = .id, colour = esite)) + geom_line() + scale_y_continuous(&quot;Probability&quot;) + 
    scale_colour_viridis_d() + theme(legend.position = &quot;none&quot;)

NM_dsm_rf_ice &lt;- grid.arrange(NM.dsm.rf.tax.pd.es1, NM.dsm.rf.tax.pd.es2, 
    NM.dsm.rf.tax.pd.es3, NM.dsm.rf.tax.pd.es4, NM.dsm.rf.tax.pd.es5, NM.dsm.rf.tax.pd.es6, 
    NM.dsm.rf.tax.pd.es7, NM.dsm.rf.tax.pd.es99, ncol = 3)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_dsm_rf_ice.pdf&quot;, 
    NM_dsm_rf_ice, width = 10, height = 10, dpi = 300, units = &quot;in&quot;)

# Subset so that it only plots observations belonging to each ESG and
# then displays correct and missclassified observations
NM.dsm.rf.tax.pd.es1 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES1&quot; &amp; esite == &quot;ES1&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = TAXOUSDA_250m, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es2 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES2&quot; &amp; esite == &quot;ES2&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = TAXOUSDA_250m, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es3 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES3&quot; &amp; esite == &quot;ES3&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = TAXOUSDA_250m, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es4 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES4&quot; &amp; esite == &quot;ES4&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = TAXOUSDA_250m, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es5 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES5&quot; &amp; esite == &quot;ES5&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = TAXOUSDA_250m, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es6 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES6&quot; &amp; esite == &quot;ES6&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = TAXOUSDA_250m, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es7 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES7&quot; &amp; esite == &quot;ES7&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = TAXOUSDA_250m, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)
NM.dsm.rf.tax.pd.es99 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES99&quot; &amp; esite == &quot;ES99&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict) %&gt;% 
    ggplot(aes(x = TAXOUSDA_250m, y = .y.hat, group = .id, colour = predict)) + 
    geom_line() + scale_y_continuous(&quot;Probability&quot;, limits = c(0, 1)) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;)

NM_dsm_rf_ice.sub &lt;- grid.arrange(NM.dsm.rf.tax.pd.es1, NM.dsm.rf.tax.pd.es2, 
    NM.dsm.rf.tax.pd.es3, NM.dsm.rf.tax.pd.es4, NM.dsm.rf.tax.pd.es5, NM.dsm.rf.tax.pd.es6, 
    NM.dsm.rf.tax.pd.es7, NM.dsm.rf.tax.pd.es99, ncol = 3)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_dsm_rf_ice.sub.pdf&quot;, 
    NM_dsm_rf_ice.sub, width = 10, height = 10, dpi = 300, units = &quot;in&quot;)


# Categorical ICE plot

ice.plot.categorical.byclass &lt;- function(R6, filtered.df) {
    p = ggplot(filtered.df, mapping = aes_string(x = R6$feature.name, y = &quot;.y.hat&quot;, 
        group = &quot;.id&quot;)) + scale_y_continuous(y.axis.label)
    p = p + geom_boxplot(aes_string(group = R6$feature.name))
    rug.dat = cbind(R6$.__enclos_env__$private$sampler$get.x(), data.frame(.y.hat = R6$results$.y.hat[1]), 
        .id = 1)
    rug.dat = rug.dat[sample(1:nrow(rug.dat)), ]
    sides = ifelse(R6$n.features == 2 &amp;&amp; R6$feature.type[1] == R6$feature.type[2], 
        &quot;bl&quot;, &quot;b&quot;)
    p = p + geom_rug(data = rug.dat, alpha = 0.2, sides = sides, position = position_jitter(width = 0.1, 
        height = 0.1))
    # p = p + facet_wrap(&#39;.class&#39;)
    return(p)
}

NM.dsm.rf.tax.pd.es1 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES1&quot; &amp; esite == &quot;ES1&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict)
NM.dsm.rf.tax.pd.es2 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES2&quot; &amp; esite == &quot;ES2&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict)
NM.dsm.rf.tax.pd.es4 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES4&quot; &amp; esite == &quot;ES4&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict)
NM.dsm.rf.tax.pd.es5 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES5&quot; &amp; esite == &quot;ES5&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict)
NM.dsm.rf.tax.pd.es6 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES6&quot; &amp; esite == &quot;ES6&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict)
NM.dsm.rf.tax.pd.es7 &lt;- NM.dsm.rf.tax.results %&gt;% filter(.type == &quot;ice&quot; &amp; 
    .class == &quot;ES7&quot; &amp; esite == &quot;ES7&quot;) %&gt;% group_by(.id) %&gt;% group_by(predict)


NM.dsm.rf.tax.pd.es1.plot &lt;- ice.plot.categorical(NM.dsm.rf.tax.pd, NM.dsm.rf.tax.pd.es1)
NM.dsm.rf.tax.pd.es2.plot &lt;- ice.plot.categorical(NM.dsm.rf.tax.pd, NM.dsm.rf.tax.pd.es2)
NM.dsm.rf.tax.pd.es3.plot &lt;- ice.plot.categorical(NM.dsm.rf.tax.pd, NM.dsm.rf.tax.pd.es3)
NM.dsm.rf.tax.pd.es4.plot &lt;- ice.plot.categorical(NM.dsm.rf.tax.pd, NM.dsm.rf.tax.pd.es4)
NM.dsm.rf.tax.pd.es5.plot &lt;- ice.plot.categorical(NM.dsm.rf.tax.pd, NM.dsm.rf.tax.pd.es5)
NM.dsm.rf.tax.pd.es6.plot &lt;- ice.plot.categorical(NM.dsm.rf.tax.pd, NM.dsm.rf.tax.pd.es6)
NM.dsm.rf.tax.pd.es7.plot &lt;- ice.plot.categorical(NM.dsm.rf.tax.pd, NM.dsm.rf.tax.pd.es7)


NM_dsm_rf_ice.tax.sub &lt;- grid.arrange(NM.dsm.rf.tax.pd.es1.plot, NM.dsm.rf.tax.pd.es2.plot, 
    NM.dsm.rf.tax.pd.es3.plot, NM.dsm.rf.tax.pd.es4.plot, NM.dsm.rf.tax.pd.es5.plot, 
    NM.dsm.rf.tax.pd.es6.plot, NM.dsm.rf.tax.pd.es7.plot, ncol = 3)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_dsm_rf_tax_ice.sub.pdf&quot;, 
    NM_dsm_rf_ice.tax.sub, width = 10, height = 10, dpi = 300, units = &quot;in&quot;)





# Categorical ICE plot
p = ggplot(NM.dsm.rf.tax.pd$results[NM.dsm.rf.tax.pd$results$.type == &quot;ice&quot;, 
    ], mapping = aes_string(x = NM.dsm.rf.tax.pd$feature.name, y = &quot;.y.hat&quot;, 
    group = &quot;.id&quot;)) + scale_y_continuous(y.axis.label)
p = p + geom_boxplot(aes_string(group = NM.dsm.rf.tax.pd$feature.name))
aggr = NM.dsm.rf.tax.pd$results[NM.dsm.rf.tax.pd$results$.type == &quot;pdp&quot;, 
    ]
p = p + geom_line(data = aggr, mapping = aes_string(x = NM.dsm.rf.tax.pd$feature.name, 
    y = &quot;.y.hat&quot;), size = 2, color = &quot;gold&quot;)
p = p + geom_line(data = aggr, mapping = aes_string(x = NM.dsm.rf.tax.pd$feature.name, 
    y = &quot;.y.hat&quot;), size = 1, color = &quot;black&quot;)
rug.dat = cbind(NM.dsm.rf.tax.pd$.__enclos_env__$private$sampler$get.x(), 
    data.frame(.y.hat = NM.dsm.rf.tax.pd$results$.y.hat[1]), .id = 1)
rug.dat = rug.dat[sample(1:nrow(rug.dat)), ]
sides = ifelse(NM.dsm.rf.tax.pd$n.features == 2 &amp;&amp; NM.dsm.rf.tax.pd$feature.type[1] == 
    NM.dsm.rf.tax.pd$feature.type[2], &quot;bl&quot;, &quot;b&quot;)
p = p + geom_rug(data = rug.dat, alpha = 0.2, sides = sides, position = position_jitter(width = 0.1, 
    height = 0.1))
p = p + facet_wrap(&quot;.class&quot;)
p
#---------------------------------------------------------------------------------------------

CO.dsm.mrrtf.pd = Partial$new(CO.dsm.svm, feature = &quot;MRRTF&quot;, ice = TRUE, 
    center.at = min(co.esg.dsm.cov.rm$MRRTF))
plot(CO.dsm.mrrtf.pd)
CO.dsm.ndvi.pd.results &lt;- CO.dsm.ndvi.pd$results
CO.dsm.ndvi.pd.results$.id &lt;- as.factor(CO.dsm.ndvi.pd.results$.id)
ES6 &lt;- CO.dsm.ndvi.pd.results %&gt;% filter(.class == &quot;ES6&quot; &amp; .type == &quot;ice&quot; &amp; 
    .id %in% c(CO_es6_list$site)) %&gt;% group_by(.id)
ES6 &lt;- CO.dsm.ndvi.pd$results %&gt;% filter(.class == &quot;ES6&quot; &amp; .type == &quot;ice&quot;)
test &lt;- ES6 %&gt;% filter(NDVI_Dec_Mean == max(NDVI_Dec_Mean))

plot_df &lt;- CO.dsm.ndvi.pd$results %&gt;% filter(.type == &quot;ice&quot;) %&gt;% group_by(.id)
plot_df &lt;- CO.dsm.ndvi.pd.results %&gt;% filter(.type == &quot;ice&quot; &amp; .class == 
    &quot;ES6&quot; &amp; .id %in% c(CO_es6_list$site)) %&gt;% group_by(.id) %&gt;% plot_ly(x = ~NDVI_Dec_Mean, 
    y = ~.y.hat, type = &quot;scatter&quot;, mode = &quot;lines+markers&quot;)

plot_df &lt;- CO.dsm.ndvi.pd.results %&gt;% filter(.type == &quot;ice&quot; &amp; .class == 
    &quot;ES6&quot; &amp; .id %in% CO_es6_list) %&gt;% group_by(.id) %&gt;% ggplot(aes(x = NDVI_Dec_Mean, 
    y = .y.hat, group = .id)) + geom_line() + geom_point() + theme_minimal()

plot_df &lt;- CO.dsm.ndvi.pd.results %&gt;% filter(.type == &quot;ice&quot; &amp; .class == 
    &quot;ES6&quot; &amp; (!(.id %in% c(CO_es6_list$site)))) %&gt;% group_by(.id) %&gt;% ggplot(aes(x = NDVI_Dec_Mean, 
    y = .y.hat, group = .id)) + geom_line() + geom_point() + theme_minimal()


# show plots
plot_df$plots

CO.dsm.ndvi.pd = Partial$new(CO.dsm.rf, feature = &quot;NDVI_Dec_Mean&quot;, ice = TRUE, 
    center.at = min(co.esg.dsm.cov.rm$NDVI_Dec_Mean))
plot(CO.dsm.ndvi.pd)
# Partial dependence plots using mlr package
CO.dsm.mrrtf.pd = generatePartialDependenceData(CO_dsm_train.svm.prob, 
    CO_dsm_var, &quot;MRRTF&quot;)
plotPartialDependence(CO.dsm.mrrtf.pd, data = getTaskData(CO_dsm_var))

# CO PDP: Top 4
CO.dsm.svm.pdp = generatePartialDependenceData(CO_dsm_train.svm.prob, CO_dsm_var, 
    c(&quot;MRRTF&quot;, &quot;calp3&quot;, &quot;nwness&quot;, &quot;AWCtS_M_sl5_250m&quot;, &quot;BLDFIE_M_sl1_250m&quot;, 
        &quot;CLYPPT_M_sl7_250m&quot;), fun = mean)
plotPartialDependence(CO.dsm.svm.pdp, data = getTaskData(CO_dsm_var))
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_dsm_svm_pdp.pdf&quot;)

CO.dsm.rf.pdp = generatePartialDependenceData(CO_dsm_train.rf.prob, CO_dsm_var, 
    c(&quot;NDVI_Dec_Mean&quot;, &quot;relht64&quot;, &quot;LST_Day_Nov_Mean&quot;, &quot;relht16&quot;, &quot;TAXOUSDA_250m&quot;, 
        &quot;NDVI_Feb_Mean&quot;), fun = mean)
plotPartialDependence(CO.dsm.rf.pdp, data = getTaskData(CO_dsm_var))
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_dsm_rf_pdp.pdf&quot;)

CO.dsm.xgb.pdp = generatePartialDependenceData(CO_dsm_train.xgb.prob, CO_dsm_var, 
    c(&quot;TAXOUSDA_250m&quot;, &quot;relht32&quot;, &quot;NDVI_Feb_Mean&quot;, &quot;NDVI_Feb_std&quot;), fun = mean)
plotPartialDependence(CO.dsm.xgb.pdp, data = getTaskData(CO_dsm_var))
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_dsm_xgb_pdp.pdf&quot;)

CO.dsm.rf.int = generatePartialDependenceData(CO_dsm_train.rf.prob, CO_dsm_var, 
    c(&quot;NAMrad_K&quot;, &quot;NAMrad_Th&quot;), interaction = TRUE)
plotPartialDependence(CO.dsm.rf.int, facet = &quot;NAMrad_Th&quot;)</code></pre>
<p>##Model dependent method for feature importance</p>
<pre class="r"><code>############################################################################################################################################
#-------------------Feature Importance---------------------------------

#---------------------------------------------------------------------------------------  
# #Model dependent method for feature importance 
# 
#   #-------Filter methods for feature selection and importance
#     # NM_abiotic_IG = generateFilterValuesData(NM_abiotic_var, method = c(&quot;information.gain&quot;))
#     # plotFilterValues(NM_abiotic_IG)
#     #
#     # NM_abiotic_RF = generateFilterValuesData(NM_abiotic_var, method = c(&quot;randomForest.importance&quot;))
#     # plotFilterValues(NM_abiotic_RF)
#   
#   #-----svm
#     NM_dsm_var.train.svm = mlr::train(learner = NM_dsm_lrns[[1]],
#                                              task = NM_dsm_var)
#     NM_dsm_svm_FI &lt;-  mlr::getFeatureImportance(NM_dsm_var.train.svm)
#   
#     NM.svm.varImp &lt;- generateFeatureImportanceData(task=NM_dsm_var,
#   
#     ##Backburner for now#######################################################################
#     #rminer approach for calculating variable importance using SA.
#     model &lt;- rminer::fit(esite ~., getTaskData(NM_dsm_var), model=&quot;svm&quot;)
#     I &lt;- Importance(model, getTaskData(NM_dsm_var),method=&quot;sensv&quot;)
#     L=list(runs=1,sen=t(I$imp),sresponses=I$sresponses)
#     print(round(I$imp,digits=2))
#     imax=which.max(I$imp)
#     imax
#     mgraph(L,graph=&quot;IMP&quot;,leg=names(getTaskData(NM_dsm_var)),col=&quot;gray&quot;,Grid=10)
#     vecplot(I,graph=&quot;VEC&quot;,xval=1,graph=&quot;VEC&quot;(),xval=2,cex=1.2,TC=2,
#             main=&quot;VEC curve for x2 influence on y (class B)&quot;,xlab=&quot;x2&quot;)
#   
#   L=list(runs=1,sen=t(I$imp),sresponses=I$sresponses) # create a simple
#   par(mar=c(2.0,2.0,2.0,2.0)) # enlarge PDF margin
#   mgraph(L,graph=&quot;IMP&quot;,leg=names(cmath),col=&quot;gray&quot;,Grid=10,PDF=&quot;imp-1&quot;)
#   txt=paste(&quot;VEC curve for&quot;,names(cmath)[imax],&quot;influence on class&quot;,levels(y)
#   [TC])
#   mgraph(L,graph=&quot;VEC&quot;,xval=imax,Grid=10,data=cmath[H$tr,],TC=1,main=txt,PDF=
#   &quot;vec-1&quot;)
#   
#   
#     library(rminer)
#   M &lt;- fit(y~., data=train, model=&quot;svm&quot;, kpar=list(sigma=0.10), C=2)
#   svm.imp &lt;- Importance(M, data=train)
#     # ############################################################################################
#   
#   #-----randomForest
#     NM_dsm_var.train.rf = mlr::train(learner = NM_dsm_lrns[[2]],
#                                              task = NM_dsm_var)
#     NM_dsm_rf_FI &lt;-  mlr::getFeatureImportance(NM_dsm_var.train.rf, type=2)
#   
#         # #randomForest variable importance calcualtion and plotting
#         # varImpPlot(getLearnerModel(NM_abiotic_var.train.rf), type=2)
#   
#   #-----xgboost
#     NM_dsm_var.train.xgboost = train(NM_dsm_lrns[[3]],
#                                              task = NM_dsm_var)
#     NM_dsm_xgboost_FI &lt;-  getFeatureImportance(NM_dsm_var.train.xgboost)
#   
#         # #Plotting method using xgb.ggplot.importance function
#         # NM_abiotic_xgboost_FI_mlr.dt &lt;- data.table:::data.table(colnames(NM_abiotic_xgboost_FI_mlr$res), t(NM_abiotic_xgboost_FI_mlr$res))
#         # names(NM_abiotic_xgboost_FI_mlr.dt)&lt;- c(&quot;Feature&quot;, &quot;Gain&quot;)
#         # xgb.ggplot.importance(importance_matrix = NM_abiotic_xgboost_FI_mlr.dt,
#         #                       top_n = 40,
#         #                       n_clusters = 1)
#   
#   
#         #------------------Code from XGBOOST to calculate and plot variable importance
#         # #Produces same result as plotting the output from &quot;getFeatureImportance
#         # library(xgboost)
#         # NM_abiotic_xgboost_FI &lt;-  getLearnerModel(NM_abiotic_var.train.rf)
#         #   xgb.importance(getTaskFeatureNames(NM_abiotic_var.filtered.task), model = getLearnerModel(NM_abiotic_var.train.xgboost))
#         # xgb.ggplot.importance(
#         #   importance_matrix = NM_abiotic_xgboost_FI,
#         #   rel_to_first = TRUE,
#         #   top_n = 40,
#         #   n_clusters = 1
#         # )
#         # xgb.ggplot.importance(importance_matrix = NM_abiotic_xgboost_FI,
#         #                       top_n = 40,
#         #                       n_clusters = 1)</code></pre>
<p>##Raster predict for CO and NM DSM models</p>
<pre class="r"><code>detach(&quot;package:caret&quot;, unload = TRUE)
# CO raster predictions
#---------------------------------------------------------------------------------------
# create raster brick of CO covariate data
co.mask &lt;- &quot;/data/data2/data/esgMapping/analysis/data/derived_data/raster_covariates/CO_mask&quot;
co.raster.list &lt;- preStack(path = co.mask, pattern = &quot;.tif&quot;)
co.cov.brick &lt;- brick(stack(co.raster.list, bands = 1), filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_Covariate_Brick.grd&quot;)
co.cov.brick &lt;- brick(&quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_Covariate_Brick.grd&quot;)

# Need to create hyper-temp bricks, one for each variable, e.g.,
# co.ndvi.brick, co.lst.day.brick...... then write new raster.predict
# function that can read in several imput brick, extract a chunk, join
# them and then predict. That way you don&#39;t need to create one giant
# brick that will crash the system.

# Predict modeled surfaces for DSM database
CO_svm_dsm_prediction &lt;- mlr.raster.predict.run(x = co.cov.brick, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_svm_dsm_prediction.tif&quot;, 
    model = CO_dsm_train.svm, block_n = 120, mask = CO_raster, layers = 1)
CO_svm_dsm_prediction &lt;- raster(&quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_svm_dsm_prediction.tif&quot;)
CO_rf_dsm_prediction &lt;- mlr.raster.predict.run(x = co.cov.brick, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_rf_dsm_prediction.tif&quot;, 
    model = CO_dsm_train.rf, block_n = 120, mask = CO_raster, layers = 1)
CO_rf_dsm_prediction &lt;- raster(&quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_xgb_dsm_prediction.tif&quot;)
CO_xgb_dsm_prediction &lt;- mlr.raster.predict.run(x = co.cov.brick, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_xgb_dsm_prediction.tif&quot;, 
    model = CO_dsm_train.xgb, block_n = 120, mask = CO_raster, layers = 1)
CO_xgb_dsm_prediction &lt;- raster(&quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_xgb_dsm_prediction.tif&quot;)
CO_ens_dsm_prediction &lt;- mlr.raster.predict.run(x = co.cov.brick, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_ens_dsm_prediction.tif&quot;, 
    model = CO_dsm_train.ens, block_n = 120, mask = CO_raster, layers = 9)  #1 layer for each class plus 1 for the final response

# abiotic prediction
CO_ens_abiotic_prediction &lt;- mlr.raster.predict.run(x = co.cov.brick, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_ens_abiotic_prediction.tif&quot;, 
    model = CO_abiotic_train.ens, block_n = 120, mask = CO_raster, layers = 9)

#---------------------------------------Predict model probabilities
CO_svm_dsm_prediction_prob &lt;- mlr.raster.predict.run(x = co.cov.brick, 
    filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_svm_dsm_prediction_prob.tif&quot;, 
    model = CO_dsm_train.svm.prob, block_n = 120, mask = CO_raster, layers = 9)

CO_rf_dsm_prediction_prob &lt;- mlr.raster.predict.run(x = co.cov.brick, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_rf_dsm_prediction_prob.tif&quot;, 
    model = CO_dsm_train.rf.prob, block_n = 120, mask = CO_raster, layers = 9)

CO_xgb_dsm_prediction_prob &lt;- mlr.raster.predict.run(x = co.cov.brick, 
    filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_xgb_dsm_prediction_prob.tif&quot;, 
    model = CO_dsm_train.xgb.prob, block_n = 120, mask = CO_raster, layers = 9)

# lookup key
unique(co.points@data[10:11])

# Plot prediction probabilites
red &lt;- colorRampPalette(c(&quot;tomato4&quot;, &quot;yellow2&quot;))
blue &lt;- colorRampPalette(c(&quot;green2&quot;, &quot;blue2&quot;))
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/CO_ESG_RF_class_probabilites.pdf&quot;, 
    8, 8, bg = &quot;transparent&quot;)
spplot(CO_rf_dsm_prediction_prob[[1:8]], names.attr = c(&quot;Shallow shrubland&quot;, 
    &quot;Saline uplands&quot;, &quot;Finer shrublands&quot;, &quot;Saline hills&quot;, &quot;Bottoms and flats&quot;, 
    &quot;Deep rocky&quot;, &quot;Sandy Grasslands&quot;, &quot;Outcrops and slopes&quot;), strip = strip.custom(style = 1, 
    bg = &quot;white&quot;), par.strip.text = list(cex = 0.7), col.regions = c(red(20), 
    blue(20)), layout = c(4, 2), cuts = 30, contour = F, labels = F, pretty = T, 
    maxpixels = 1e+05, colorkey = list(space = &quot;top&quot;, width = 0.8), at = seq(1, 
        0, length = 40))
dev.off()


#--------------------------------------Confusion Index
## Calculate confusion index and most probable class
CO_svm_dsm_CI &lt;- CI.calc.run(CO_svm_dsm_prediction_prob, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_svm_dsm_CI.tif&quot;)
CO_rf_dsm_CI &lt;- CI.calc.run(CO_rf_dsm_prediction_prob, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_rf_dsm_CI.tif&quot;)
CO_xgb_dsm_CI &lt;- CI.calc.run(CO_xgb_dsm_prediction_prob, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_xgb_dsm_CI.tif&quot;)
CO_ens_dsm_CI &lt;- CI.calc.run(CO_ens_dsm_prediction, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_ens_dsm_CI.tif&quot;)


# Plot confusion index
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/CO_ESG_SVM_confusion_index.pdf&quot;, 
    8, 8, bg = &quot;transparent&quot;)
levelplot(CO_svm_dsm_CI, par.settings = RdBuTheme, maxpixels = 8e+05)
dev.off()
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/CO_ESG_RF_confusion_index.pdf&quot;, 
    8, 8, bg = &quot;transparent&quot;)
levelplot(CO_rf_dsm_CI, par.settings = RdBuTheme, maxpixels = 8e+05)
dev.off()
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/CO_ESG_XGB_confusion_index.pdf&quot;, 
    8, 8, bg = &quot;transparent&quot;)
levelplot(CO_xgb_dsm_CI, par.settings = RdBuTheme, maxpixels = 8e+05)
dev.off()
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/CO_ESG_ENS_confusion_index.pdf&quot;, 
    6, 6, bg = &quot;transparent&quot;)
levelplot(CO_ens_dsm_CI, par.settings = RdBuTheme, maxpixels = 8e+05)
dev.off()


# Standardized Shannon Entropy Index
CO_svm_dsm_SSEI &lt;- SSEI.calc.run(CO_svm_dsm_prediction_prob[[-9]], filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_svm_dsm_SSEI.tif&quot;)
CO_rf_dsm_SSEI &lt;- SSEI.calc.run(CO_rf_dsm_prediction_prob[[-9]], filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_rf_dsm_SSEI.tif&quot;)
CO_xgb_dsm_SSEI &lt;- SSEI.calc.run(CO_xgb_dsm_prediction_prob[[-9]], filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_xgb_dsm_SSEI.tif&quot;)
CO_ens_dsm_SSEI &lt;- SSEI.calc.run(CO_ens_dsm_prediction[[-9]], filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/CO_ens_dsm_SSEI.tif&quot;)

# Standardized Shannon Entropy Index
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/CO_ESG_SVM_SSEI.pdf&quot;, 
    6, 6, bg = &quot;transparent&quot;)
levelplot(CO_svm_dsm_SSEI, par.settings = RdBuTheme, maxpixels = 8e+05, 
    margin = FALSE)
dev.off()
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/CO_ESG_RF_SSEI.pdf&quot;, 
    6, 6, bg = &quot;transparent&quot;)
levelplot(CO_rf_dsm_SSEI, par.settings = RdBuTheme, maxpixels = 8e+05, 
    margin = FALSE)
dev.off()
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/CO_ESG_XGB_SSEI.pdf&quot;, 
    6, 6, bg = &quot;transparent&quot;)
levelplot(CO_xgb_dsm_SSEI, par.settings = RdBuTheme, maxpixels = 8e+05, 
    margin = FALSE)
dev.off()
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/CO_ESG_ENS_SSEI.pdf&quot;, 
    6, 6, bg = &quot;transparent&quot;)
levelplot(CO_ens_dsm_SSEI, par.settings = RdBuTheme, maxpixels = 8e+05, 
    margin = FALSE)
dev.off()

# Theoretical map accuracy
CO_ens_dsm_prediction.maxP &lt;- calc(CO_ens_dsm_prediction[[1:8]], max)
CO_ens_dsm_TheoreticalMapAccuracy &lt;- cellStats(CO_ens_dsm_prediction.maxP, 
    mean)
#---------------------------------------------------------------------------------------
# Color palette used in rangelands article
cbPalette &lt;- c(&quot;#00a884&quot;, &quot;#beffe7&quot;, &quot;#ffa77f&quot;, &quot;#ffffbf&quot;, &quot;#737301&quot;, &quot;#fe0000&quot;, 
    &quot;#0071fe&quot;, &quot;#732500&quot;)
# Plot SVM
CO_svm_dsm_plot &lt;- gplot(CO_svm_dsm_prediction, maxpixels = 8e+05) + geom_raster(aes(fill = factor(value))) + 
    coord_equal() + labs(x = &quot;Long&quot;, y = &quot;Lat&quot;, fill = &quot;ESG&quot;) + ggtitle(&quot;Ecological Site Group&quot;) + 
    scale_fill_manual(values = cbPalette)
CO_svm_dsm_plot
ggsave(CO_svm_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/CO_svm_dsm_plot.pdf&quot;, 
    width = 6, height = 6)
ggsave(CO_svm_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/CO_svm_dsm_plot.png&quot;, 
    width = 6, height = 6, type = &quot;cairo-png&quot;)

# Plot RF
CO_rf_dsm_plot &lt;- gplot(CO_rf_dsm_prediction, maxpixels = 8e+05) + geom_raster(aes(fill = factor(value))) + 
    coord_equal() + labs(x = &quot;Long&quot;, y = &quot;Lat&quot;, fill = &quot;ESG&quot;) + ggtitle(&quot;Ecological Site Group&quot;) + 
    scale_fill_manual(values = cbPalette)
CO_rf_dsm_plot
ggsave(CO_rf_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/CO_rf_dsm_plot.pdf&quot;, 
    width = 6, height = 6)
ggsave(CO_rf_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/CO_rf_dsm_plot.png&quot;, 
    width = 6, height = 6, type = &quot;cairo-png&quot;)

# Plot xgb
CO_xgb_dsm_plot &lt;- gplot(CO_xgb_dsm_prediction, maxpixels = 8e+05) + geom_raster(aes(fill = factor(value))) + 
    coord_equal() + labs(x = &quot;Long&quot;, y = &quot;Lat&quot;, fill = &quot;ESG&quot;) + ggtitle(&quot;Ecological Site Group&quot;) + 
    scale_fill_manual(values = cbPalette)
CO_xgb_dsm_plot
ggsave(CO_xgb_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/CO_xgb_dsm_plot.pdf&quot;, 
    width = 6, height = 6)
ggsave(CO_xgb_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/CO_xgb_dsm_plot.png&quot;, 
    width = 6, height = 6, type = &quot;cairo-png&quot;)

# Plot ENS
CO_ens_dsm_plot &lt;- gplot(CO_ens_dsm_prediction[[9]], maxpixels = 8e+05) + 
    geom_raster(aes(fill = factor(value))) + coord_equal() + labs(x = &quot;Long&quot;, 
    y = &quot;Lat&quot;, fill = &quot;ESG&quot;) + ggtitle(&quot;Ecological Site Group&quot;) + scale_fill_manual(values = cbPalette)
CO_ens_dsm_plot
ggsave(CO_ens_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/CO_ens_dsm_plot.pdf&quot;, 
    width = 6, height = 6)
ggsave(CO_ens_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/CO_ens_dsm_plot.png&quot;, 
    width = 6, height = 6, type = &quot;cairo-png&quot;)

# Plot ENS
CO_ens_abiotic_plot &lt;- gplot(CO_ens_abiotic_prediction[[9]], maxpixels = 8e+05) + 
    geom_raster(aes(fill = factor(value))) + coord_equal() + labs(x = &quot;Long&quot;, 
    y = &quot;Lat&quot;, fill = &quot;ESG&quot;) + ggtitle(&quot;Ecological Site Group&quot;) + scale_fill_manual(values = cbPalette)
CO_ens_abiotic_plot
ggsave(CO_ens_abiotic_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/CO_ens_abiotic_plot.pdf&quot;, 
    width = 8, height = 8)
ggsave(CO_ens_abiotic_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/CO_ens_abiotic_plot.png&quot;, 
    width = 8, height = 8, type = &quot;cairo-png&quot;)


#---------------------------------------------------------------------------------------


#---------------------------------------------------------------------------------------
# create raster brick of NM covariate data
nm.mask &lt;- &quot;/data/data2/data/esgMapping/analysis/data/derived_data/raster_covariates/NM_mask&quot;
# aeroradiometric grids have a large hole within study area due to
# White Sands, therefore removed
nm.raster.list &lt;- preStack(path = nm.mask, pattern = &quot;.tif&quot;)[-170:-173]
nm.cov.brick &lt;- brick(stack(nm.raster.list, bands = 1), filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_Covariate_Brick.grd&quot;)
nm.cov.brick &lt;- brick(&quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_Covariate_Brick.grd&quot;)
# NM predict modeled surfaces
NM_svm_dsm_prediction &lt;- mlr.raster.predict.run(x = nm.cov.brick, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_svm_dsm_prediction.tif&quot;, 
    model = NM_dsm_train.svm, block_n = 120, mask = NM_raster, layers = 1)

NM_rf_dsm_prediction &lt;- mlr.raster.predict.run(x = nm.cov.brick, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_rf_dsm_prediction.tif&quot;, 
    model = NM_dsm_train.rf, block_n = 120, mask = NM_raster, layers = 1)

NM_xgb_dsm_prediction &lt;- mlr.raster.predict.run(x = nm.cov.brick, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_xgb_dsm_prediction.tif&quot;, 
    model = NM_dsm_train.xgb, block_n = 120, mask = NM_raster, layers = 1)

NM_ens_dsm_prediction &lt;- mlr.raster.predict.run(x = nm.cov.brick, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_ens_dsm_prediction.tif&quot;, 
    model = NM_dsm_train.ens, block_n = 120, mask = NM_raster, layers = 8)


#---------------------------------------Predict model probabilities
NM_svm_dsm_prediction_prob &lt;- mlr.raster.predict.run(x = nm.cov.brick, 
    filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_svm_dsm_prediction_prob.tif&quot;, 
    model = NM_dsm_train.svm.prob, block_n = 120, mask = NM_raster, layers = 8)

NM_rf_dsm_prediction_prob &lt;- mlr.raster.predict.run(x = nm.cov.brick, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_rf_dsm_prediction_prob.tif&quot;, 
    model = NM_dsm_train.rf.prob, block_n = 120, mask = NM_raster, layers = 8)

NM_xgb_dsm_prediction_prob &lt;- mlr.raster.predict.run(x = nm.cov.brick, 
    filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_xgb_dsm_prediction_prob.tif&quot;, 
    model = NM_dsm_train.xgb.prob, block_n = 120, mask = NM_raster, layers = 8)

# lookup key
unique(nm.points@data[14:15])

# Plot prediction probabilites
red &lt;- colorRampPalette(c(&quot;tomato4&quot;, &quot;yellow2&quot;))
blue &lt;- colorRampPalette(c(&quot;green2&quot;, &quot;blue2&quot;))
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ESG_RF_class_probabilites.pdf&quot;, 
    8, 8, bg = &quot;transparent&quot;)
spplot(NM_rf_dsm_prediction_prob[[1:7]], names.attr = c(&quot;Gravelly-Calcic&quot;, 
    &quot;Loamy-Clayey&quot;, &quot;Bedrock and Colluvium&quot;, &quot;Bottomland&quot;, &quot;Sandy&quot;, &quot;Deep sand&quot;, 
    &quot;Gypsic&quot;), strip = strip.custom(style = 1, bg = &quot;white&quot;), par.strip.text = list(cex = 0.7), 
    col.regions = c(red(20), blue(20)), layout = c(4, 2), cuts = 30, contour = F, 
    labels = F, pretty = T, maxpixels = 1e+05, colorkey = list(space = &quot;top&quot;, 
        width = 0.8), at = seq(1, 0, length = 40))
dev.off()


#--------------------------------------Confusion Index
## Calculate confusion index and most probable class
NM_svm_dsm_CI &lt;- CI.calc.run(NM_svm_dsm_prediction_prob, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_svm_dsm_CI.tif&quot;)
NM_rf_dsm_CI &lt;- CI.calc.run(NM_rf_dsm_prediction_prob, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_rf_dsm_CI.tif&quot;)
NM_xgb_dsm_CI &lt;- CI.calc.run(NM_xgb_dsm_prediction_prob, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_xgb_dsm_CI.tif&quot;)
NM_ens_dsm_CI &lt;- CI.calc.run(NM_ens_dsm_prediction, filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_ens_dsm_CI.tif&quot;)


# Plot confusion index
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ESG_SVM_confusion_index.pdf&quot;, 
    8, 8, bg = &quot;transparent&quot;)
levelplot(NM_svm_dsm_CI, par.settings = RdBuTheme, maxpixels = 8e+05)
dev.off()
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ESG_RF_confusion_index.pdf&quot;, 
    8, 8, bg = &quot;transparent&quot;)
levelplot(NM_rf_dsm_CI, par.settings = RdBuTheme, maxpixels = 8e+05)
dev.off()
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ESG_XGB_confusion_index.pdf&quot;, 
    8, 8, bg = &quot;transparent&quot;)
levelplot(NM_xgb_dsm_CI, par.settings = RdBuTheme, maxpixels = 8e+05)
dev.off()
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ESG_ENS_confusion_index.pdf&quot;, 
    8, 8, bg = &quot;transparent&quot;)
levelplot(NM_ens_dsm_CI, par.settings = RdBuTheme, maxpixels = 8e+05)
dev.off()

# Standardized Shannon Entropy Index
NM_svm_dsm_SSEI &lt;- SSEI.calc.run(NM_svm_dsm_prediction_prob[[-8]], filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_svm_dsm_SSEI.tif&quot;)
NM_rf_dsm_SSEI &lt;- SSEI.calc.run(NM_rf_dsm_prediction_prob[[-8]], filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_rf_dsm_SSEI.tif&quot;)
NM_xgb_dsm_SSEI &lt;- SSEI.calc.run(NM_xgb_dsm_prediction_prob[[-8]], filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_xgb_dsm_SSEI.tif&quot;)
NM_ens_dsm_SSEI &lt;- SSEI.calc.run(NM_ens_dsm_prediction[[-8]], filename = &quot;/data/data2/data/esgMapping/analysis/data/derived_data/NM_ens_dsm_SSEI.tif&quot;)

# Scaled Shannon Entropy Index
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ESG_SVM_SSEI.pdf&quot;, 
    6, 6, bg = &quot;transparent&quot;)
levelplot(NM_svm_dsm_SSEI, par.settings = RdBuTheme, maxpixels = 8e+05, 
    margin = FALSE)
dev.off()
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ESG_RF_SSEI.pdf&quot;, 
    6, 6, bg = &quot;transparent&quot;)
levelplot(NM_rf_dsm_SSEI, par.settings = RdBuTheme, maxpixels = 8e+05, 
    margin = FALSE)
dev.off()
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ESG_XGB_SSEI.pdf&quot;, 
    6, 6, , bg = &quot;transparent&quot;)
levelplot(NM_xgb_dsm_SSEI, par.settings = RdBuTheme, maxpixels = 8e+05, 
    margin = FALSE)
dev.off()
CairoPDF(&quot;/data/data2/data/esgMapping/analysis/figures/NM_ESG_ENS_SSEI.pdf&quot;, 
    6, 6, bg = &quot;transparent&quot;)
levelplot(NM_ens_dsm_SSEI, par.settings = RdBuTheme, maxpixels = 8e+05, 
    margin = FALSE)
dev.off()

# Theoretical map accuracy
NM_ens_dsm_prediction.maxP &lt;- calc(NM_ens_dsm_prediction[[1:7]], max)
NM_ens_dsm_TheoreticalMapAccuracy &lt;- cellStats(NM_ens_dsm_prediction.maxP, 
    mean)


#---------------------------------------------------------------------------------------
# Plot SVM
NM_svm_dsm_plot &lt;- gplot(NM_svm_dsm_prediction, maxpixels = 8e+05) + geom_raster(aes(fill = factor(value))) + 
    coord_equal() + labs(x = &quot;Long&quot;, y = &quot;Lat&quot;, fill = &quot;ESG&quot;) + ggtitle(&quot;Ecological Site Group&quot;) + 
    scale_fill_manual(values = cbPalette)
# NM_svm_dsm_plot
ggsave(NM_svm_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/NM_svm_dsm_plot.pdf&quot;, 
    width = 6, height = 6)
ggsave(NM_svm_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/NM_svm_dsm_plot.png&quot;, 
    width = 6, height = 6, type = &quot;cairo-png&quot;)

# Plot RF
NM_rf_dsm_plot &lt;- gplot(NM_rf_dsm_prediction, maxpixels = 8e+05) + geom_raster(aes(fill = factor(value))) + 
    coord_equal() + labs(x = &quot;Long&quot;, y = &quot;Lat&quot;, fill = &quot;ESG&quot;) + ggtitle(&quot;Ecological Site Group&quot;) + 
    scale_fill_manual(values = cbPalette)
# NM_rf_dsm_plot
ggsave(NM_rf_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/NM_rf_dsm_plot.pdf&quot;, 
    width = 6, height = 6)
ggsave(NM_rf_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/NM_rf_dsm_plot.png&quot;, 
    width = 6, height = 6, type = &quot;cairo-png&quot;)

# Plot xgb
NM_xgb_dsm_plot &lt;- gplot(NM_xgb_dsm_prediction, maxpixels = 8e+05) + geom_raster(aes(fill = factor(value))) + 
    coord_equal() + labs(x = &quot;Long&quot;, y = &quot;Lat&quot;, fill = &quot;ESG&quot;) + ggtitle(&quot;Ecological Site Group&quot;) + 
    scale_fill_manual(values = cbPalette)
# pred_plot
ggsave(NM_xgb_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/NM_xgb_dsm_plot.pdf&quot;, 
    width = 6, height = 6)
ggsave(NM_xgb_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/NM_xgb_dsm_plot.png&quot;, 
    width = 6, height = 6, type = &quot;cairo-png&quot;)

# Plot ENS
NM_ens_dsm_plot &lt;- gplot(NM_ens_dsm_prediction[[8]], maxpixels = 8e+05) + 
    geom_raster(aes(fill = factor(value))) + coord_equal() + labs(x = &quot;Long&quot;, 
    y = &quot;Lat&quot;, fill = &quot;ESG&quot;) + ggtitle(&quot;Ecological Site Group&quot;) + scale_fill_manual(values = cbPalette)
# NM_ens_dsm_plot
ggsave(NM_ens_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/NM_ens_dsm_plot.pdf&quot;, 
    width = 6, height = 6)
ggsave(NM_ens_dsm_plot, file = &quot;/data/data2/data/esgMapping/analysis/figures/NM_ens_dsm_plot.png&quot;, 
    width = 6, height = 6, type = &quot;cairo-png&quot;)
#---------------------------------------------------------------------------------------


#---------------------------------------------------------------------------------------
# save.image(&#39;/data/data2/data/esgMapping/R/CO_NM_ESG_modeling.RData&#39;)
load(&quot;/data/data2/data/esgMapping/R/CO_NM_ESG_modeling.RData&quot;)
#---------------------------------------------------------------------------------------</code></pre>
<p>##ESG class distribution</p>
<pre class="r"><code># look at ESG class distribution
co.esite.val.dist &lt;- plyr::count(co.validation.points.covariates[, 2])
par(mar = c(10.5, 4, 3.5, 4))
mp &lt;- barplot(co.esite.val.dist$freq, axes = FALSE, axisnames = FALSE)
text(mp, par(&quot;usr&quot;)[3], labels = co.esite.val.dist$x, srt = 45, adj = c(1.1, 
    1.1), xpd = TRUE, cex = 0.9)
axis(2)

nm.esite.val.dist &lt;- plyr::count(nm.validation.points.covariates[, 2])
par(mar = c(10.5, 4, 3.5, 4))
mp &lt;- barplot(nm.esite.val.dist$freq, axes = FALSE, axisnames = FALSE)
text(mp, par(&quot;usr&quot;)[3], labels = nm.esite.val.dist$x, srt = 45, adj = c(1.1, 
    1.1), xpd = TRUE, cex = 0.9)
axis(2)</code></pre>
<p>##External Cross-validation</p>
<pre class="r"><code>#---------------External Cross-validation
#----External validation load in external validation points and predict on them
co.validation.points.covariates
nm.validation.points.covariates &lt;- nm.validation.points.covariates.narm
# modify level b/c missing two for CO
levels(co.validation.points.covariates[, 2]) &lt;- c(levels(co.validation.points.covariates[, 
    2]), &quot;ES1&quot;, &quot;ES6&quot;)

library(purrr)
cf_extract &lt;- function(model, data) {
    mod &lt;- predict(model, newdata = data[, model$features])
    cf &lt;- confusionMatrix(mod$data$response, data[, 2])
    return(cf$overall)
}

# CO create list of models to run in loop
datasub &lt;- c(&quot;all&quot;, &quot;hyper&quot;, &quot;ndvi&quot;, &quot;abiotic&quot;, &quot;sg&quot;, &quot;dsm&quot;)
CO.models &lt;- data.frame()
for (i in 1:length(datasub)) {
    CO.models &lt;- bind_rows(CO.models, data.frame(paste0(&quot;CO_&quot;, datasub[i], 
        &quot;_train.svm&quot;), paste0(&quot;CO_&quot;, datasub[i], &quot;_train.rf&quot;), paste0(&quot;CO_&quot;, 
        datasub[i], &quot;_train.xgb&quot;), paste0(&quot;CO_&quot;, datasub[i], &quot;_train.ens&quot;)))
}
CO.models &lt;- as.vector(t(CO.models))


cf_results &lt;- data.frame()
for (i in 1:length(CO.models)) {
    ext.res &lt;- cf_extract(get(CO.models[i]), data = co.validation.points.covariates)
    ext.res &lt;- ext.res %&gt;% data.frame() %&gt;% t() %&gt;% set_names(c(&quot;Accuracy&quot;, 
        &quot;Kappa&quot;, &quot;AccuracyLower&quot;, &quot;AccuracyUpper&quot;, &quot;AccuracyNull&quot;, &quot;AccuracyPValue&quot;, 
        &quot;McnemarPValue&quot;))
    cf_results &lt;- bind_rows(cf_results, ext.res)
}

CO_ext_val_accuracy &lt;- bind_cols(CO.class.accuracy[, 1:2], cf_results)

ESG.class.plot(data = CO_ext_val_accuracy, class = 3, title = &quot;MLRA 35 ESG Ext-Validation Accuracy&quot;, 
    filename = &quot;CO_Model_ExtVal_Accuracy&quot;, scale = 1)
ESG.class.plot(data = CO_ext_val_accuracy, class = 3, title = &quot;MLRA 35 ESG Ext-Validation Accuracy&quot;, 
    filename = &quot;CO_Model_ExtVal_Accuracy&quot;, scale = 0.6)

# Plot
ggplot(CO_ext_val_accuracy, aes(x = Data, y = Accuracy)) + geom_jitter(aes(colour = factor(Model)), 
    size = 4, width = 0.2, height = 0) + # geom_point(col=&#39;tomato2&#39;, size=3) + # Draw points
geom_segment(aes(x = Data, xend = Data, y = 0.32, yend = 0.56), linetype = &quot;dashed&quot;, 
    size = 0.1) + # coord_flip() + # Draw dashed lines
ylab(&quot;Accuracy&quot;) + xlab(&quot;Dataset&quot;) + # coord_flip() + # Draw dashed lines
labs(title = &quot;MLRA 35 ESG External Model Accuracy&quot;, color = &quot;Model&quot;)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/CO_external_model_comparison5.pdf&quot;, 
    width = 4.3, height = 2.6)


# CO DSM ENS
CO_dsm_ens_extVal &lt;- predict(get(CO.models[24]), newdata = co.validation.points.covariates[, 
    get(CO.models[24])$features])

# NM create list of models to run in loop
datasub &lt;- c(&quot;all&quot;, &quot;hyper&quot;, &quot;ndvi&quot;, &quot;abiotic&quot;, &quot;sg&quot;, &quot;dsm&quot;)
NM.models &lt;- data.frame()
for (i in 1:length(datasub)) {
    NM.models &lt;- bind_rows(NM.models, data.frame(paste0(&quot;NM_&quot;, datasub[i], 
        &quot;_train.svm&quot;), paste0(&quot;NM_&quot;, datasub[i], &quot;_train.rf&quot;), paste0(&quot;NM_&quot;, 
        datasub[i], &quot;_train.xgb&quot;), paste0(&quot;NM_&quot;, datasub[i], &quot;_train.ens&quot;)))
}
NM.models &lt;- as.vector(t(NM.models))


cf_results &lt;- data.frame()
for (i in 1:length(NM.models)) {
    ext.res &lt;- cf_extract(get(NM.models[i]), data = nm.validation.points.covariates)
    ext.res &lt;- ext.res %&gt;% data.frame() %&gt;% t() %&gt;% set_names(c(&quot;Accuracy&quot;, 
        &quot;Kappa&quot;, &quot;AccuracyLower&quot;, &quot;AccuracyUpper&quot;, &quot;AccuracyNull&quot;, &quot;AccuracyPValue&quot;, 
        &quot;McnemarPValue&quot;))
    cf_results &lt;- bind_rows(cf_results, ext.res)
}

NM_ext_val_accuracy &lt;- bind_cols(NM.class.accuracy[, 1:2], cf_results)

ESG.class.plot(data = NM_ext_val_accuracy, class = 3, title = &quot;MLRA 42 ESG Ext-Validation Accuracy&quot;, 
    filename = &quot;NM_Model_ExtVal_Accuracy&quot;, scale = 1)
ESG.class.plot(data = NM_ext_val_accuracy, class = 3, title = &quot;MLRA 42 ESG Ext-Validation Accuracy&quot;, 
    filename = &quot;NM_Model_ExtVal_Accuracy&quot;, scale = 0.6)

# Plot
ggplot(NM_ext_val_accuracy, aes(x = Data, y = Accuracy)) + geom_jitter(aes(colour = factor(Model)), 
    size = 4, width = 0.2, height = 0) + # geom_point(col=&#39;tomato2&#39;, size=3) + # Draw points
geom_segment(aes(x = Data, xend = Data, y = 0.32, yend = 0.56), linetype = &quot;dashed&quot;, 
    size = 0.1) + # coord_flip() + # Draw dashed lines
ylab(&quot;Accuracy&quot;) + xlab(&quot;Dataset&quot;) + # coord_flip() + # Draw dashed lines
labs(title = &quot;MLRA 42 ESG External Model Accuracy&quot;, color = &quot;Model&quot;)
ggsave(&quot;/data/data2/data/esgMapping/analysis/figures/NM_external_model_comparison5.pdf&quot;, 
    width = 4.3, height = 2.6)


# NM DSM ENS
NM_dsm_ens_extVal &lt;- predict(get(NM.models[24]), newdata = nm.validation.points.covariates[, 
    get(NM.models[24])$features])

#---------------------------------------------------------------------------------------</code></pre>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span> Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.5.2 (2018-12-20)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

Matrix products: default

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
 [1] workflowr_1.3.0   Rcpp_1.0.1        digest_0.6.19    
 [4] rprojroot_1.3-2   backports_1.1.4   git2r_0.25.2.9000
 [7] magrittr_1.5      evaluate_0.13     stringi_1.4.3    
[10] fs_1.2.7          whisker_0.3-2     rmarkdown_1.12   
[13] tools_3.5.2       stringr_1.4.0     glue_1.3.1       
[16] xfun_0.6          yaml_2.2.0        compiler_3.5.2   
[19] htmltools_0.3.6   knitr_1.23       </code></pre>
</div>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
